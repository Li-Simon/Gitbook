# 自己动手写神经网络

这里需要实现NN，CNN,RNN，区别在于是否加卷积。对于NN，要保证层数不说限制，每层的神经元不受限制，只处理一维输入,输出也就一维,也就是分类问题；CNN处理二维输入\(图像\)，设计卷积和卷积核，实现的例子就是LeNet-5。RNN就是实现最简单的RNN。三者通用的东西是什么？可以提出来，我们需要用到哪些模块？这个心里是要清楚的。  
![](/assets/LeNet.png)

![](/assets/WriteNNByHand.png)

## NN

## CNN

![](/assets/BP_CNN1.png)  

&emsp;&emsp;$$n_l$$是层神经元数目；  
&emsp;&emsp;$$W^{(l)} \in R^{n_l \times n_{l-1}}$$是第l-1层到第l层的的权重矩阵。  
&emsp;&emsp;$$W^{(l)}_{ij}$$表示第l-1层的j神经元到l层的i神经元之间的权重。  
&emsp;&emsp;$$f()$$是激活函数。  
&emsp;&emsp;$$\mathbf{b^{(l)}}=[b^{(l)}_1,b^{(l)}_2,...,b^{(l)}_{n_l}] \in R^{n_l}$$表示第l-1层到第l层的的偏置。
&emsp;&emsp;$$\mathbf{z^{(l)}}=[z^{(l)}_1,z^{(l)}_2,...,z^{(l)}_{n_l}]\in R^{n_l}$$表示第l层神经元的的状态。    
&emsp;&emsp;$$\mathbf{a^{(l)}}=[a^{(l)}_1,a^{(l)}_2,...,a^{(l)}_{n_l}]\in R^{n_l}$$表示第l层神经元的的激活值。  
##信息前向传播
因此我们根据下面的公式计算每层的神经元状态与输出值，计算从第2层到最后一层L。  
&emsp;&emsp;$$\mathbf{z^{(l)}}=\mathbf{W^{(l)}a^{(l-1)} + b^{(l)}}$$   
&emsp;&emsp;$$\mathbf{a^{(l)}}=f(\mathbf{z^{(l)}})$$   
对于第L层，最终输出$$\mathbf{a^{(L)}}$$。信息传播如下：  
&emsp;&emsp;$$\mathbf{x = a^{(1)} \to z^{(2)}\to a^{(2)} \to ...\to a^{(L-1)} \to z^{(L)} \to a^{(L)} = y}$$  
##误差反向传播
训练数据为$$\mathbf{(x^{(1)},y^{(1)}),...,(x^{(N)},y^{(N)})}$$,即N个数据，假设输出是$$n_L$$维的，也就是：  
&emsp;&emsp;$$\mathbf{y^{(i)}}=(y^{(i)}_1,y^{(i)}_2,...,y^{(i)}_{n_L})^T\in R^{n_L \times 1}$$  
对于第i个数据，假设误差是均方误差，$$\mathbf{o^{(i)}}$$是输出。   
&emsp;&emsp;$$E_i = \frac{1}{2} (\mathbf{y^{(i)}-o^{(i)}})^2$$  
&emsp;&emsp;&emsp;&emsp;=$$\frac{1}{2} \displaystyle \sum_{k=1}^{n_L}(y_k^{(i)}-o_k^{(i)})^2$$
因此对总的N个数据。总的误差表示如下： 
&emsp;&emsp;$$E =\frac{1}{N} \displaystyle \sum_{i=1}^{N}E_i$$



  

