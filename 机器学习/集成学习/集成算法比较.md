### RF,GBDT,XGBOOST,LightGBM比较

### RF

随机森林\(RF\)是Bagging的扩展变体，它在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入随机特征选择，主要包含两个特征：  
1. 随机的有放回的样本选择  
2. 随机选择特征  
其原理是通过减少通过投票来减少整体的方差。虽然每个基分类器的bias大，但是通过多个彼此不相关的基分类器能减少总体的方差。

##### 优点：

1. 数据集合上表现良好，相对于其他算法有较大的优势\(训练速度，预测准确度\)  
2. 能够处理高纬度的数据，并且不用特征选择，而且在训练完后，给出特征的重要性  
3.容易做并行处理 

##### 缺点：

1. 在噪声较大的数据集上，分类与回归问题会出现过拟合。  

### GBDT

传统的Boosting算法，比如AdaBoost主要关注的是正确错误样本的权重，而GBDT的每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以在残差减小的梯度方向建立模型，在GradientBoost中，每个新的模型的建立时为了使得之前的模型的残差往梯度下降的方法。  
在GradientBoosting算法中，关键即使利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合棵CART回归树。  
GBDT会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树，而不是分类树\(尽管GBDT调整后也可以用于分类，但不代表GBDT的树是分类树\)

##### GBDT的优点：

GBDT的性能在RF的基础上又有一步提升，因此其优点也很明显：  
1. 它能灵活的处理各种类型的数据  
2. 在相对较少的调参时间下，预测的准确度较高

##### 缺点：

由于它是Boosting,因此基学习器之间存在串行关系，难以并行训练数据。

### XGBoost
XGBoost相对于GBDT的优势在于;  
1. 传统的GBDT在优化的时候，之利用了一阶信息，而XGBoost对代价函数进行二阶泰勒展开，利用了二阶信息[^2]  
2. XGBoost在模型中加入了正则项，用来控制模型的复杂程度，使得学出来的模型更简单，防止过拟合。  
3. 列抽样， XGBoost借鉴了RF做法，支持列抽样，不仅防止过拟合，而且能减少计算量。  
4. 对缺失值的处理\(Why?\)。对于特征的值有缺失的样本，XGBoost还可以自动学习出它的分裂方向；
5. XGBoost工具支持并行\(Why?\)。注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代 中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行[^1]。

### LightGBM
#####LightGBM的优化点
1. 采用直方图算法[^4]
2. 树的生长策略优化
3. 相对于xgboost和GBDT,LightGBM提出了两个新方法，使得LightGBM的效率要显著要高于GBDT和xgboost。这两种新方法是：Gradient-based One-Side Sampling (GOSS：基于梯度的one-side采样) 和Exclusive Feature Bundling (EFB：互斥的特征捆绑）  

这两种新方法：
1. GOSS（从减少样本角度）：排除大部分小梯度的样本，仅用剩下的样本计算信息增益。
2. EFB（从减少特征角度）：捆绑互斥特征，也就是他们很少同时取非零值（也就是用一个合成特征代替）。

GBDT是基于决策树的集成算法，采用前向分布算法，在每次迭代中，都是通过负梯度拟合残差，从而学习一颗决策树，最耗时的步骤就是找最优划分点。一种流行的方法就是预排序，核心是在已经排好序的特征值上枚举所有可能的特征点。另一种改进则是直方图算法，他把连续特征值划分到k个桶中取，划分点则在这k个点中选取。$$k \le d$$ 所以在内存消耗和训练速度都更佳，且在实际的数据集上表明，离散化的分裂点对最终的精度影响并不大，甚至会好一些。原因在于决策树本身就是一个弱学习器，采用Histogram算法会起到正则化的效果，有效地防止模型的过拟合。LightGBM也是基于直方图的。

####直方图算法（Histogram） 
直方图算法是先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。遍历数据时，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。
它的优点如下：
直方图只需对直方图统计量计算信息增益，相比较于预排序算法每次都遍历所有的值，信息增益的计算量要小很多
通过利用叶节点的父节点和相邻节点的直方图的相减来获得该叶节点的直方图，从而减少构建直方图次数，提升效率
存储直方图统计量所使用的内存远小于预排序算法。  
####树的生长策略优化
LightGBM 通过 leaf-wise (best-first)策略来生长树。它将选取具有最大信息增益最大的叶节点来生长。 当生长相同的叶子时，leaf-wise 算法可以比 level-wise 算法减少更多的损失。  
当 数据较小的时候，leaf-wise 可能会造成过拟合。 所以，LightGBM 可以利用额外的参数 max_depth 来限制树的深度并避免过拟合（树的生长仍然通过 leaf-wise 策略）。

#### GOSS\(Gradient-based One-Side Sampling\) 
在AdaBoost中采用权重很好诠释了样本的重要性，GBDT没有这种权重，但是我们注意到每个数据样本的梯度可以被用来做采样的信息。也就是，如果一个样本的梯度小，那么表明这个样本已经训练好了，它的训练误差很小了，我们可以丢弃这些数据。当然，改变数据分布会造成模型的精度损失。GOSS则通过保存大梯度样本，随机选取小梯度样本，并为其弥补上一个常数权重。这样，GOSS更关注训练不足的样本，同时也不会改变原始数据太多。[^2] [^3]
#### EFB(Exclusive Feature Bundling)
高维数据一般是稀疏的，可以设计一种损失最小的特征减少方法。并且，在稀疏特征空间中，许多特征都是互斥的，也就是它们几乎不同时取非0值。因此，我们可以安全的把这些互斥特征绑到一起形成一个特征，然后基于这些特征束构建直方图，这样又可以加速了。  
EFB算法可以把很多特征绑到一起，形成更少的稠密特征束，这样可以避免对0特征值的无用的计算。加速计算直方图还可以用一个表记录数据的非0值。  

[^1]:  RF、GBDT、XGBoost面试级整理 [https://blog.csdn.net/qq\_28031525/article/details/70207918]  

[^2]:  『 论文阅读』LightGBM原理-LightGBM  [https://blog.csdn.net/shine19930820/article/details/79123216](https://blog.csdn.net/shine19930820/article/details/79123216)   

[^3]:  LightGBM原理之论文详解 https://zhuanlan.zhihu.com/p/35155992  
[^4]:  机器学习——LightGBM https://www.cnblogs.com/wkslearner/p/9333168.html


