# 稀疏低秩分解

## 范数

### L0,L1,L1范数

L0范数表示向量中非0元素的个数。如果我们对一个矩阵加L0约束，也就是要求矩阵为0的元素越多越好。也就是希望矩阵是稀疏的。  
L1范数是指向量中各个元素绝对值之和。也称为"稀疏规则算子"\(Lasso regularization\)。加L1范数能实现权值稀疏。  
由于L0范数很难优化求解\(NP难问题\)，而且L1范数十L0范数的最优凸近似。\(n=1是保证$$L^n$$范数是凸函数的最小整数\)。因此一般用L1来实现稀疏性。  
参数稀疏性带来的好处就是特征选择与强的可解释性。  
L2范数叫做Ridge回归，他能防止模型过拟合，因为他尽量让$$||W||_2$$尽量小，但与L1不同，他不会让参数等于0。因为参数越小，模型越简单，因此不容易产生过拟合。  
L2好处是能防止过拟合，还可以使得条件数很多的Hessian阵优化更容易，也就是使得原来不正定的矩阵变得正定，这里的矩阵一般就是Hessian矩阵。增量拉格朗日乘子法就是这么做的，L-M算法也是这么做的。  
![](/assets/L0_L1.png)

### 核范数

核范数$$||W||_*$$是指矩阵奇异值之和，英文是Nuclear Norm。它的作用就是为了实现Low-Rank\(低秩\)。所谓的秩就是线性方程组中独立的线性方程组数目。矩阵的秩就是矩阵的非0奇异值的个数。因此，加核范数能实现低秩矩阵。  
如下是核范数的两个应用。

#### 矩阵填充

也就是推荐系统中，在User-Item矩阵中，有些用于没有购买一些items，那么我们是否该向他们推荐这些items，这在数学上是矩阵填充问题，在推荐系统中就是向从没有购买过某商品的用户推荐某商品。  
我们可以用低秩约束来实现矩阵填充，因为我们的先验是，矩阵各行\(列\)之间是高度相关的，因此矩阵是低秩的。

#### 鲁棒PCA

主成分分析，可以有效的找出数据中最主要的元素和结构，去除噪声与冗余，将原来的复杂数据姜维，揭露隐藏在复杂数据背后的简单结构。最简单的主成分分析就是PCA了。PCA的目的就是为了找到主元，最大程度的去除冗余和噪音的干扰。  
鲁棒主成分分析\(Robust PCA\)考虑的是这样一个问题：一般我们的数据矩阵X会包含结构信息，也包含噪声。那么我们可以将这个矩阵分解为两个矩阵，一个是低秩的\(由于内部有一定的结构信息，造成各行和各列之间是线性相关的\)，另一个是稀疏的\(由于含有噪声，而噪声是稀疏的\)。则鲁棒主成分分析可以写成以下的优化问题：  
  $$\displaystyle \min _{A,E} rank(A) + \lambda ||E||_0$$;  s.t. $$X = A + E$$  
与经典PCA问题一样，鲁棒PCA本质上也是寻找数据低维空间上的最佳投影问题。对于低秩数据观测矩阵X，假设X收到随机\(稀疏\)噪声的影响，则X的低秩就会破坏，使X变成满秩的。所以我们需要将X分解成包含其真实结构的低秩矩阵和稀疏噪声矩阵之和。为了找到低秩矩阵，实际上就找到了矩阵的本质低维空间。那有了PCA,为什么还需要有这个Robust PCA, Robust在哪？因为PCA假设我们的数据的噪声是高斯分布的，对于大的噪声或者严重的离群点，PCA就会被它影响，导致无法工作。而Robust PCA则不存在这个假设。它只是假设噪声是稀疏的，而不管噪声的强弱如何。  
由于rank和L0范数在优化上存在非凸和非光滑特性，所以我们一般将它转化成求解以下一个松弛的凸优化问题：  
  $$\displaystyle \min _{A,E} ||A||_* + \lambda ||E||_1$$;  s.t. $$X = A + E$$  

 说个应用吧。考虑同一副人脸的多幅图像，如果将每一副人脸图像看成是一个行向量，并将这些向量组成一个矩阵的话，那么可以肯定，理论上，这个矩阵应当是低秩的。但是，由于在实际操作中，每幅图像会受到一定程度的影响，例如遮挡，噪声，光照变化，平移等。这些干扰因素的作用可以看做是一个噪声矩阵的作用。所以我们可以把我们的同一个人脸的多个不同情况下的图片各自拉长一列，然后摆成一个矩阵，对这个矩阵进行低秩和稀疏的分解，就可以得到干净的人脸图像（低秩矩阵）和噪声的矩阵了（稀疏矩阵），例如光照，遮挡等等。至于这个的用途，你懂得。

![](/assets/robust_PCA.png)
####背景建模


