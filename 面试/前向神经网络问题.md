# 前向神经网络问题
####为啥Sigmoid和Tanh激活函数会导致梯度消失的现象？
因为在参数很大或者很小的时候，他们的导数会趋于0，因此造成了梯度消失。  
####ReLU系列激活函数相对于Sigmoid和Tanh激活函数的有点事什么？它们有什么局限性以及如何改进？  
#####优点
(1)计算梯度简单，因为Sigmod和Tanh涉及到指数运算  
(2)ReLU能有效解决后两者的题都消失问题，提供相对宽的激活边界   
(3)ReLU的单侧抑制提供了网络的稀疏表达能力  
#####缺点
ReLU的局限性在于其训练过程中会导致神经元死亡的问题，由于激活函数和是$$mf(x) = \max(0,z)$$导致梯度在经过该ReLU单元时，被置为0。且之后再也不能被任何数据激活，因为流经这个神经元的梯度永远为0，因此不对任何数据产生响应。因此会导致一定比例的神经元不可逆的死亡，进而参数梯度永远无法更新，整个训练过程失败。  
为了解决这一问题，人们设计了Leaky ReLU(LReLU)其表达式是：  
$$\kern{8 em}f(z) = z; z> 0$$    
$$\kern{8 em}f(z) = az; z\le 0$$    
一般a是一个很小的正数，但是怎么选取a也是一个问题。为解决这个问题，人们发明了参数化的PReLU(Parametric ReLU),它与LReLU的主要区别是，斜率参数a作为网络中一个可学习的参数。而另一个LReLU的变种增加了"随机化"机制，具体的，在训练过程中，斜率a作为一个满足某种分布的随机采样；测试时再固定下来。Random ReLU(RReLU)在一定程度上能起到正则化的作用。  
####平方误差损失函数和交叉熵损失函数分别适合什么场景？  
平方损失函数:回归问题，输出是连续值，并且最后一层不含Sigmoid或者Softmax激活函数的NN。因为会导致梯度很小而发生梯度消失的问题  
交叉熵：分类问题，
####神经网络训练技巧


