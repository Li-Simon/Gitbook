# 共轭梯度法

共轭梯度法（Conjecture Gradient）是介于最速下降法与牛顿法之间的一个方法，它仅需要一阶导数信息，但克服了最速下降法收敛慢的缺点，同时又避免了牛顿法需要储存和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化问题最有效的算法之一。  
向量共轭的定义：  
若$$\mathbf{A}$$是正定对称阵，若非0矢量$$\mathbf{u,v}$$满足，$$\mathbf{u^{T}Av} = 0$$，则称矢量$$\mathbf{u,v}$$是共轭的。  
假设：  
$$P = \mathbf{[p_1,p_2,...,p_n]}$$是一组基于$$\mathbf{A}$$的共轭矢量，则$$\mathbf{Ax = b}$$的解$$\mathbf{x^*}$$可以表示为：  
$$\mathbf{x^*} = \sum_{i=1}^{n}{\alpha}_i\mathbf{P_i} $$,因此基于基矢量展开，我们有：  
$$\kern{4 em}\mathbf{Ax_*} = \sum_{i=1}^{n}{\alpha}_i\mathbf{AP_i} $$  
左乘$$\mathbf{P^T_{k}}$$：  
$$\kern{4 em}\mathbf{P^T_{k}Ax_*} = \sum_{i=1}^{n}{\alpha}_iP^T_{k}\mathbf{AP_i} $$  
代入：$$\mathbf{\mathbf{P^T_{k}Ax_*} = b}$$,$$\mathbf{u^TAv = \left\langle u,v \right\rangle_A}$$,利用$$i != k$$有$$\mathbf{\left\langle p_k,p_i \right\rangle _A = 0}$$ 就得到：

如果我们的小心的选择$$\mathbf{p_k}$$,为了获得解$$\mathbf{x_*}$$的一个好的近似，我们并不需要所有的基向量。因此，我们把共轭梯度算法当成一种迭代算法，它也允许我们近似n很大，以至于直接求解需要花费太多时间的系统。  
我们给$$\mathbf{x_*}$$一个初始猜测值$$\mathbf{x_0}$$，假设$$\mathbf{x_0 = 0}$$,在求解的过程中，我们需要一种标准来告诉我们是否我们的解更靠近真实的解$$\mathbf{x_*}$$,实际上，求解$$\mathbf{Ax = b}$$等价于求解如下二次函数的极小值：  
$$\kern{4 em}\mathbf{f(x) = \frac{1}{2}x^TAx - x^Tb}$$,  
他存在唯一的最小值，因为他的二阶导是正定的：  
$$\kern{4 em}\mathbf{D^2f(x) = A}$$,  
解就是一阶导数等于0的地方  
$$\kern{4 em}\mathbf{Df(x) = Ax - b}$$,  
假设第一个基矢$$\mathbf{p_0}$$就是$$f(x)$$在$$\mathbf{x = x_0}$$处的负梯度，我们可以假设$$\mathbf{x_0 = 0}$$.因此：  
$$\kern{4 em}\mathbf{p_0 = b - Ax_0}$$.  
令$$\mathbf{r_0}$$表示第k步的残差：  
$$\mathbf{r_k = b - Ax_k}$$,$$\mathbf{r_k}$$是$$\mathbf{f(x)}$$在$$\mathbf{x = x_k}$$处的负梯度。  
为了让$$\mathbf{p_k}$$与前面的所有的$$\mathbf{p_i}$$相互共轭，$$\mathbf{p_k}$$可以如下构造：  
$$\mathbf{p_k = r_k - \sum _{i < k}\frac{p_i^TAr_{k}}{p_i^TAp_i}}$$  
沿着这个方向，因此下一步的优化方向就是：  
$$\kern{4 em}\mathbf{x_{k+1} = x_k + {\alpha}_kp_k}$$,满足：  
$$g'({\alpha}_k = 0)$$  
因此：  
$$\kern{4 em}\mathbf{f(x_{k+1}) = f(x_k + {\alpha}_kp_k) = :g({\alpha}_k)}$$,  
$$\kern{4 em}\mathbf{\alpha _k= \frac{p_k^Tb}{p_k^TAp_k} = \frac{p_k^T(r_k + Ax_k)}{p_k^TAp_k} = \frac{p_k^Tr_k}{p_k^TAp_k}}$$  
算法：  
$$\mathbf{r_0 := b - Ax_0}$$  
$$\mathbf{p_0 := r_0}$$  
$$\mathbf{k := 0}$$  
repeat:  
$$\kern{4 em}\mathbf{\alpha _k := r_0}$$  
$$\kern{4 em}\mathbf{\alpha _k := \frac{r_k^Tr_k}{p_k^TAp_k}}$$  
$$\kern{4 em}\mathbf{x_{k+1} := x_k + \alpha _kp_k}$$  
$$\kern{4 em}\mathbf{r_{k+1} := r_k + \alpha _kAp_k}$$  
if $$r_{k+1}$$ is sufficiently small, then exit loop.  
$$\kern{4 em}\mathbf{\beta _k := \frac{r_{k+1}^Tr_{k+1}}{r_k^Tr_k}}$$  
$$\kern{4 em}\mathbf{p_{k+1} := r_{k+1} + \beta _kp_k}$$  
$$\kern{4 em}\mathbf{k := k+1}$$

