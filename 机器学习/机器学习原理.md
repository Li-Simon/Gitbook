#机器学习原理
##偏差，方差，噪声
偏差-方差分解可以用来对学习算法的期望泛化错误率进行拆分。同时，偏差-方差分解为我们设计模型与分析提供了一个比较清晰的方向。  
假设测试样本是$$\mathbf{x}$$，令$$\mathbf{y_D}$$为$$\mathbf{x}$$在数据集上的标记(可能存在标记错误的情况)，y为$$\mathbf{x}$$的真实标记，$$f(\mathbf{x};D)$$为训练集D上学得模型f在$$\mathbf{x}$$上的预测输出，以回归模型为例，学习算法的期望预测为：  
$$\kern{4 em} \hat f(\mathbf{x}) = E_D[f(\mathbf{x},D) ]$$  
方差是：  
$$\kern{4 em} var(\mathbf{x}) = E_D[f(\mathbf{x},D) - \hat f(\mathbf{x})]^2$$  
噪声是：
$$\kern{4 em} \epsilon ^2= E_D[y_D - y]^2$$    
输出期望与真实标记之间的差别成为偏差（Bias），即：
$$\kern{4 em} bias ^2(\mathbf{x}))= E_D[\hat f(\mathbf{x}) - y]^2$$   
为方便讨论，假设噪声的期望为0； 即：$$E_D[y_D - y] = 0$$. 下面来对模型的期望泛化误差进行分解：  
$$\kern{4 em} E(f;D) = E_D[(f(\mathbf{x},D) - y_D)^2]$$  
$$\kern{8 em} = E_D[(f(\mathbf{x},D) - \hat f(\mathbf{x})+ \hat f(\mathbf{x})- y_D)^2]$$  
$$\kern{8 em} = E_D[(f(\mathbf{x},D) - \hat f(\mathbf{x}))^2]+ E_D[(\hat f(\mathbf{x})- y_D)2] + E_D[2(f(\mathbf{x},D) - \hat f(\mathbf{x}))(\hat f(\mathbf{x})- y_D)]$$   
$$\kern{8 em} = E_D[(f(\mathbf{x},D) - \hat f(\mathbf{x}))^2]+ E_D[(\hat f(\mathbf{x})- y_D)^2]$$   
$$\kern{8 em} = E_D[(f(\mathbf{x},D) - \hat f(\mathbf{x}))^2]+ E_D[(\hat f(\mathbf{x})- y + y -y_D)^2]$$   
$$\kern{8 em} = E_D[(f(\mathbf{x},D) - \hat f(\mathbf{x}))^2]+ E_D[(\hat f(\mathbf{x})- y)^2] + E_D[(y -y_D)^2] + E_D[2(\hat f(\mathbf{x})- y)(y-y_D)]$$   
$$\kern{8 em} = E_D[(f(\mathbf{x},D) - \hat f(\mathbf{x}))^2]+ E_D[(\hat f(\mathbf{x})- y)^2] + E_D[(y -y_D)^2] $$ 
因此就得到：
$$\kern{4 em} E(f;D)= bias^2(\mathbf{x}) + var(\mathbf{x}) + \epsilon ^2 $$ 
也就是泛化误差分成偏差，方差与噪声之和。我们设计模型分析时，可以从这三方面去考虑。  














































































































































































































