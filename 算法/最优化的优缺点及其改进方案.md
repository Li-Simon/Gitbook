# 最优化的优缺点及其改进方案
##常见的损失函数
###分类问题
####0-1损失函数
y是标签，f是预测结果。 
对于二分类问题$$Y = [-1,1]$$   
0-1损失函数：  
$$\kern{4 em} L_{0-1}(f,y) =1_{fy \le 0}$$  
Hinge损失函数：  
$$\kern{4 em} L_{hinge}(f,y) = \max(0,1-fy)$$  
Logistic损失函数：  
$$\kern{4 em} L_{logistic}(f,y) = \log_2(1+\exp(-fy))$$  
当预测值$$f \in [-1, 1]$$时，可以定义如下损失函数：  
交叉熵（Cross Entropy）:  
$$\kern{4 em} L_{cross entropy} = -\log_2(\frac{1 + fy}{2})$$  
###回归问题
对于回归问题，Y=R,我们希望$$f(x_i, \theta) \approx y_i$$,最常用的损失函数是平方损失函数：  
$$\kern{4 em} L_{square}(f,y) = (f-y)^2$$  
平方损失函数对异常点惩罚很大，因此对异常点比较敏感。为了解决这个问题，引入了绝对损失函数：  
$$\kern{4 em} L_{square}(f,y) = |f-y|$$  
但是绝对损失函数在0点不可以求导。  
一个方法是在0点进行函数光滑化也就是在0点附件用平方函数代替。$$\delta->0$$
$$\kern{4 em} L_{square}(f,y) = \sqrt{(f-y)^2+\delta^2}$$    
或者用Huber损失函数：  
Huber损失函数：  
$$\kern{4 em} L_{square}(f,y) = (f-y)^2, |f-y|>\delta$$  
$$\kern{4 em} L_{square}(f,y) = 2\delta|f-y| - \delta^2, |f-y|>\delta$$  
还得补充与思考文本处理，信号处理中的损失函数。  
####机器学习中哪些是凸优化问题，哪些是非凸优化问题
凸函数条件：Hassian矩阵为半正定。  
逻辑回归，线性回归，SVM是凸优化问题。PCA，矩阵分解以及NN是非凸优化问题。  
##梯度下降
####问题1：怎样验证自己代码中梯度公式的正确性？
$$\kern{4 em} |\frac{L(\theta + he_i) - L(\theta + he_i)}{2h} - \frac{\partial L(\theta)}{\theta_i}| \approx Mh^2$$  
若$$M \le 10^7$$,则:  
$$\kern{4 em}  |\frac{L(\theta + he_i) - L(\theta + he_i)}{2h} - \frac{\partial L(\theta)}{\theta_i}|  \le h$$ 
因此取$$h=10^{-7}$$看,看上式是否成立，不成立则$$M\ge 10^7$$,则取$$h=10^{-8}$$，看上式得值是否变为原来的$$10^{-2}$$。如果是则，求导正确，不是则求导错误。  
####问题2：当数据量特别大的时候经典梯度下降法存在什么问题，需要怎么改进？  
因为经典的梯度下降是对整个训练集进行的，如果训练集特别大，每次更新要针对整个数据集，因此需要很大的计算力，计算很费时间。  
一般采用随机梯度下降发，每次训练一个数据。但是该方法很难收敛，会在极值点振荡，因此一般采用mini-batch Gradient Descent方法。一般batch的size m取2的整数次幂。  
为了充分利用数据以及避免数据的特定顺序给算法收敛带来的印象，一般每次遍历数据前，对数据先随机排序，然后每次取m个数据，直到遍历所有数据。有必要再按新规则排序，重复以上步骤。  
如何选择学习率$$\alpha$$，为了加快学习速度与提高精度。一开始取大的学习速率，误差曲线进入平台期后，再减小学习速率做更精细的调整。  
####问题3：深度学习中常用的优化方法是随机梯度下降法，但是SGD偶尔也会失效，无法给出满意的训练结果，这是为什么？
当SGD遇到山谷的时候，由于沿山谷方向的梯度比较小，因此结果会在山谷方向来回移动，而不能像整体梯度下降一样能沿着山谷有一个确定方向的移动。  其次是遇到平坦区域，有些方向的梯度接近于0 ，因此需要很长世间才能走出这片区域。    
####问题4：为了改进SGD，研究者都做了哪些改动？提出了哪些方法？他们各有哪些特点？
#####动量(Momentum)方法
为了解决SGD在山谷振荡，鞍点停滞的现象。我们引入了带动量的SGD。普通SGD是：  
$$\kern{8 em}\theta_{t+1}=\theta_t - \eta g_t$$
带动量的SGD公式是：  
$$\kern{8 em}v_{t}=\gamma v_{t-1} + \eta g_t$$  
$$\kern{8 em}\theta_{t+1}=\theta_t - v_t$$  
通过$$0 < \gamma < 1$$，能实现梯度的累积，也就是速度的累积，因此能更好的冲出山谷与平坦（平原）区域。  
#####AdaGrad方法
####问题5：L1正则化使得模型参数具有稀疏性的原理是什么？
