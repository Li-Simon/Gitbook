# CNN

CNN主要是处理空间数据，比如图像数据。CNN主要包含LetNet-5,AlexNet,GoogLeNet,VGG,Residual Net,Dense Net。  
CNN核心：局部感受野，权值共享，时间或空间亚采样这三种思想来保证某种程度的平移，尺度，形变不变性。
###CNN基础
#### [如何理解CNN中的卷积？](https://blog.csdn.net/cheneykl/article/details/79740810)

#### CNN相对于全连接网络的好处

减少模型参数数量，减少计算时间。

#### CNN的卷积核与特征提取

[CNN中卷积核数目以及参数数据的计算](https://blog.csdn.net/yanzi6969/article/details/78019683)。根据的是相邻两层feature maps个数，以及每个map的大小来计算出所用卷积核的数目。

#### CNN的卷积核大小为啥是奇数?

不能保持对称性，比如原来图像长度宽是MxM，卷积核大小是nxn，则卷积作用后图像大小是\(M-n+1\)x\(M-n+1\)，如果n是偶数，则n-1是奇数，因此处理后的图像不再对称了。  
在这里，我们将要计算模型中，两层之间的参数数目。

#### [如何计算CNN两层之间参数数目](https://blog.csdn.net/yanzi6969/article/details/78019683)

![](/assets/LeNet-5.png)  
C1层是卷积层，单通道下用了6个卷积核，这样就得到了6个feature map，其中每个卷积核的大小为5\*5，用每个卷积核与原始的输入图像进行卷积，这样feature map的大小为\(32-5+1\)×\(32-5+1\)= 28×28，所需要的参数的个数为\(5×5+1\)×6= 156\(其中5×5为卷积模板参数，1为偏置参数\)，连接数为\(5×5+1\)×28×28×6=122304\(其中28×28为卷积后图像的大小\)。

S2层为 pooling 层，也可以说是池化或者特征映射的过程，拥有6个 feature map，每个feature map的大小为14\*14，每个feature map的隐单元与上一层C1相对应的feature map的 2×2 单元相连接，这里没有重叠。计算过程是：2×2 单元里的值相加然后再乘以训练参数w，再加上一个偏置参数b\(每一个feature map共享相同w和b\)，然后取sigmoid 值，作为对应的该单元的值。所以S2层中每 feature map 的长宽都是上一层C1的一半。S2层需要2×6=12个参数，连接数为\(4+1\)×14×14×6 = 5880。注：这里池化的过程与ufldl教程中略有不同。下面为卷积操作与池化的示意图：

![](/assets/Conv_Pooling.png)

计算CNN相邻两层之间参数数目，假设上层有大小是NxMxM，其中n是通道数目，MxM是图像大小。如果连接下一层的卷积核大小是nxn，我们可以对上一层的a个通道做卷积，则每次操作需要a个卷积核，再加上一个偏置，则需要axnxn+1个参数，假设下一层有K个通道，则两层之间所有的参数数目是K\(axnxn+1\)。这里a可以取不同值，也可以是不同值的组合，比如在LeNet-5中，S2到C3之间就存在不同通道的组合，C3的前6个特征图以S2中3个相邻的特征图子集为输入。接下来6个特征图以S2中4个相邻特征图子集为输入。然后的3个以不相邻的4个特征图子集为输入。最后一个将S2中所有特征图为输入。这样C3层有1516（6_（3\_25+1）+6_（4_25+1\)+3_\(4\_25+1）+\(25\_6+1\)=1516\)个可训练参数和151600（10\_10\*1516=151600）个连接。

#### CNN的pooling的作用
降维与信息提取。

