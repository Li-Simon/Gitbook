# 第四章 最优化方法

这章主要讨论常见的非线性最优化方法，以及优化技巧与不同方法的优缺点。  
##无约束与有约束问题
最优化问题分为有约束优化问题与无约束优化问题。
无约束优化问题
  minf(x)
有约束优化问题
   minf0(x)
   s.t. fi(x)≤0, i=1,2,...,m
   s.t. hi(x)=0, i=1,2,...,p
但是我们可以通过拉格朗日乘子转化为无约束优化问题
   minL(x,λ,v)=f0(x)+∑i=1mλifi(x)+∑i=1pvihi(x)

##优化搜索方式
主要有两大类方法：
1. 线性搜索:先确定方向，再确定步长，典型的就是梯度下降法与牛顿法，qusai-netwon法
2. 置信域方法：先确定步长，再找步长方向。常见的有Gauss-Netwon法，L-M，Dog-Leg方法 

##利用导数的阶数
从应用的泰勒展开的阶数来说，可分为一阶算法与二阶算法。

1. 一阶方法
   1. SGD
   2. Mini-batch SGD
2. 二阶方法
   1. 牛顿法
   2. 拟牛顿法
      1. BFGS
      2. DFP
      3. Brody类方法

##问题凸与否
就解决的问题而言，有凸优化算法与非凸优化算法。对凸优化问题而言，由于局部极小就是全局最小，因此通过梯度下降方法就可以很好的解决。大多数我们面对的是非凸优化问题，存在大量的局部极小值，我们也不需要得到全局最小值，只需要求得一个较好的局部极小值就可以了，因为局部极小值对应的解是稳定的，就可能是一个满足系统要求的解。    
1. 全局最小算法
   1. 模拟退火
   2. 遗传算法
 
##本章写作方式
思考这一章该怎么去写？但是得先回答以下两个问题，你写这一章的目的是什么？你想把它写成什么样子？答案不能独立于问题而存在，答案与问题是一对，\[Key,Value\],且一个key可以对于与多个value。  
本章写作的目的是为了解决实际的问题，实际的问题一般分为无约束优化问题与带约束问题优化问题，因此本章的分类是就问题是否带约束来分的。  
带约束的优化问题也可以细分，看求解的问题线性而是非线性，非为带约束线性优化问题，比如普通的SVM，带约束的非线性优化问题\(非线性优化是相对于线性规划而言的，除了线性规划以外的优化问题称为非线性优化问题\)，比如核函数SVM。因此优化问题可以分为如下三类：

### 1. 线性规划

线性规划暂时不讲，因为现在主要讨论非线性优化问题，此外可以通过平方把线性优化问题转化为非线性优化问题，一个例子就是求解线性方程组问题转化为最小二乘问题。

### 2. 无约束非线性优化问题

一维搜索问题，包含二分法，弦割法，牛顿法,\(黄金分割法\)这些也不重用，因此主要讨论如下几种算法。
1. 梯度下降法
2. 牛顿法
3. 拟牛顿法
4. L-M\(Levenberg-Marquardt\)算法
5. DogLeg算法
6. 共轭梯度法

## 3. 带约束非线性优化问题
原则就是通过拉格朗日乘子法把等式约束，用KKT乘子法把不等式约束问题转化为无约束优化问题来求解，这样可以利用无约束优化问题来求解。
主要有如下几种方法
1. 拉格朗日乘子法
2. 罚函数法
3. 障碍函数法

### 不等式约束的KKT乘子法

1. 局部极小算法
   1. 一阶方法
      1. SGD
      2. Mini-batch SGD
   2. 二阶方法
      1. 牛顿法
      2. 拟牛顿法
         1. BFGS
         2. DFP
         3. Brody类方法
   3. Trust Region method
      1. L-M算法
      2. DogLeg算法
   4. 线性搜索法
   5. 凸优化
   6. 非凸优化
2. 全局最小算法
   1. 模拟退火
   2. 遗传算法

## 需要做的事情

1. 把最前沿的非凸优化，比如ADMM算法加进来
2. 分布式机器学习中应用的优化算法要有清晰的轮廓，以及她与单机机器学习系统中的优化算法有啥区别。因此，是否有必要加一个section，关于分布式机器学习优化方法？
3. 运筹学，线性规划，非线性规划里面处理的基本东西要在你的脑海中有一个基本的印象。
4. 有必要加一节对偶。

[^1]

[^1]: 刘铁岩 《分布式机器学习--算法，理论与实战》

