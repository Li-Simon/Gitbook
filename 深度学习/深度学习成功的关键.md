# 深度学习成功的关键

## 神经网络成功的原因[^1]

1. 有逐层的处理 
2. 有特征的内部变化  
3. 有足够的模型复杂度   

### 为什么深度学习现在才流行

模型的复杂度对应的就是模型的学习能力。越复杂的模型，学习能力会越强。哪为什么以前我们不用复杂的模型，现在却用了？主要有如下三点：  
1. 我们今天有更大的数据  
2. 有强大的计算设备  
3. 有很多有效的训练技巧，比如逐层训练

这就解释了为啥以前不用复杂的神经网络，而现在用了。此外，由带隐藏层神经网络的万有逼近定理，可以知道神经网络的强大学习能力。  
 深度学习相对于以前的机器学习的优点是，以前需要我们手动的设计特征，现在深度学习的端对端学习把特征学习和分类器学习器联系在一起了。

### 深度学习的逐层抽象

这里主要讨论CNN。  
只需要一层隐藏层的神经网络也可以逼近任何连续函数，哪为啥需要深度神经网络呢？这就是浅层神经网络不能实现的逐层抽象。深度神经网络学到的特征是分层的，比如最常见的图像识别中，最底层的是像素特征，再上一层是边特征，再上层是由边组成的局部轮廓，最上面是全局特征。逐层抽象是深度神经网络相对于浅层神经网络的一大优势。  
其实我自己对逐层抽象的理解就是从表示方式来说，逐层抽象需要的神经元会远小于单隐藏层的神经网络。比如要表示一幅$$n\times n$$的图像,单隐藏层的神经网络需要$$n^2$$个神经元。  而如果是双层网络的话，我们可以做类似于PCA或者SVD的操作，一层表示X轴对应的隐变量成分，一层表示Y轴的隐变量成分，因此，我们需要的神经元数目只是正比于n而不是当隐藏层的$$n^2$$。因此，逐层抽象能简化模型的复杂度。  
一个问题是怎么用神经网络来做SVD？也就是做推荐。  
当然，你或许也知道一些机器学习方法也有类似的逐层抽象的模型，比如Boosting模型，他们为啥没有取得深度学习的成功？其原因就是，第一，复杂度还不够，第二，更关键的是，它始终在原始空间里面做事情，所有的这些学习器都在原始特征空间，中间没有进行任何特征变换。原始空间的理解就是，拿图像识别来说，它只是通过决策树来一层层的逼近目标，通过信息熵或者基尼系数来做决策，它不会去对图像做抽象，只有像素级别的逼近。

### 深度学习成功的关键

主要包含这么几点：  
1. 逐层的处理，也就是逐层抽象  
2. 有一个内部的特征变换  
3. 有足够的复杂度

### 深度学习能够成功的领域

就目前而言，深度学习在图像，视频，语音方面取得巨大的成功，而在其他问题上，比如Kaggle上的机票，订旅馆，推荐方面，还是传统机器学习方法占主导。若要总结深度学习能在哪些领域/问题方面能够成功，那就是处理的这些对象，是由一些很底层的结构的，这些底层的结构是在我们处理的任务上市通用的，也就是局部的时间平移不变性；  
对于图像而言，图像的底层的边缘特征很难随时间而变。对语音，我们处理的也是一段时间，一个地区的语音任务。对于不同地区，就涉及到方言的问题，恐怕就没有通用的模型了，我们必须对不同的语言，不同的方言训练不同的模型。  
人对于推荐系统，这些对象，就是人类的喜好，很容易随着时间而变迁，因此很难有通用的模型。对于这些问题，现在深度学习并没有比传统机器学习，比如Random Forest, GBDT强，是否将来的神经网络可以比这些传统的机器学习方法好？还是机器学习方法根本上就很难胜任这些问题，探索这些更本质的问题，是我们需要重视的一个方向，这个问题没有弄清楚，那么我们把深度学习用在这些问题，可能就是走在一条错误的路上。

![](/assets/DL_abstract_featuer.png)

### 深度学习的缺点

1. 参数调节很难，在不同任务，比如跨图像与语音就需要重新调节参数。    
2. 结果重复性差。数据与方法相同，超参数不一样，结果就不一样。  

### 一些要做的事情

深度神经网络是建立在可微函数与BP算法的基础之上的，那么能不能用不可微的函数把神经网络做的更深呢？这时就不能用BP算法来训练了。  
现实的很多问题就不是可微的，因此我们需要发展非可微的深层网络。此外，我们还可以借鉴几十年来机器学习的成功模型，作为构建深度模型的基础，这些模型中有一些就不是可微的。通过这样来构建出能在当前深度学习效果不如传统机器学习的问题上比机器学习更胜一筹。

#### gcForest

比如周志华gcForest，它是一个基于树模型的方法，主要是借用集成学习的很多想法。其次在很多不同的任务上，它的模型得到的结果和深度神经网络是高度相似的，除了一些大规模的图像等等。在其他的任务上，特别是跨任务表现非常好，我们可以用同样一套参数，用在不同的任务中得到不错的性能，就不需要逐任务的慢慢调参数。

### 训练CNN的技巧

1. 优化方面的
   1. BP算法
   2. SGD, Momentum, Adam系列
2. 减小模型参数
   1. Pooling, 
3. 特征提取的
   1. 卷积
   2. 稀疏连接，权值共享
4. 防止过拟合的
   1. 正则化方法
      1. L1/L2
      2. 提前终止
   2. 扩大数据量方法
      1. 数据增强：旋转，对折，对比度减弱/模糊化，对比度增强，加噪声
   3. 模型集成方法
      1. Bagging，结合多个模型得到模型平均，减少泛化误差
      2. Dropout
5. 防止梯度消失的
   1. ReLU
   2. Batch Normalization

[^1]:  周志华：满足这三大条件，可以考虑不用深度神经网络 [https://36kr.com/p/5129474.html](https://36kr.com/p/5129474.html)

