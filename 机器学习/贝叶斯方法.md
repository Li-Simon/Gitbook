# 贝叶斯方法

## 贝叶斯公式

首先我们讨论贝叶斯公式，以及先验分布于后验分布[^1]。  
  $$p(\mathbf{\theta|X}) = \frac{p(\mathbf{X|\theta})p(\mathbf{\theta})}{p(\mathbf{X})}$$  
其中：  
$$p(\mathbf{\theta})$$是先验分布，$$p(\mathbf{X|\theta})$$是似然值，$$p(\mathbf{\theta|X})$$是后验分布。在贝叶斯体系中，模型参数$$\mathbf{\theta}$$不再认为是固定不变的量，而是服从一定分布的随机变量。在没有数据支持的情况下，我们对其有一个假设性的分布$$p(\mathbf{\theta}$$,这个分布称为先验分布，而在观测到数据集$$X=[x_1,...,x_n]$$以后，根据数据集上表现出来的似然值$$p(\mathbf{X|\theta})$$,可以得到调整后的后验分布$$p(\mathbf{\theta|X})$$。  
概率统计模型有两个常见的任务:一是参数估计，二是预测，也就是在给定一组训练数据X，评估某新的观测数据o的概率。  

| 模型估计方法 | 参数估计 | 预测 |
| :--- | :--- | :--- |
| 最大似然估计 |  |  |
| 贝叶斯方法 |  |  |
| 最大后验概率方法 |  |  |

## 朴素贝叶斯

朴素贝叶斯是基于特征条件独立假设。输入变量中所有特征之间是独立的因此，联合条件概率是各个条件概率的乘积。  
输入：$$X \in R^n$$  
输出： $$Y \in (1,2,...,K)$$  
条件概率：  $$P(X=x|Y=C_k) = P(X^{(1)} = x^{(1)},X^{(2)} = x^{(2)},...,X^{(n)} = x^{(n)}|Y=C_k)$$,  k=1,2,...,K

### 条件独立假设：

$$P(X=x|Y=y) = P(X=x|Y=C_k) = P(X^{(1)} = x^{(1)},X^{(2)} = x^{(2)},...,X^{(n)} = x^{(n)}|Y=C_k)$$  
  $$ = \displaystyle \prod _{j=1}^nP(X^{(j)} = x^{(j)}|Y=C_k)$$  
在上面的公式中，先验概率计算如下：  
  $$P(Y=C_k) = \frac{\displaystyle \sum _{i=1}^N I(y_i = c_k)}{N}, k=1,2...,K$$

### 条件概率计算

假设第j个特征$$x^{(j)}$$的可能取值集合是$$(a_{j1},a_{j2},...,a_{js_j})$$，条件概率计算如下：  
  $$P(X^{(j)} = a_{jl}|Y=C_k) = \frac{\displaystyle \sum _{i=1}^N I(x_i^{(j)} = a_{jl},y_i = c_k)}{\displaystyle \sum _{i=1}^N I(y_i = c_k)}$$  
  $$j=1,2,...,n; l =1,2,...,S_j; k=1,2...,K$$

### 后验概率计算

基于贝叶斯定理计算后验概率：  
  $$P(Y=C_k|X=x) = \frac{P(X = x|Y=C_k)P(Y=C_k)}{\sum_k P(X = x|Y=C_k)P(Y=C_k)}$$  
把条件独立假设代入可以得到：  
  $$P(Y=C_k|X=x) = \frac{P(Y=C_k)\displaystyle \prod _{j=1}^nP(X^{(j)} = x^{(j)}|Y=C_k)}{\sum_k P(Y=C_k)\displaystyle \prod _{j=1}^nP(X^{(j)} = x^{(j)}|Y=C_k)}$$  
因为分母对不同的类别k都是一样的，因此只需要求分子就可以了。 计算不同的类别k对应的值，分类结果对应概率最大的。  
因此朴素贝叶斯可以表示为：  
  $$y = f(x) = arg max_{c_k} P(Y=C_k)\displaystyle \prod _{j=1}^nP(X^{(j)} = x^{(j)}|Y=C_k)$$

### Laplace Smoothing

在实际计算条件概率的时候，或出现0的情况，这会影响后面的后验概率的计算。一般通过Laplace Smoothing来处理。  
  $$P(X^{(j)} = a_{jl}|Y=C_k) = \frac{\displaystyle \sum _{i=1}^N I(x_i^{(j)} = a_{jl},y_i = c_k) + \lambda}{\displaystyle \sum _{i=1}^N I(y_i = c_k) + s_j \lambda}$$  
$$\lambda > 0, j=1,2,...,n; l =1,2,...,S_j; k=1,2...,K$$

基本上到此，朴素贝叶斯的介绍也就完结了，朴素贝叶斯不需要计算任何参数，计算简单，缺点是分类性能不一定高。

### 极大似然估计

极大似然估计是试图在所有的模型参数可能取值中，找到一个能使得数据出现的可能性最大的值。

## 贝叶斯网络

[^1]: 《计算广告》刘鹏  p166

