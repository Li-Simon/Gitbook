#机器学习原理
##偏差，方差，噪声
偏差-方差分解可以用来对学习算法的期望泛化错误率进行拆分。同时，偏差-方差分解为我们设计模型与分析提供了一个比较清晰的方向。  
假设测试样本是$$\mathbf{x}$$，令$$\mathbf{y_D}$$为$$\mathbf{x}$$在数据集上的标记(可能存在标记错误的情况)，y为$$\mathbf{x}$$的真实标记，$$f(\mathbf{x};D)$$为训练集D上学得模型f在$$\mathbf{x}$$上的预测输出，以回归模型为例，学习算法的期望预测为：  
$$\kern{4 em} \hat f(\mathbf{x}) = E_D[f(\mathbf{x},D) ]$$  
方差是：    
$$\kern{4 em} var(\mathbf{x}) = E_D[f(\mathbf{x},D) - \hat f(\mathbf{x})]^2$$  
噪声是：  
$$\kern{4 em} \epsilon ^2= E_D[y_D - y]^2$$    
输出期望与真实标记之间的差别成为偏差（Bias），即：  
$$\kern{4 em} bias ^2(\mathbf{x}))= E_D[\hat f(\mathbf{x}) - y]^2$$   
为方便讨论，假设噪声的期望为0； 即：$$E_D[y_D - y] = 0$$. 下面来对模型的期望泛化误差进行分解：    
$$\kern{4 em} E(f;D) = E_D[(f(\mathbf{x},D) - y_D)^2]$$  
$$\kern{8 em} = E_D[(f(\mathbf{x},D) - \hat f(\mathbf{x})+ \hat f(\mathbf{x})- y_D)^2]$$  
$$\kern{8 em} = E_D[(f(\mathbf{x},D) - \hat f(\mathbf{x}))^2]+ E_D[(\hat f(\mathbf{x})- y_D)2] + E_D[2(f(\mathbf{x},D) - \hat f(\mathbf{x}))(\hat f(\mathbf{x})- y_D)]$$   
$$\kern{8 em} = E_D[(f(\mathbf{x},D) - \hat f(\mathbf{x}))^2]+ E_D[(\hat f(\mathbf{x})- y_D)^2]$$   
$$\kern{8 em} = E_D[(f(\mathbf{x},D) - \hat f(\mathbf{x}))^2]+ E_D[(\hat f(\mathbf{x})- y + y -y_D)^2]$$   
$$\kern{8 em} = E_D[(f(\mathbf{x},D) - \hat f(\mathbf{x}))^2]+ E_D[(\hat f(\mathbf{x})- y)^2] + E_D[(y -y_D)^2] + E_D[2(\hat f(\mathbf{x})- y)(y-y_D)]$$   
$$\kern{8 em} = E_D[(f(\mathbf{x},D) - \hat f(\mathbf{x}))^2]+ E_D[(\hat f(\mathbf{x})- y)^2] + E_D[(y -y_D)^2] $$ 
因此就得到：
$$\kern{4 em} E(f;D)= bias^2(\mathbf{x}) + var(\mathbf{x}) + \epsilon ^2 $$ 
也就是泛化误差分成偏差，方差与噪声之和。我们设计模型分析时，可以从这三方面去考虑。  

###噪声
为了消除噪声的影响，我们需要对数据进行清洗，进行预处理。在很大程度上就是为了得到更干净的数据。  
###Bias
偏差小，说明的是模型很准，可能有过拟合的倾向，增加模型的复杂度可以使得bias减小。但是泛化能力变差，也就是variance会大，模型对数据很敏感。 如果模型bias大，可以通过对简单的模型进行boost，来提升模型的准确性，也就是Boost系列算法做的事情,如：AdaBoost, GBDT, XGBoost。

###方差
方差小，说明模型很稳定，也就是说模型的泛化能力好，可能测试集上的数据相对于训练集来说有一些变化，但是也不影响模型的输出结果，此时，可能模型欠拟合，因此泛化能力好。如果模型variance大，
可以通过训练多个模型，并让各个模型之间的关联性很小，通过求平均来减小Variance，这就是集成模型中bagging系列算法做的事情，如Bagging,Random Forest.
##最大似然函数
输入：$$X \in R^n$$  
输出： $$Y \in (1,2,...,K)$$  
条件概率：  $$P(X=x|Y=C_k) = P(X^{(1)} = x^{(1)},X^{(2)} = x^{(2)},...,X^{(n)} = x^{(n)}|Y=C_k)$$,  k=1,2,...,K  
假设有N个样本点$$(X_i,Y_i), i=1,2,...,N$$  
条件概率： $$P(Y=y|X=x,\mathbf{\theta})$$，其中$$\mathbf{\theta}$$是模型的参数。  
似然函数定义为：每个样本点发生概率的乘积： 
$$L(\mathbf{\theta}) = \displaystyle \prod _{n=1}^NP(Y=Y_n|X=X_n,\mathbf{\theta})$$  
我们需要求得一个模型，使得在所有样本点在该模型发生的几率极大，几率是联合几率，是每个样本发生几率的乘积。也就是极大化似然函数。  
分类与回归问题似乎都可以转化成求解似然函数。  
###用极大似然函数求解回归问题
输入：$$\mathbf{X} \in R^n$$  
输出： $$Y \in R$$  
我们的模型是$$f(\mathbf{X}, \mathbf{\Theta})$$,其中$$\mathbf{\Theta}$$是模型参数，X是输入变量。
假设我们的损失函数是平方误差，因此我们的cost function可以表示如下：  
$$\kern{4 em} L(\mathbf{X},\mathbf{\Theta}) = \frac{1}{2}\displaystyle \sum _{n=1}^N(f(X_n, \mathbf{\Theta}) - Y_n)^2$$  
等价于：  
$$\kern{4 em} e^{L(\mathbf{X},\mathbf{\Theta})}= e^{\displaystyle \prod _{n=1}^N \frac{1}{2}(f(X_n, \mathbf{\Theta}) - Y_n)^2} = \displaystyle \prod _{n=1}^Ne^{ \frac{1}{2}(f(X_n, \mathbf{\Theta}) - Y_n)^2}$$   
实际上，最小化$$L(\mathbf{X},\mathbf{\Theta})$$等价于最小化$$e^{L(\mathbf{X},\mathbf{\Theta})}$$,等价于最大化$$e^{-L(\mathbf{X},\mathbf{\Theta})}$$,也就是等价于最大化
$$\Gamma(\Theta) = \displaystyle \prod _{n=1}^Ne^{ -\frac{1}{2}(f(X_n, \mathbf{\Theta}) - Y_n)^2} = \displaystyle \prod _{n=1}^N P(X_n, \Theta)$$  
其中
$$\kern{4 em} P(X_n, \Theta) = e^{ -\frac{1}{2}(f(X_n, \mathbf{\Theta}) - Y_n)^2}$$  
定义成回归模型中样本$$(X_n, Y_n)$$发生的几率，这样我们就把回归问题与几率联系起来了。我们把求解损失函数最小，转化成极大化似然函数的求解。  
对于不同的损失函数，我们都可以转化成对应的概率。  
对于分类问题，怎么用一个统一的框架来解析？可以是基于概率的。
##最大熵原理
##奥卡姆剃刀













































































































































































































