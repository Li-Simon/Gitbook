# 决策树与集成学习

1. 决策树
   1. ID3
   2.  C4.5
   3.  CART
   4.  Decision Stump
2. 集成学习 
   1. Bagging 
      1. 随机森林 
   2. Boost 
      1. Adaboost 
      2. GBDT 
      3. XGBoost 
      4. LightGBM
      
      
#Ensemble learning（团体学习）
利用多个学习方法来获得更好的预测能力，机器学习中的系综学习的样本是有限的，但是统计力学中的系综方法其样本是无限的。
机器学习中的系综方法包括Bagging,boosting
#Bagging算法（Bootstrap aggregating）
[参考](https://www.cnblogs.com/maybe2030/p/4652492.html)
 是一种团体学习方法,可以看成事一种圆桌会议，或者投票选举的形式，其思想是“群众的眼光是雪亮的”，可以训练多个模型，之后将这些模型进行加权组合，一般这类方法的效果，都会好于单个模型的效果。 在实践中， 在特征一定的情况下，大家总是使用Bagging的思想去提升效果。
算法步骤
给定一个大小为n的训练集D，Bagging算法从中均匀、有放回地（即使用自助抽样法）选出m个大小为$$n'$$的子集$$D_{i}$$，作为新的训练集。在这m个训练集上使用分类、回归等算法，则可得到m个模型，同一个训练集中的成员可以有重重复，再通过取平均值、取多数票等方法，即可得到Bagging的结果。
#Boosting
在Bagging方法中，我们假设每个训练样本的权重都是一致的； 而Boosting算法则更加关注错分的样本，越是容易错分的样本，约要花更多精力去关注。对应到数据中，就是该数据对模型的权重越大，后续的模型就越要拼命将这些经常分错的样本分正确。 最后训练出来的模型也有不同权重，所以boosting更像是会整，级别高，权威的医师的话语权就重些。
训练：先初始化每个训练样本的权重相等为1/d，d为样本数量； 之后每次使用一部分训练样本去训练弱分类器，且只保留错误率小于0.5的弱分类器，对于分对的训练样本，将其权重 调整为 error(Mi)/(1-error(Mi)) ，其中error(Mi)为第i个弱分类器的错误率（降低正确分类的样本的权重，相当于增加分错样本的权重）；

　　测试：每个弱分类器均给出自己的预测结果，且弱分类器的权重为log(1-error(Mi))/error(Mi) ) 权重最高的类别，即为最终预测结果。

　　在adaboost中，弱分类器的个数的设计可以有多种方式，例如最简单的就是使用一维特征的树作为弱分类器。

　　adaboost在一定弱分类器数量控制下，速度较快，且效果还不错。

　　我们在实际应用中使用adaboost对输入关键词和推荐候选关键词进行相关性判断。随着新的模型方法的出现， adaboost效果已经稍显逊色，我们在同一数据集下，实验了GBDT和adaboost，在保证召回基本不变的情况下，简单调参后的Random Forest准确率居然比adaboost高5个点以上，效果令人吃惊。。。。

　　Bagging和Boosting都可以视为比较传统的集成学习思路。 现在常用的Random Forest，GBDT，GBRank其实都是更加精细化，效果更好的方法。 后续会有更加详细的内容专门介绍。



