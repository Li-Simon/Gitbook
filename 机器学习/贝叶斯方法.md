# 贝叶斯方法

## 贝叶斯公式

首先我们讨论贝叶斯公式，以及先验分布于后验分布[^1]。  
  $$p(\mathbf{\theta|X}) = \frac{p(\mathbf{X|\theta})p(\mathbf{\theta})}{p(\mathbf{X})}$$  
其中：$$p(\mathbf{\theta})$$是先验分布，$$p(\mathbf{X|\theta})$$是似然值，$$p(\mathbf{\theta|X})$$是后验分布。在贝叶斯体系中，模型参数$$\mathbf{\theta}$$不再认为是固定不变的量，而是服从一定分布的随机变量。在没有数据支持的情况下，我们对其有一个假设性的分布$$p(\mathbf{\theta}$$,这个分布称为先验分布，而在观测到数据集$$X=[x_1,...,x_n]$$以后，根据数据集上表现出来的似然值$$p(\mathbf{X|\theta})$$,可以得到调整后的后验分布$$p(\mathbf{\theta|X})$$。我们想要的求得的模型参数，要使得观测数据集出现的概率极大。这就是求参数$$\theta$$的极大似然方法。通常通过迭代方法来求极大化$$L(\theta|X)$$，比如EM算法。         
概率统计模型有两个常见的任务:一是参数估计，二是预测，也就是在给定一组训练数据X，评估某新的观测数据o的概率。  
###若干常见模型估计方法
| 模型估计方法 | 参数估计 | 预测 |
| :--- | :--- | :--- |
| 最大似然估计 | $$\mathbf{\hat \theta}_X^{ML}= \arg \min_{\theta}p(\mathbf{X|\theta})$$| $$p(o|(\mathbf{X})=p(o|\mathbf{\hat \theta}_X^{ML})$$ |
| 贝叶斯方法 |  $$p(\mathbf{\theta|X})=p(\mathbf{X|\theta})p(\mathbf{\theta})$$| $$p(o|(\mathbf{X})= \int p(o|\mathbf{\theta})p(\mathbf{\theta|X})d\mathbf{\theta}$$ |
| 最大后验概率方法 |$$\mathbf{\hat \theta}_X^{MAP}= \arg \min_{\theta}p(\mathbf{\theta|X})$$| $$p(o|(\mathbf{X})=p(o|\mathbf{\hat \theta}_X^{MAP})$$ |

## 朴素贝叶斯

朴素贝叶斯是基于特征条件独立假设。输入变量中所有特征之间是独立的因此，联合条件概率是各个条件概率的乘积。  
输入：$$X \in R^n$$  
输出： $$Y \in (1,2,...,K)$$  
条件概率：  $$P(X=x|Y=C_k) = P(X^{(1)} = x^{(1)},X^{(2)} = x^{(2)},...,X^{(n)} = x^{(n)}|Y=C_k)$$,  k=1,2,...,K

### 条件独立假设：

$$P(X=x|Y=y) = P(X=x|Y=C_k) = P(X^{(1)} = x^{(1)},X^{(2)} = x^{(2)},...,X^{(n)} = x^{(n)}|Y=C_k)$$  
  $$ = \displaystyle \prod _{j=1}^nP(X^{(j)} = x^{(j)}|Y=C_k)$$  
在上面的公式中，先验概率计算如下：  
  $$P(Y=C_k) = \frac{\displaystyle \sum _{i=1}^N I(y_i = c_k)}{N}, k=1,2...,K$$

### 条件概率计算

假设第j个特征$$x^{(j)}$$的可能取值集合是$$(a_{j1},a_{j2},...,a_{js_j})$$，条件概率计算如下：  
  $$P(X^{(j)} = a_{jl}|Y=C_k) = \frac{\displaystyle \sum _{i=1}^N I(x_i^{(j)} = a_{jl},y_i = c_k)}{\displaystyle \sum _{i=1}^N I(y_i = c_k)}$$  
  $$j=1,2,...,n; l =1,2,...,S_j; k=1,2...,K$$

### 后验概率计算

基于贝叶斯定理计算后验概率：  
  $$P(Y=C_k|X=x) = \frac{P(X = x|Y=C_k)P(Y=C_k)}{\sum_k P(X = x|Y=C_k)P(Y=C_k)}$$  
把条件独立假设代入可以得到：  
  $$P(Y=C_k|X=x) = \frac{P(Y=C_k)\displaystyle \prod _{j=1}^nP(X^{(j)} = x^{(j)}|Y=C_k)}{\sum_k P(Y=C_k)\displaystyle \prod _{j=1}^nP(X^{(j)} = x^{(j)}|Y=C_k)}$$  
因为分母对不同的类别k都是一样的，因此只需要求分子就可以了。 计算不同的类别k对应的值，分类结果对应概率最大的。  
因此朴素贝叶斯可以表示为：  
      $$y = f(x) = \arg \max_{c_k} P(Y=C_k)\displaystyle \prod _{j=1}^nP(X^{(j)} = x^{(j)}|Y=C_k)$$

### Laplace Smoothing

在实际计算条件概率的时候，或出现0的情况，这会影响后面的后验概率的计算。一般通过Laplace Smoothing来处理。  
  $$P(X^{(j)} = a_{jl}|Y=C_k) = \frac{\displaystyle \sum _{i=1}^N I(x_i^{(j)} = a_{jl},y_i = c_k) + \lambda}{\displaystyle \sum _{i=1}^N I(y_i = c_k) + s_j \lambda}$$  
$$\lambda > 0, j=1,2,...,n; l =1,2,...,S_j; k=1,2...,K$$

基本上到此，朴素贝叶斯的介绍也就完结了，朴素贝叶斯不需要计算任何参数，计算简单，缺点是分类性能不一定高。


### 极大似然估计

极大似然估计是试图在所有的模型参数可能取值中，找到一个能使得数据出现的可能性最大的值。

## 贝叶斯学派与频率学派
以上帝抛N次硬币为例，统计出现正面朝上的次数与概率：  
频率学派：认为上帝只有一枚硬币，硬币出现朝上的概率等于当抛的次数N趋于无穷的时候，出现的正面朝上的次数n除以N。频率学派认为上帝只有一枚硬币。  
贝叶斯学派:认为上帝有无限多的硬币，所有这些硬币出现正面朝上的概率p满足一个分布P(p)，就是先验分布。每一组N个数据是一枚硬币抛出来的，也就是一组数据对应一个模型，这个模型由p决定。当出现下一组数据的时候，上帝就换了另外一枚硬币，也就是另外一个模型。  

[^1]: 《计算广告》刘鹏  p166

