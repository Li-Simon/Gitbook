# 循环神经网络问题

建模序列化数据的一种主流深度学习模型。
##设计目的
捕获长距离输入之间的依赖。
###### 背景：

传统的前馈神经网络一般输入的都市一个定长的向量，无法处理变长的序列信息，即使通过一些方法把序列处理成定长的向量，模型也很难捕捉序列中的长距离依赖关系。RNN则通过将神经元串行起来处理序列化的数据。由于每个神经元能用它的内部变量保存之前输入的序列信息，因此整个序列被浓缩成抽象的表示，并可以据此进行分类或生成新的序列。近年来，得益于计算能力的大幅度提升和模型的改进，RNN在很多领域取得了突破性的进展--机器翻译，图像描述，推荐系统，智能聊天机器人，自动作词作曲等。

#### 处理文本数据时，循环神经网络与前馈神经网络相比有什么特点？

Softmax function:  
$$\kern{8 em} softmax(\mathbf{x}_i) = \frac{\exp(x_i)}{\displaystyle \sum_{j=1}^n \exp(x_j)}$$

### 展开图计算

![](/assets/RNN_Unfold_map.png)  
当前神经元的输入是，系统上一个的状态$$h^{(t-1)}$$，以及外部信号$$x^{(t)}$$,因此当前神经元的输出状态是：  
$$\kern{8 em} h^{(t)} = f(h^{(t-1)},x^{(t)};\mathbf{\theta})$$  
可以看出，当前状态包含整个过去序列的信息。  
$$\kern{2 em}$$从图可以看出，RNN可以处理变长度的输入问题，这是它区别于CNN的地方，他可以用来处理文本与语音问题。在最后的输入 $$h^{(T)}$$，我们可以再加一个softmax则可以用用来做文本分类任务。通过最小化损失误差(即输出的y和真实的类别之间的差距)，我们可以不断的训练网络，得到一个分类模型。相比于CNN等前馈网络，RNN由于具备对序列信息的刻画能力，往往能得到更准确的结果。  
相对于循环图，展开图有如下两个优点：  
\(1\)无论序列长度，学成的模型始终具有相同的输入大小；因为它指的是从一种状态到另外一种状态的转移，而不是在可变长度的历史状态上操作。  
\(2\)我们可以在每个时间步使用相同参数的相同转移函数f。  
这两个因素使得学习在所有时间步和所有序列长度上操作单一的模型f是可能的，而不需要在所有时间步学习独立的模型$$g^{(t)}$$.  
$$\kern{8 em} h^{(t)} =  g^{(t)}(x^{(t)},x^{(t-1)},...,x^{(2)},x^{(1)})$$  
$$ \kern{9 em}= f(h^{(t-1)},x^{(t)};\mathbf{\theta})$$

### 循环神经网络

循环神经网络中一些重要的设计模式包含如下几种：  
\(1\)每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络，如下图所示。  
RNN的输入到隐藏的连接由权值矩阵U参数化，隐藏到隐藏的循环由权重矩阵W参数化，隐藏到输出层的连接由权重矩阵V参数化。$$L^{(t)}$$是损失函数。  
![](/assets/RNN_General.png)  
前向传播公式：假设激活函数是tanh，我们有如下的更新方程：  
$$\kern{8 em} \mathbf{a}^{(t)} = \mathbf{b} + \mathbf{W}\mathbf{h}^{(t-1)} + \mathbf{U}x^{(t)}$$    
$$\kern{8 em} \mathbf{h}^{(t)} = \tanh(\mathbf{a})$$    
$$\kern{8 em} \mathbf{o}^{(t)} = \mathbf{c} + \mathbf{V}\mathbf{h}^{(t)} $$    
$$\kern{8 em} \mathbf{\hat y}^{(t)} = softmax(\mathbf{o}^{(t)})$$    
其中$$\mathbf{b},\mathbf{c}$$是偏置向量,$$\mathbf{U},\mathbf{V},\mathbf{W}$$是权重矩阵。  
这一循环网络将一个输入序列映射到相同长度的输出序列。与$$\mathbf{x}$$序列配对的$$\mathbf{y}$$的总的损失就是所有时间步的损失之和。例如，$${L}^{(t)}$$为给定的$$x^{(1)},x^{(2)},...,x^{(t)}$$后$$y^{(t)}$$的负对数似然，则：  
$$\kern{8 em} L((x^{(1)},x^{(2)},...,x^{(\tau)}),(y^{(1)},y^{(2)},...,y^{(\tau)})) = \displaystyle \sum_{t} L^{(t)}$$  
$$\kern{8 em} = -\displaystyle \sum_{t} \log p_{model}(y^{(t)}|(x^{(1)},x^{(2)},...,x^{(t)})$$  
其中$$p_{model}(y^{(t)}|(x^{(1)},x^{(2)},...,x^{(t)})$$需要读取模型输出向量$$\mathbf{\hat y}^{(t)}$$对应于$$\mathbf{y}^{(t)}$$的项。
应用于展开图且代价为$$O(\tau)$$的反向传播算法是通过时间反向传播(back-propagation through time, BPTT)。  
\(2\)每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络。如下图所示：  
该图中，RNN被训练将特定输出值放入o中，并且o是被允许传播到未来的唯一信息。此处没有从h前向传播的直接连接。之前的h仅通过产生的预测间接地连接到当前。o通常缺乏过去的重要信息，除非它非常高维并且内容丰富。这使得该图中的RNN不那么强大，但是它更容易训练，因为每个时间步可以与其他时间步分离训练，允许训练期间更多的并行化。  

![](/assets/RNN_DesignPattern_2.png)  
\(3\)隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络，如下图所示：  
这样的网络可以用于概括序列并产生用于进一步处理的固定大小的表示。在结束处可能存在目标，或者通过更下游模块的反向传播来获得输出$$o^{(t)}$$上的梯度。
![](/assets/RNN_DesignPattern_3.png)
这三种结构分别用于什么情况？  
###循环神经网络的梯度消失问题
RNN的求解可以采用BPTT(Back Propagation Through Time，基于事件的反向传播)算法实现。梯度消失问题导致RNN不能捕获长距离输入之间的依赖关系。  
传统的RNN梯度可以表示如下：  
$$\kern{4 em}\frac{\partial net_{t}}{\partial net_{1}} = 
\frac{\partial net_{t}}{\partial net_{t-1}}
\frac{\partial net_{t-1}}{\partial net_{t-2}}...
\frac{\partial net_{2}}{\partial net_{1}}$$  
其中：  
$$\kern{8 em}net_{t} = Ux_t + Wh_{t-1}$$  
$$\kern{8 em}h_{t} = Ux_t + f(net_t)$$  
$$\kern{8 em}y = g(Vh_t)$$  
$$\kern{4 em}\frac{\partial net_{t}}{\partial net_{t-1}}
=\frac{\partial net_{t}}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial net_{t-1}} = W diag[f'(net_{t-1})]$$    
$$\kern{7 em}= W_{ij}f'(net_{t-1}^{j})$$  
是一个nxn的矩阵，n是隐韩层$$net_{t-1}$$神经元的个数。  


