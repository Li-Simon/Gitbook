# 最优化算法

最优化问题分为有约束优化问题与无约束优化问题。  
无约束优化问题  
&emsp;&emsp;$$min f(x)$$  
有约束优化问题  
&emsp;&emsp; $$\min f_0(\mathbf{x})$$  
&emsp;&emsp; s.t. $$f_i\mathbf{(x) \le 0}$$, i=1,2,...,m   
&emsp;&emsp; s.t. $$h_i\mathbf{(x) = 0}$$, i=1,2,...,p     
但是我们可以通过拉格朗日乘子转化为无约束优化问题  
&emsp;&emsp; $$\min L(x,\lambda,v) = f_0(x) + \displaystyle \sum_{i=1}^m \lambda_if_i(x) + \displaystyle \sum_{i=1}^p v_i h_i(x)$$ 

主要有两大类方法：
1. 线性搜索:先确定方向，再确定步长，典型的就是梯度下降法与牛顿法，qusai-netwon法
2. 置信域方法：先确定步长，再找步长方向。常见的有L-M，Dog-Leg方法


[最速下降法，牛顿法，LBFGS](http://www.cnblogs.com/maybe2030/p/4751804.html)

##梯度下降法（Gradient Descent）

梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。
**梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，梯度趋于0， 所以步长越小，前进越慢。**
把损失函数展开到一阶。  
&emsp;&emsp;$$f(x + \alpha \mathbf{d} ) = f(x_0) + \alpha f'(x_0) \mathbf{d} + O({\alpha}^2)$$  
&emsp;&emsp;$$x_{t+1} = x_t -\alpha f'(x_t)$$,


梯度下降法的搜索迭代示意图如下图所示：

![](/assets/Algo_GradientDescent.png)

梯度下降法的缺点：

（1）越靠近极小值的地方收敛速度越慢，如下图所示

![](/assets/Algo_GradientDescentDisadv1.png)

从上图可以看出，梯度下降法在接近最优解的区域收敛速度明显变慢，利用梯度下降法求解需要很多次的迭代。

在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。

比如对一个线性回归（Linear Logistics）模型，假设下面的h\(x\)是要拟合的函数，J\(theta\)为损失函数，theta是参数，要迭代求解的值，theta求解出来了那最终要拟合的函数h\(theta\)就出来了。其中m是训练集的样本个数，n是特征的个数。  
&emsp;&emsp;$$h(\theta) = \sum^{n}_{j = 0}\theta_{k}x_{j} $$  
&emsp;&emsp;$$J(\theta) = \frac{1}{2m}\sum_{i = 0}^{m}(y^i - h_{\theta}(x^i))^2 $$  
//n是特征的个数, m的训练样本的数目

# 1\)批量梯度下降法（Batch Gradient Descent, BDG）

\(1\)将$$J(\theta)$$对$$\theta$$求偏导，得到每个$$\theta$$对应的梯度：  
&emsp;&emsp;$$\frac{\partial J(\theta)}{\partial \theta_{j}} = -\frac{1}{m}\sum_{i=1}^m(y^i - h_{\theta}(x^i))x_{j}^{i}$$  
\(2\)由于要最小化风险函数，所以按照每个参数$$\theta$$的负梯度方向来更新$$\theta$$  
&emsp;&emsp;$$\theta_{j}' = \theta{j} - \frac{\partial J(\theta)}{\partial \theta_{j}}  \theta{j} + \frac{1}{m}\sum_{i=1}^m(y^i - h_{\theta}(x^i))x_{j}^{i}$$

（3）从上面公式可以注意到，它得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果m很大，那么可想而知这种方法的迭代速度会相当的慢。所以，这就引入了另外一种方法——随机梯度下降。  
一个实验结果，说明随机梯度下降法能收敛到最终结果：

对于批量梯度下降法，样本个数m，x为n维向量，一次迭代需要把m个样本全部带入计算，迭代一次计算量为m\*n2。

# 2\)随机梯度下降（Stochastic Gradient Descent, SGD）

\(1\)上面的风险函数可以写成如下这种形式，损失函数对应的是训练集中每个样本的粒度，而上面批量梯度下降对应的是所有的训练样本

&emsp;&emsp;$$J(\theta) = \frac{1}{2m}\sum_{i = 0}^{m}(y^i - h_{\theta}(x^i))^2 $$
&emsp;&emsp;&emsp;&emsp;$$= \frac{1}{m}\sum_{i=1}^{m}cost(\theta,(x^i,y^i))$$  
&emsp;&emsp;$$cost(\theta,(x^i,y^i)) = \frac{1}{2}(y^i - h_{\theta}(x^i))^2$$  
\(2\)每个样本的损失函数，对$$\theta$$求偏导得到对应的梯度，来更新$$\theta$$  
&emsp;&emsp;$$\theta_{j}' = \theta_{j} + (y^i - h_{\theta}(x^i))x_{j}^{i}$$  
\(3\)随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将$$\theta$$迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。  
随机梯度下降每次迭代只使用一个样本，迭代一次计算量为n2，当样本个数m很大的时候，随机梯度下降迭代一次的速度要远高于批量梯度下降方法。两者的关系可以这样理解：随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。

# 比较批量梯度下降法与随机梯度下降法：

用$$y = 2*x1 + x2 -1 + rand(0,1)$$产生1000组数，用这一组数据来反求产生函数中的系数$$(2,1,-1)^T$$.  
迭代停止条件就是，训练得到的相邻两次参数的差的范数小于0.000001\(严格来说，停止条件是损失函数一介导数等于0，也就是$$L'(\theta) = -\frac{1}{m}X^T(X\theta - Y) = 0$$\).  
1）批量梯度下降法：1000个样本，设置的learn rate = 0.0001, 一次迭代所有1000个样本，最终经过38000次迭代收敛到可以接受的标准。如果设置learn rate 是较大的值，比如为1，则发现结果马上发散了，因此批量梯度下降法对learn rate比较敏感，而下面的随机梯度下降法就不会出现这个问题，因为每次只对一个样本进行处理，即使这个样本对参数的梯度很大，但是下一个样本又可以马上把参数拉回来。  
![](/assets/BatchGradientDescentTest1.png)

2\)随机梯度下降法：

1000个样本，设置的learn rate = 1, 一次迭代处理一个样本，最终经过25000次迭代收敛到可以接受的标准。

![](/assets/StocGradientDescentTest1.png)  
3\)   

牛顿法  
![](/assets/NetwonIterationMethodsTest1.png)

对批量梯度下降法和随机梯度下降法的总结：  
　　批量梯度下降---最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。  
　　随机梯度下降---最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。  
**从实验数据来看，批量梯度下降法收敛到的是全局最优值，而随机梯度下降法收敛到最优值附件的地方。随机梯度下降法的好处是收敛远快于批量梯度下降法。**
###线性方程组求解转为成一个最小二乘问题  
&emsp;&emsp;$$\mathbf{Y = X*\lambda}$$ 
问题可以转化成寻找一组$$\lambda$$使得:  
&emsp;&emsp;$$L(\lambda) = (X*\lambda - Y)^T(X*\lambda - Y)$$极小  
利用$$L(\lambda)$$对向量
$$\lambda$$[求导](https://zhuanlan.zhihu.com/p/24709748)可以得到[^1]：  
&emsp;&emsp;$$\frac{\partial L(\lambda)}{\partial \lambda} = 2X^T(X\lambda -Y) = 0$$  
我们也可以得到：  
&emsp;&emsp;$$\frac{\partial^2 L(\lambda)}{\partial \lambda^2} = 2X^TX$$

&emsp;&emsp;X\(n+1\) = X\(n\) - f'\(X\(n\)\)/f''\(X\(n\)\)

```cpp
MatrixXd Iteration::BathGradDescent(MatrixXd paras, int iterator_num)
{
    int rows = m_input_data.rows();
    int cols = m_input_data.cols();
    MatrixXd init_paras = paras;
    if (m_input_data.cols() == paras.rows())
    {

        MatrixXd grad = (1.0 / rows)*m_input_data.transpose()*(m_output_data - m_input_data*init_paras);
        paras += m_learn_rate*grad;
    }
    else
    {
        //error
    }
    return paras;
}

MatrixXd Iteration::StocGradDescent(MatrixXd paras, int index)
{
    int rows = m_input_data.rows();
    int cols = m_input_data.cols();
    MatrixXd input_data_i(1,cols);
    MatrixXd output_data_i(1, 1);
    output_data_i(0, 0) = m_output_data(index,0);

    for (int i = 0; i < cols; i++)
    {
        input_data_i(0, i) = m_input_data(index, i);
    }


    if (m_input_data.cols() == paras.rows())
    {
        MatrixXd grad = (1.0 / rows) *input_data_i.transpose()*(input_data_i*paras - output_data_i);    
        paras = paras - grad;
    }
    else
    {
        //error
    }
    return paras;
}
```


