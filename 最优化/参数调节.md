# 优化收敛位置

[参考知乎上面](https://www.zhihu.com/question/68109802)  
对于N个参数的系统（神经网络），对应的Hassian阵是N阶的，我们假定其本征值的正负概率都是0.5.因此，要保证是局部最小值，则Hassian正定，意味着所有的本征值都为正。其概率是$$\frac{1}{2^N}$$。同样，是局部最大值，则Hassian阵负定，概率也为$$\frac{1}{2^N}$$。因此，最有可能的是本征值有正有负，也就是鞍点的情形。但是即使是普通的鞍点，函数不会震荡，而是马上逃离。  
实际的情形是，函数收敛到了一个大的平坦区域，其表现就是一阶导数很小，以至于在训练结束的时候还没有逃离出这个平坦区域。那么，我们得从数学上讨论这种平坦区域（平坦马鞍面）具有什么样的性质。可以看下面一个方程：  
$$f(x,y) = \frac{\alpha}{2}x^2 - \frac{\beta}{2}y^2$$  
$$f_{x} = \alpha x; f_{y} = -\beta y$$.假设$$\alpha > 0, \beta > 0$$  
\(0, 0\)点是唯一的鞍点。在x方向，该点是局部最小值，但在y方向，是局部极大值，如果$$\beta$$很小，很小是它与学习率r相乘结果与1来说的。假设学习率是r一开始$$y = \Delta _0$$,则一次迭代后  
$$\Delta _1 = \Delta _0 + r*\beta*\Delta _0 = (1 + r\beta)\Delta $$  
$$\Delta _2 = \Delta _1 + r*\beta*\Delta _1 = (1 + r\beta)\Delta _1$$  
n次迭代后：   

如果训练总的次数为N， $$r\beta \leq \frac{1}{N}$$,那N次迭代之后 $$(1 + r\beta)^N \Delta _0 \leq (1 + \frac{1}{N})^N \approx e\Delta _0 = 2.71828 \Delta _0$$，这定量的给出了在训练结束的时候，函数能多大程度的逃离（0，0）点。  
这个模型可以推广到N维情形。假设Hassian阵有N-K个本征值非负，K个小于零。  
考虑通过平移后，鞍点处于（0，0....0）点附近的二阶展开：  
$$L(X) = L(0) + \sum _{i=K + 1} ^{i=N}\frac{\alpha _i}{2}x_i^2 - \sum _{i=1} ^{i=K}\frac{\alpha _i}{2}x_i^2$$  
如果满足所有的$$\alpha _i \ll 1, i \in [1,K]$$，这时候，函数会在很长时间内，局域在鞍点附件。  
可以考虑的时，怎么通过设计Cost Function来避免产生这些超级马鞍面。实际上是通过带冲量的方法来加速收敛/    
这些超级马鞍面的产生要考虑激活函数吗，Relu比Sigmoid不容易产生这些马鞍面？

# 激活函数的作用

增加非线性，增强学习能力

# 为啥使用收敛慢的（随机）梯度下降法

因为只需要计算一阶导数，而不需要计算二阶，因为几十万以上的参数，计算二劫导数太费时间,内存也是个问题（除非用L-BFGS）



