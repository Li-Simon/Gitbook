# 正则化方法原理
##Ridge回归，Shrink与SVD
$$ \kern{4 em}\hat \beta ^{ridge} =  \arg min_{\beta}{\displaystyle \sum_{i=1}^N(y_i -\beta_0-\displaystyle \sum_{j=1}^px_{ij}\beta_j)^2 + \lambda \displaystyle \sum_{j=1}^p\beta_j^2}$$  
等价于:   
$$ \kern{4 em}\hat \beta ^{ridge} =  \arg min_{\beta}{\displaystyle \sum_{i=1}^N(y_i -\beta_0-\displaystyle \sum_{j=1}^px_{ij}\beta_j)^2} $$   
$$\kern{4 em} subject to:  \displaystyle \sum_{j=1}^p\beta_j^2 \le t$$.  
上面的问题也等价于假定，$$y_i,\beta_i$$服从如下分布：  
$$ \kern{4 em} y_i \in N(\beta_0 + x_i^T\beta,\sigma^2)$$  
$$ \kern{4 em} \beta_i \in N(0,\tau^2)$$   
其中：$$\lambda = \sigma^2/\tau^2$$  
因此RRS(Root sum square)可以写成如下形式：  
$$\kern{4 em}RRS(\lambda) =(\mathbf{y} - \mathbf{X\beta})^T(\mathbf{y} - \mathbf{X\beta}) + \lambda \beta^T\beta $$  
通过对$$\beta$$求导并令其为0，可以得到：  
$$ \kern{4 em}\hat \beta ^{ridge} = (\mathbf{X}^T\mathbf{X} + \lambda I)^{-1}\mathbf{X}^T\mathbf{y}$$    
为了建立起L2与SVD之间的联系， 我们对X进行SVD分解：  
$$\kern{4 em} \mathbf{X} = \mathbf{UDV^T}$$  
我们重写没有正则化的最小二乘拟合：  
$$\kern{4 em} \mathbf{X}\beta^{ls} = (\mathbf{X}^T\mathbf{X} )^{-1}\mathbf{X}^T\mathbf{y} =\mathbf{UU^T}\mathbf{y} $$  
对于L2正则化的最小二乘法：  
 $$\kern{4 em} \mathbf{X}\beta^{ls} = (\mathbf{X}^T\mathbf{X} + \lambda I)^{-1}\mathbf{X}^T\mathbf{y} =\mathbf{UD}(\mathbf{D^2+\lambda I})^{-1}\mathbf{DU^T}\mathbf{y} $$    
 $$\kern{7 em} = \displaystyle \sum_{j=1}^p \mathbf{u_j}\frac{d_j^2}{d_j^2 + \lambda}\mathbf{u_j}^T\mathbf{y}$$  
 从上面的分式$$\frac{d_j^2}{d_j^2 + \lambda}$$可以知道，对于$$d_j \ll \lambda$$,则相应的方向会收缩到0。可以认为L2就是对数据进行了SVD分解后，只保留了$$d_j \gt \lambda$$的分量。 


##Lasso(L1)回归
$$ \kern{4 em}\hat \beta ^{ridge} =  \arg min_{\beta}{\displaystyle \sum_{i=1}^N(y_i -\beta_0-\displaystyle \sum_{j=1}^px_{ij}\beta_j)^2 + \lambda \displaystyle \sum_{j=1}^p|\beta_j|}$$  
等价于:   
$$ \kern{4 em}\hat \beta ^{ridge} =  \arg min_{\beta}{\displaystyle \sum_{i=1}^N(y_i -\beta_0-\displaystyle \sum_{j=1}^px_{ij}\beta_j)^2} $$   
$$\kern{4 em} subject to:  \displaystyle \sum_{j=1}^p|\beta_j| \le t$$.  
L1更容易产生稀疏性。

