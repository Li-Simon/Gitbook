# 线性与非线性方程组解的稳定性分析
最速下降法中，Hessian矩阵的条件数决定了系统的收敛速度。这一节，本来是自己想通过条件数来分析L-M算法解的稳定性的。    
# 线性方程组解的稳定性分析

系统解的稳定性定义成系统小的扰动对系统解的影响，。  
[Example 1](http://engrwww.usask.ca/classes/EE/840/notes/ILL_Conditioned Systems.pdf):  
$$\begin{bmatrix}400&-201\\ 
\\-800&401\end{bmatrix}\begin{bmatrix} x_1\\ x_2\end{bmatrix}=\begin{bmatrix} 200\\ -200\end{bmatrix}$$  
Solution:  
$$x_1 = -100, x_2 = -200$$  
If we give small change to matrix A, change $$\mathbf{A_{11}}$$from 400 to 401 then:  
$$\begin{bmatrix}401&-201\\ 
\\-800&401\end{bmatrix}\begin{bmatrix} x_1\\ x_2\end{bmatrix}=\begin{bmatrix} 200\\ -200\end{bmatrix}$$  
This time the solution is:  
$$x_1 = 40000, x_2 = 79800$$

Ill-conditiond:  
When the solution is highly sensitive to the values of the coefficient matrix A or the righthand  
side constant vector b, the equations are called to be ill-conditioned.

# Condition Number

Let's linear equations:  
&emsp;&emsp;$$ \mathbf{Ax = b}$$,  
Let us investigate first, how a small change in the $$\mathbf{b}$$ vector changes the solution vector. $$\mathbf{x}$$ is the solution of the original system and let $$\mathbf{x + {\Delta}x}$$ is the solution when $$\mathbf{b}$$ changes from $$\mathbf{b}$$ to $$\mathbf{b + {\Delta}b}$$ .

Then we can write:  
&emsp;&emsp;$$\mathbf{A(x + {\Delta}x) = b + {\Delta}b}$$  
or   
&emsp;&emsp;$$ \mathbf{Ax + A{\Delta}x = b + {\Delta}b}$$  
But because $$\mathbf{Ax = b}$$, it follows that   
&emsp;&emsp;$$ \mathbf{A{\Delta}x = {\Delta}b}$$  
So：  
&emsp;&emsp;$$ \mathbf{\delta x=A^{-1}{\Delta}b}$$  
Using the matrix norm properties:  
&emsp;&emsp;$$ \mathbf{||Ax|| \leq ||A||||x||} $$   
We can get:  
&emsp;&emsp;$$ \mathbf{||A^{-1}{\Delta}b|| \leq ||A^{-1}||||{\Delta}b||}$$,  
Also, we can get:  
&emsp;&emsp;$$\mathbf{||b|| =  ||Ax|| \leq ||A||||x||}$$  
Using equations 2.5 and 2.6 we can get:  
&emsp;&emsp;$$\mathbf{\frac{||{\Delta}x||}{||A||\cdot||x||} \leq  \frac{||A^{-1}||\cdot||{\Delta}b||}{||b||}}$$  
Let's define condition number as: $$\mathbf{K(A)=||A||\cdot||A^{-1}||}$$  
we can rewrite equation 2.7 as:  
&emsp;&emsp;$$ \mathbf{\frac{||{\Delta}x||}{||x||} \leq  K(A)\cdot\frac{||{\Delta}b||}{||b||}}$$

Now, let us investigate what happens if a small change is made in the coefficient matrix  
$$\mathbf{A}$$. Consider $$\mathbf{A}$$ is changed to $$\mathbf{A+\Delta{A}}$$ and the solution changes from $$\mathbf{x}$$ to $$\mathbf{x+\Delta{x}}$$  
&emsp;&emsp;$$ \mathbf{(A + \Delta A)(x + \Delta x) = b} $$,  
 we can obtain:  
&emsp;&emsp;$$ \mathbf{\frac{||{\Delta}x||}{||x + \Delta x||} \leq  K(A)\cdot\frac{||\Delta A||}{||A||}}$$

$$\mathbf{K(A)}$$ is a measure of the relative sensitivity of the solution to changes in the right-hand  
side vector b.When the condition number $$\mathbf{K(A)}$$  becomes large, the system is regarded as being illconditioned.  
Matrices with condition numbers near 1 are said to be well-conditioned.

# 非线性方程组解的稳定性分析

For non-linear system,

# 最小二乘法求解线性，非线性方程组

所有的线性方程组或者非线性方程组可以转化成一个最小二乘法问题，使得拟合的方程与观察值之间的平方和最小。  
&emsp;&emsp;$$ \mathbf{\hat{\beta} = argmin_{\beta}S(\beta)=argmin_{\beta}\sum_{i = 1}^{m}[y_i - f(x_i,\beta)]^2}$$  
$$y_i$$是观测值，$$x_i$$是已知变量，一共m组观测值。

# 当$$\mathbf{f(x,\beta)}$$是线性方程组时

即 $$\mathbf{f(X,\beta) = A\beta}$$  
cost function 可以表示如下：  
&emsp;&emsp;$$\mathbf{L(\beta) = ||Y - A\beta||^2}$$  
其中$$\mathbf{Y= [y_1, y_2...y_m]^T}$$  
cost function 对$$\mathbf{\beta}$$  
&emsp;&emsp;$$\mathbf{L(\beta) = Y^TY - 2Y^TA\beta + {\beta}^TA^TA\beta}$$  
求导，  
&emsp;&emsp;$$ -2A^TY + 2 (A^TA)\beta = 0$$  
再让导数等于0,就可以求得解为：$$\mathbf{\beta}$$  
&emsp;&emsp;$$ \mathbf{\beta = (A^TA)^{-1}A^TY}$$

# 我们接下来主要讨论非线性的情况：

也就是残差$$\mathbf{y_i - f(x_i,\beta)}$$不能表示成线性形式。  
当然我们有很多方法来求解方程(4.1),比如
[梯度下降法](https://en.wikipedia.org/wiki/Gradient_descent)，
[牛顿法](https://en.wikipedia.org/wiki/Newton's_method_in_optimization),
[高斯-牛顿法](https://en.wikipedia.org/wiki/Gauss–Newton_algorithm)，以及
[Levenberg-Marquardt](https://en.wikipedia.org/wiki/Levenberg–Marquardt_algorithm) 
[梯度下降法](https://en.wikipedia.org/wiki/Gradient_descent)  
把损失函数展开到一阶。  
&emsp;&emsp;$$f(x + \alpha \mathbf{d}) = f(x_0) + \alpha f'(x_0)\mathbf{d} + O({\alpha}^2)$$  
&emsp;&emsp;$$x_{t+1} = x_t -\alpha f'(x_t)$$,

[牛顿法](https://en.wikipedia.org/wiki/Newton's_method_in_optimization)  
把cost function进行展开到二阶：  
&emsp;&emsp;$$ f(x_{t+1}) = f(x_t) + g(x_{t+1} - x_t)  + \frac{1}{2} (x_{t+1} -x_t)^TH(x_{t+1} -x_t)$$  
求导，$$\frac{\partial f}{\partial x_t} = g + H(x_{t+1} -x_t)$$,让导数为0就有  
&emsp;&emsp;$$x_{t+1} = x_t - H^{-1}g$$  
要是H是正定的，上面的就是凸函数，也就一定有了最小值。可惜H不一定是正定的，这就引导出了下面的方法

[高斯-牛顿法](https://en.wikipedia.org/wiki/Gauss–Newton_algorithm)  
cost function可以表示成残差的形式：  
&emsp;&emsp;$$L(x) = \sum_{i=1}^{m}r_i(x)^2$$  
根据牛顿法：  
&emsp;&emsp;$$vx_{t+1} = x_t - H^{-1}g$$,  
梯度表示为：  
&emsp;&emsp;$$g_j=2 \sum _i r_i \frac{\partial r_i}{\partial x_j}$$  
对方程\(4.9\)求二阶导，我们可以得到Hessian矩阵：  
&emsp;&emsp;$$H_{jk} = 2\sum _i (\frac{\partial r_i}{\partial x_j}\frac{\partial r_i}{\partial x_k} + r_i\frac{\partial ^2 r_i}{\partial x_j \partial x_k})$$,  
当残差$$r_i$$很小的时候,我们就可以去掉最后一项，因此  
&emsp;&emsp;$$H_{jk} \approx  2 \sum _i J_{ij}J_{ik} \quad With \quad J_{ij} = \frac{\partial r_i}{\partial x_j}$$  
这样Hessian就是半正定了。  
因此参数迭代公式\(4.10\)可以写成：  
&emsp;&emsp;$$  x_{t+1} = x_t - (J^TJ)^{-1}J^Tr$$,  
[Levenberg-Marquardt](https://en.wikipedia.org/wiki/Levenberg–Marquardt_algorithm)  
方法用于求解非线性最小二乘问题，结合了梯度下降法和高斯-牛顿法。  
其主要改变是在Hessian阵中加入对角线项。当然，这是一种L2正则化方式。  
&emsp;&emsp;$$x_{t+1} = x_t - (H + \lambda I_n)^{-1}g$$,  
&emsp;&emsp;$$x_{t+1} = x_t - (J^TJ + \lambda I_n)^{-1}J^Tr$$  
L-M算法的不足点。  
当$$\lambda$$很大时，$$J^TJ + \lambda I_n$$根本没用，Marquardt为了让梯度小的方向移动的快一些，来防止小梯度方向的收敛，把中间的单位矩阵换成了$$diag(J^TJ)$$,因此迭代变成：  
&emsp;&emsp;$$x_{t+1} = x_t - (J^TJ + \lambda diag(J^TJ))^{-1}J^Tr$$

阻尼项（damping parameter）$$\lambda$$的选择，Marquardt 推荐一个初始值$$\lambda_0$$与因子$$v > 1$$，开始时，$$\lambda = \lambda_0$$,然后计算cost functions,第二次计算$$\lambda = \lambda_0/v$$,如果两者cost function都比初始点高，然后我们就增大阻尼项，通过乘以v,直到我们发现当$$\lambda = \lambda_0v^k$$时，cost function下降。  
如果使用$$\lambda = \lambda_0/v$$使得cost fucntion下降，然后我们就把$$\lambda_0/v$$作为新的$$\lambda$$  
当$$\lambda = 0$$时，就是高斯-牛顿法，当$$\lambda$$趋于无穷时，就是梯度下降法。如果使用$$\lambda/v$$没有是损失函数下降，使用$$\lambda$$导致损失函数下降，那么我们就继续使用$$\lambda$$做为阻尼项。
#归一化的残差
如果我们假设数据中每一点的贡献是等权重，因此，我们有必要对每一项残差加一个权重，来归一化每一项残差，这样做的目的是未来防止某些点(离群点，异常值点)的偏差过大，从而对模型结果造成显著影响，我们才假设每一点的误差贡献是一样的。因此公式(4-9)可以变成，
cost function可以表示成残差的形式：  
&emsp;&emsp;$$L(x) = \sum_{i=1}^{m}\lambda_i r_i(x)^2 = \sum_{i=1}^{m}\frac{1}{y_i^2} (y_i - f_i(x_i))^2 $$       
此时，Jacobia变成  
&emsp;&emsp;$$H_{jk} \approx  2 \sum _i J_{ij}J_{ik} \quad With \quad J_{ij} = y_i \frac{\partial r_i}{\partial x_j}$$  
&emsp;&emsp;$$ J_S = \Sigma J,  \Sigma_{ij} = \frac{1}{y_i}\delta_{ij} $$


## Levenberg-Marquardt算法与L2正则化等价

我们在cost fucntion加上L2正则项  
&emsp;&emsp;$$ L(\beta) = \sum_{i=1}^{m}[y_i - f(x_i,\beta)]^2 + \lambda||\beta||^2$$  
可以表示成：    
&emsp;&emsp;$$L(\beta) = L(\beta_0) + (\beta - \beta_0)^T\nabla_\beta L(\beta_0) + \frac{1}{2}(\beta - \beta_0)^TH(\beta - \beta_0)+ \lambda||\beta||^2+O(\beta^3)$$  
求一阶导数：  
&emsp;&emsp;$$ L(\beta) = L(\beta_0) + (\beta - \beta_0)^Tg + \frac{1}{2}(\beta - \beta_0)^TH(\beta - \beta_0)+ \frac{1}{2}\lambda||\beta||^2+O(\beta^3)$$  
令其为0：  
&emsp;&emsp;$$\frac{\partial L(\beta)}{\partial \beta} = g + H(\beta - \beta_0) + \lambda\beta = 0$$  
就得到：  
&emsp;&emsp;$$ \beta = (H + \lambda I_n)^{-1}H\beta_0 - (H + \lambda I_n)^{-1}g$$,  
当$$\lambda$$相对于H的本征值来说较小时，则$$(H + \lambda I_n)^{-1}H = I$$,因此上式可以化为：  
&emsp;&emsp;$$ \beta = \beta_0 - (H + \lambda I_n)^{-1}g$$,  
也就是通常的L-M算法。  


  