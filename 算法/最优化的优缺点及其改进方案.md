# 最优化的优缺点及其改进方案
###常见的损失函数
####0-1损失函数
y是标签，f是预测结果。  
0-1损失函数：  
$$\kern{4 em} L_{0-1}(f,y) =1_{fy \le 0}$$  
Hinge损失函数：  
$$\kern{4 em} L_{hinge}(f,y) = \max(0,1-fy)$$  
Logistic损失函数：  
$$\kern{4 em} L_{logistic}(f,y) = \log_2(1+\exp(-fy))$$  
当预测值$$f \in [-1, 1]$$时，可以定义如下损失函数：  
交叉熵（Cross Entropy）:  
$$\kern{4 em} L_{cross entropy} = -\log_2(\frac{1 + fy}{2})$$  
###梯度下降
####问题1：怎样验证自己代码中梯度公式的正确性？

####问题2：当数据量特别大的时候经典梯度下降法存在什么问题，需要怎么改进？
####问题3：深度学习中常用的优化方法是随机梯度下降法，但是SGD偶尔也会失效，无法给出满意的训练结果，这是为什么？
####问题4：为了改进SGD，研究者都做了哪些改动？提出了哪些方法？他们各有哪些特点？
####问题5：L1正则化使得模型参数具有稀疏性的原理是什么？
