# 前向神经网络问题
####为啥Sigmoid和Tanh激活函数会导致梯度消失的现象？
因为在参数很大或者很小的时候，他们的导数会趋于0，因此造成了梯度消失。  
####ReLU系列激活函数相对于Sigmoid和Tanh激活函数的有点事什么？它们有什么局限性以及如何改进？  
#####优点
(1)计算梯度简单，因为Sigmod和Tanh涉及到指数运算  
(2)ReLU能有效解决后两者的题都消失问题，提供相对宽的激活边界   
(3)ReLU的单侧抑制提供了网络的稀疏表达能力  
#####缺点
ReLU的局限性在于其训练过程中会导致神经元死亡的问题，由于激活函数和是$$mf(x) = \max(0,z)$$导致梯度在经过该ReLU单元时，被置为0。且之后再也不能被任何数据激活，因为流经这个神经元的梯度永远为0，因此不对任何数据产生响应。因此会导致一定比例的神经元不可逆的死亡，进而参数梯度永远无法更新，整个训练过程失败。  
为了解决这一问题，人们设计了Leaky ReLU(LReLU)其表达式是：  
$$\kern{8 em}f(z) = z; z> 0$$    
$$\kern{8 em}f(z) = az; z\le 0$$    
一般a是一个很小的正数，但是怎么选取a也是一个问题。为解决这个问题，人们发明了参数化的PReLU(Parametric ReLU),它与LReLU的主要区别是，斜率参数a作为网络中一个可学习的参数。而另一个LReLU的变种增加了"随机化"机制，具体的，在训练过程中，斜率a作为一个满足某种分布的随机采样；测试时再固定下来。Random ReLU(RReLU)在一定程度上能起到正则化的作用。  
####平方误差损失函数和交叉熵损失函数分别适合什么场景？  
平方损失函数:回归问题，输出是连续值，并且最后一层不含Sigmoid或者Softmax激活函数的NN。因为会导致梯度很小而发生梯度消失的问题  
交叉熵：分类问题，
###神经网络训练技巧
主要涉及的问题是“过拟合”：解决方法包括数据集增强，正则化，模型集成。其中dropout是模型集成方法中最高效与常用的技巧。此外，DNN训练中设计的手动调参，如学习率，权重衰减系数，Dropout比例等，这些参数的选择会显著影响模型最终的训练效果。Batch Normalization(BN)方法有效规避了这些复杂参数对网络训练产生的影响，在加速训练收敛的同时也提升了网络的泛化能力。  
####神经网络训练时是否可以将全部参数初始化为0？  
因为所有参数都是相同的值，所有神经元都是对称的，因此没有办法打破这种对称，导致无论学习多久，所有参数依然是相同的。因此我们需要随机地初始化NN参数的值，来打破这种对称性。  
####为啥Dropout可以抑制过拟合？它的工作原理和实现？  
还得先介绍Dropout的实际步骤，也就是训练的时候是多少个神经元？预测的时候是多少个？   
Dropout作用于小批量训练数据，由于其随机丢弃部分神经元的机制，相当于每次迭代都是在训练不同结构的神经网络，类比于Bagging方法，Dropout可被认为书一种实用的大规模深度神经网络的模型集成算法。这是由于传统意义上的Bagging涉及多个模型的同时训练和测试评估，当网络与参数规模庞大时，这种集成方式需要消耗大量的运算时间与空间。Dropout在小批量级别上的操作，提供了一种轻量级的Bagging集成近似，能够实现指数级数量神经网络的训练与评测。  
Dropout在具体实现中，某个神经元以概率p被丢弃，因此N个神经元在Dropout下相当于是$$2^N$$个模型的集成。这$$2^N$$个模型可认为是原始网络的子网络，它们共享部分权值，并且具有相同的网络层数，而模型的整体参数数目不变，这就大大简化了运算。对于任意神经元，每次训练中都与一组随机挑选的不同的神经元集合共同进行优化，这个过程会减弱全体神经元之间的联合适应性，减小过拟合的风险，增强泛化能力。


