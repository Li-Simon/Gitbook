# 集成学习

集成学习就是组合一系列的弱分类器生成强分类器。组合的弱分类器之间的关系有两种:

一种是彼此间相互依赖,一系列学习器之间需要串行生成，代表算法是Boosting系列算法，代表算法是AdaBoost与GBDT.  
一种是学习器彼此间无关联，一些列算法可以并行的生成，代表算法是Bagging\(Bootstrap Aggregating\)和随机森林（Random Forest）系列。随机森林是时Bagging的进行版，随机表现在两个方面，一方面是取样本点是随机的，另一方面，选择特征也是随机的（比如总共有N个特征，随机选取小于N的K个特征）  
还有一种是两者的结合物：stacking

### PAC，弱可学习，强可学习

PAC\(Probably approximately correct\)  
强可学习：如果存在一个多项式的学习算法学习呀，学习的正确率很高  
弱可学习：如果存在一个多项式的学习算法学习呀，学习的正确率仅比随机猜测略好  
在PAC学习框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。

### 集成学习的理论基础

我们考虑多个不相干分类器叠加处理二分类的问题，$$y \in (-1,+1)$$和真实的函数f,假设基分类器的错误率都是$$\epsilon$$, 即对每个分类器都有  
$$\kern{10 em}P(h_i(x) \ne f(x)) = \epsilon$$  
假设集成通过简单的投票发结合T个基分类器，若半数的基分类器正确，则集成分类正确。  
       $$\kern{10 em}H(x) = sign(\sum _{i=1}^{T}h_i(x))$$  
 价格基分类器之间的错误率是相互独立的，则由Hoeffding不等式可知，集成的错误率为：  
 $$\kern{10 em}P(H(x) \ne f(x)) = \displaystyle\sum_{i=0}^{[T/2]}\begin{pmatrix}
   T \\
   k
\end{pmatrix}(1 - \epsilon)^k\epsilon ^{T-k} \le exp(-\frac{1}{2}T(1-2\epsilon)^2)$$  
上式表明，随着集成中个体分类器数目T的增大，集成的错误率将指数下降，最终趋向于0.



