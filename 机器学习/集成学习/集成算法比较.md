###RF,GBDT,XGBOOST,LightGBM比较
###RF
随机森林(RF)是Bagging的扩展变体，它在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入随机特征选择，主要包含两个特征：
1. 随机的有放回的样本选择  
2. 随机选择特征  
其原理是通过减少通过投票来减少整体的方差。虽然每个基分类器的bias大，但是通过多个彼此不相关的基分类器能减少总体的方差。    
优点：
1. 数据集合上表现良好，相对于其他算法有较大的优势(训练速度，预测准确度)  
2. 能够处理高纬度的数据，并且不用特征选择，而且在训练完后，给出特征的重要性  
3.容易做并行处理 
缺点：  
1. 在噪声较大的数据集上，分类与回归问题会出现过拟合。   
###GBDT
传统的Boosting算法，比如AdaBoost主要关注的是正确错误样本的权重，而GBDT的每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以在残差减小的梯度方向建立模型，在GradientBoost中，每个新的模型的建立时为了使得之前的模型的残差往梯度下降的方法。   
在GradientBoosting算法中，关键即使利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合棵CART回归树。  
###XGBoost
###LightGBM