# 前向神经网络问题

#### 激活函数的作用

增加非线性，增强学习能力

#### 为啥使用收敛慢的（随机）梯度下降法

因为只需要计算一阶导数，而不需要计算二阶，因为几十万以上的参数，计算二劫导数太费时间,内存也是个问题（除非用L-BFGS）

#### 非线性激活函数在神经网络中的作用

1. 线性激活函数的劣势：如果只是线性激活函数,则多层网络与单层网络等价，因为每多一层，只是相当于乘以一个矩阵。  
2. 非线性激活函数的好处： 能把原来线性不可解的问题变为高纬度空间中的线性可解问题。线性可解的意思是，
   1. 对于分类问题，则原本线性不可分的在新的空间中线性可分了；
   2. 对于回归问题，非线性激活函数的存在使得我们可以逼近任意的连续函数。    
3. 获得更强大的函数拟合能力，万有近似定理。  
     
#### 为啥Sigmoid和Tanh激活函数会导致梯度消失的现象？
因为在参数很大或者很小的时候，他们的导数会趋于0，因此造成了梯度消失。

#### ReLU系列激活函数相对于Sigmoid和Tanh激活函数的有点事什么？它们有什么局限性以及如何改进？

##### 优点

\(1\)计算梯度简单，因为Sigmod和Tanh涉及到指数运算  
\(2\)ReLU能有效解决后两者的题都消失问题，提供相对宽的激活边界  
\(3\)ReLU的单侧抑制提供了网络的稀疏表达能力(Why?因为他可以使得一部分神经元的输出为0，因此这些神经元不起作用。)

##### 缺点

ReLU的局限性在于其训练过程中会导致神经元死亡的问题，由于激活函数和是$$mf(x) = \max(0,z)$$导致梯度在经过该ReLU单元时，被置为0。且之后再也不能被任何数据激活，因为流经这个神经元的梯度永远为0，因此不对任何数据产生响应。因此会导致一定比例的神经元不可逆的死亡，进而参数梯度永远无法更新，整个训练过程失败。  
为了解决这一问题，人们设计了Leaky ReLU\(LReLU\)其表达式是：  
&emsp;&emsp;$$f(z) = z; z> 0$$  
&emsp;&emsp;$$f(z) = az; z\le 0$$  
一般a是一个很小的正数，但是怎么选取a也是一个问题。为解决这个问题，人们发明了参数化的PReLU\(Parametric ReLU\),它与LReLU的主要区别是，斜率参数a作为网络中一个可学习的参数。而另一个LReLU的变种增加了"随机化"机制，具体的，在训练过程中，斜率a作为一个满足某种分布的随机采样；测试时再固定下来。Random ReLU\(RReLU\)在一定程度上能起到正则化的作用。

#### 平方误差损失函数和交叉熵损失函数分别适合什么场景？

平方损失函数:回归问题，输出是连续值，并且最后一层不含Sigmoid或者Softmax激活函数的NN。因为会导致梯度很小而发生梯度消失的问题  
交叉熵：分类问题，

#### 写出多层感知机的平方误差和交叉熵损失函数

给定包含m个样本的集合$$((x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)}))$$，

###### 代价函数

平方误差其整体代价函数是：  
&emsp;&emsp;$$J(W,b) = [\frac{1}{m}\displaystyle \sum_{i=1}^mJ(W,b;x^{(i)},y^{(i)})] + \frac{\lambda}{2}\displaystyle \sum_{l=1}^{N-1}\displaystyle \sum_{i=1}^{s_l}\displaystyle \sum_{j=1}^{s_l+1}(W_{ij}^{(l)})^2$$  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$= [\frac{1}{m}\displaystyle \sum_{i=1}^m\frac{1}{2}||y^{(i)}-L_{W,b}(x^{(i)})||^2] + \frac{\lambda}{2}\displaystyle \sum_{l=1}^{N-1}\displaystyle \sum_{i=1}^{s_l}\displaystyle \sum_{j=1}^{s_l+1}(W_{ij}^{(l)})^2$$  
二分类交叉熵误差其整体代价函数是：  
&emsp;&emsp;$$J(W,b) = -[\frac{1}{m}\displaystyle \sum_{i=1}^mJ(W,b;x^{(i)},y^{(i)})] + \frac{\lambda}{2}\displaystyle \sum_{l=1}^{N-1}\displaystyle \sum_{i=1}^{s_l}\displaystyle \sum_{j=1}^{s_l+1}(W_{ij}^{(l)})^2$$  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$= -[\frac{1}{m}\displaystyle \sum_{i=1}^m(y^{(i)}\ln o^{(i)} + (1-y^{(i)})\ln (1-o^{(i)}))] + \frac{\lambda}{2}\displaystyle \sum_{l=1}^{N-1}\displaystyle \sum_{i=1}^{s_l}\displaystyle \sum_{j=1}^{s_l+1}(W_{ij}^{(l)})^2$$  
多分类交叉熵误差其整体代价函数是：  
&emsp;&emsp;$$J(W,b) = -[\frac{1}{m}\displaystyle \sum_{i=1}^mJ(W,b;x^{(i)},y^{(i)})] + \frac{\lambda}{2}\displaystyle \sum_{l=1}^{N-1}\displaystyle \sum_{i=1}^{s_l}\displaystyle \sum_{j=1}^{s_l+1}(W_{ij}^{(l)})^2$$  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$= -[\frac{1}{m}\displaystyle \sum_{i=1}^m y_k^{(i)}\ln o_k^{(i)} ] + \frac{\lambda}{2}\displaystyle \sum_{l=1}^{N-1}\displaystyle \sum_{i=1}^{s_l}\displaystyle \sum_{j=1}^{s_l+1}(W_{ij}^{(l)})^2$$  
其中$$o_k^{(i)}$$  
代表第i个样本的预测属于类别k的概率，$$y_k^{(i)}$$为实际的概率\(如果第i个样本的真实类别为k，则$$y_k^{(i)} = 1$$，否则为0\)。

###### 梯度计算公式

第\(l\)层的参数为$$W^{(l)},b^{(l)}$$，每一层的线性变换为：  
&emsp;&emsp;$$ \mathbf{z}^{(l)} = \mathbf{W}^{(l)}\mathbf{x}^{(l)} + \mathbf{b}^{(l)}$$   
输出为:  
&emsp;&emsp;$$\mathbf{a}^{(l)} = f(\mathbf{z}^{(l)})$$  
其中f是非线性激活函数\(比如Sigmoid,Tanh, ReLu等\)；$$\mathbf{a}^{(l)}$$直接作为下一层的输入.即：$$\mathbf{x}^{(l+1)} = \mathbf{a}^{(l)}$$。我们通过批量梯度下降法来优化网络参数。  
&emsp;&emsp;$$ \mathbf{W_{ij}}^{(l)} = \mathbf{W_{ij}}^{(l)} - \alpha \frac{\partial}{\partial  W_{ij}^{(l)}} J(\mathbf{W},\mathbf{b})$$,  
&emsp;&emsp;$$ \mathbf{b_i}^{(l)} = \mathbf{b_i}^{(l)} - \alpha \frac{\partial}{\partial  b_{i}^{(l)}} J(\mathbf{W},\mathbf{b})$$,  
问题的核心是求$$\frac{\partial}{\partial  W_{ij}^{(l)}} J(\mathbf{W},\mathbf{b}),\frac{\partial}{\partial  b_{i}^{(l)}} J(\mathbf{W},\mathbf{b})$$  
然后从最后一层，反向迭代。  
&emsp;&emsp;$$ \frac{\partial}{\partial  W_{ij}^{(l)}} J(\mathbf{W},\mathbf{b}) = \frac{\partial J(\mathbf{W},\mathbf{b})}{\partial  z_{j}^{(l+1)}} \frac{\partial z_{j}^{(l+1)}}{\partial  \mathbf{W}_{ij}^{(l)}}$$

### 神经网络训练技巧

主要涉及的问题是“过拟合”：解决方法包括数据集增强，正则化，模型集成。其中dropout是模型集成方法中最高效与常用的技巧。此外，DNN训练中设计的手动调参，如学习率，权重衰减系数，Dropout比例等，这些参数的选择会显著影响模型最终的训练效果。Batch Normalization\(BN\)方法有效规避了这些复杂参数对网络训练产生的影响，在加速训练收敛的同时也提升了网络的泛化能力。

#### 神经网络训练时是否可以将全部参数初始化为0？

因为所有参数都是相同的值，所有神经元都是对称的，因此没有办法打破这种对称，导致无论学习多久，所有参数依然是相同的。因此我们需要随机地初始化NN参数的值，来打破这种对称性。

#### 为啥Dropout可以抑制过拟合？它的工作原理和实现？

还得先介绍Dropout的实际步骤，也就是训练的时候是多少个神经元？预测的时候是多少个？  
Dropout作用于小批量训练数据，由于其随机丢弃部分神经元的机制，相当于每次迭代都是在训练不同结构的神经网络，类比于Bagging方法，Dropout可被认为书一种实用的大规模深度神经网络的模型集成算法。这是由于传统意义上的Bagging涉及多个模型的同时训练和测试评估，当网络与参数规模庞大时，这种集成方式需要消耗大量的运算时间与空间。Dropout在小批量级别上的操作，提供了一种轻量级的Bagging集成近似，能够实现指数级数量神经网络的训练与评测。  
Dropout在具体实现中，某个神经元以概率p被丢弃，因此N个神经元在Dropout下相当于是$$2^N$$个模型的集成。这$$2^N$$个模型可认为是原始网络的子网络，它们共享部分权值，并且具有相同的网络层数，而模型的整体参数数目不变，这就大大简化了运算。对于任意神经元，每次训练中都与一组随机挑选的不同的神经元集合共同进行优化，这个过程会减弱全体神经元之间的联合适应性，减小过拟合的风险，增强泛化能力。

##### 批量归一化的基本动机与原理是什么？在卷积神经网络中如何使用？

神经网络训练过程的本质是学习数据分布，如果训练数据与测试数据的分布不同将大大降低网络的泛化能力，因此我们需要在训练开始前对所有输入数据进行归一化处理。  
归一化方法是针对每一批数据（比如m个样本），在网络的每一层的输入之前增加归一化处理（均值为0，方差为1），将所有批处理数据强制在统一的数据分布下，即对该层的任一神经元（假设是第k维）$$\hat x^{(k)}$$采用如下公式：  
&emsp;&emsp;$$\hat x^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$$  
其中$$x^{(k)}$$为该层第k个神经元的原始输入数据，$$E[x^{(k)}]$$为这一批输入数据在第k个神经元的均值，$$\sqrt{Var[x^{(k)}]}$$为这一批数据在k个审计元的标准差。

#### 卷积操作的本质特性包括系数交互和参数共享，具体解释这两种特性以及作用。

###### 稀疏交互

对于全连接网络，如果第k层有m个神经元，第k+1层有n个神经元，怎两层间的权重数目是mn,对于卷积网络，如果卷积核大小为k,则这两层间的权重个数是kn,因为后面一层，每一个神经元只与前一层k个神经元相连。一般k远远小于m，因此相对于全连接网络而言，参数数量大大减少了，因此具有稀疏交互的特征。  
稀疏交互的物理意义是，通常图像，文本，语音等现实世界中的数据都具有局部的特征结构，我看可以先学习局部的特征，再将局部的特征组合起来形成复杂和抽象的特征。以人脸识别为例，最底层的神经元可以检测出各个角度的边缘特征，中间层的神经元可以将边缘组合起来得到眼睛，鼻子，嘴巴等复特征；最后，位于上层的神经元可以根据各个器官的组合检测出人脸的特征。

###### 参数共享

参数共享是指在同一个模型的不同模块中使用相同的参数，它是卷积运算的固有属性。在学习的过程中，我们学习到的就是卷积核（的参数）。根据参数共享的思想，我们只需要学习一组参数集合，而不需要针对每个位置的每个参数都进行优化，从而大大降低了模型的储存需求。  
参数共享的物理意义是使得卷积具有平移不变性。假如图像中有一只猫，那么无论它出现图像中的什么位置，都应该将它识别为猫，也就是说神经网络的输出对于平移变换具有不变性。

#### 常用的池化操作有哪些？池化的作用是什么？

常用的池化操作主要针对非重叠区域，包括均值（mean pooling）,最大池（max pooling）。mean pooling是用一个区域的平均值代替这个区域，能够抑制由于领域大小受限造成估计值方差增大的现象，特点是对背景的保留效果更好；max pooling是用一个区域的最大值代替这个区域本身，能够抑制网络参数误差造成估计均值偏移现象，特点是更好地提取纹理信息。  
池化操作的本质是降采样。除了能保显著降低参数数量，还能够保持对平移，伸缩，旋转操作的不变性。平移不变性是对小量平移而言，尺度一般是pooling的尺寸。伸缩（尺度）不变性，因为pooling就是一个能保持区域主要信息（如最大值）的压缩变换，因此具有伸缩不变性。旋转不变性，在配合多重pooling以后，对于max pooling，对于旋转的图像，我们仍能得到相同的结果，比如图像数字5。

#### 卷积神经网络如何用于文本分类任务？

假设文本总单词量是N，每个单词可以转化为一个K维向量，向量维度K可以预先在其它语料库中获得，也可以作为未知的参数有网络训练得到。这样就组成了一个NXK的输入矩阵，可以看成是一个图像。

###### 卷积层

在这个矩阵上，我们引入hXK大小的卷积核，  进行卷积操作：  
&emsp;&emsp;$$ c_i = f(w*x_{i:i+h-1 + b})$$  
卷积后得到一个N-h+1维度大小的特征向量。通过不同大小的卷积核，提炼出不同的特征向量，构成卷积层的输出。

###### 池化层

选用K-Max进行池化，也就是对每个卷积核作用之后得到的特征向量，进行k-Max池化，也就是选择特征向量中最大的K个特征，这样，每个特征向量转化成一个k维向量，效果就是，通过池化把不同长度的句子转化成定长的向量表示。 若有M个卷积核，池化后就得到一个kXM的矩阵，k=1的话就是，1-Max,最终一个文本对应一个1XM的向量。  
后面的网络结构就与具体的任务有关了，如果是人本分类，就最后接入一个全连接层，并使用softmax激活函数输出每个类别的概率。

#### ResNet的提出背景和核心理论是什么？

ResNet的提出背景是解决或缓解深层的神经网络训练中的梯度消失问题。当时的一个问题是，层数更深的神经网络反而会有更大的训练误差。这种反常，很大程度上归结于深度神经网络的梯度消失问题。随着网络层数的增加，很误差反向传递时，在每层间是相乘的，N层网络，从输出到输入，就是N次连乘，因此很容易造成梯度的消失与膨胀，影响参数的学习。下图就是传统深度神经网络与残差网络的对比。  
![](/assets/ResNN_Principle_2.png)

如下图所示，输入x经过两个神经网络的变换得到F\(x\)，同时也短接到两层之后，最后这个包含两层神经网络模块输出H\(x\)=F\(x\)+x；这样一来，F\(x\)被设计为只需要拟合输入x与目标输出$$\hat H(x)$$的残差$$\hat H(x) -x$$，残差网络的名字因此而来。 如果某一层的输出已经很好的拟合了期望结果，那么多加入一层不会使得模型变差，因为该层的输出将直接被短接到两层之后，直接相当于学习了一个恒等映射，而跳过的两层只需要拟合上层输出和目标之间的残差即可。  
ResNet可以有效改善深层的神经网络学习问题，使得训练更深的网络成为可能。  
![](/assets/ResNN_Principle_1.png)

