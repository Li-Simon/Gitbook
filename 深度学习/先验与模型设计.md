#先验与算法设计
这一节是目的是总结怎么通过先验来设计算法。根据“没有免费午餐定理”我们知道，不存在一个普适的算法在所有场合下都表现很好，因此我们必须根据样本的先验知识来选择模型与设计算法。因此，我们到底该怎么设计我们的算法，也就是基于不同的问题背景，得出先验知识，然后把先验知识作为我们设计算法的基本条件。我们首先要分析的是一些比较成功的网络，比如CNN，RNN，可能还有一些机器学习模型；分析它怎么利用起了先验知识而获得了相比于其它模型更好的成果。因此，这节的核心是先验与算法设计。最终的目的是通过分析已有的成功经验，借鉴这些经验而把它用在未来的问题上面去，也就是《论语》中的"告诸往而知来"。  
### NO FREE LUNCH
没有免费的午餐定理：没有一个模型是对所有的样本是最优的。  
比如下面的例子：  
![](/assets/NO_FREE_LUNCH.png)  
黑点是训练样本，白点是测试样本点。根据训练样本，我们得到模型A，B。如果测试点不同，则A，B的相对表现不同。问题的关键在于，我们先验的假设了\(测试\)样本点在所有空间是均匀分布的，因此，在训练集上训练出的所有模型的效果是一样的，因为你总有样本集是落在你模型预测的曲线之上的。  
因此，如果样本点在整个数据空间是均匀分布的前提下，则不存在一个模型或算法在所有问题上比其它算法更优越。  
但是现实遇到的问题是，数据的分布是受限制的，数据存在一个先验分布，因此会存在一些模型比其它模型更有效。这里NFL不再有是因为问题的前提--数据是均匀分布的--不再成立。因此，脱离场景谈方法是无效的，我们只能针对特定的问题，寻找最佳的优化方法。

### 正则化

没有免费的午餐定理告诉我们没有一个模型是对所有的样本是最优的[^1]，因此，对于特定的样本，我们需要把先验加进模型中\(也就是损失函数中\)，这个先验就是我们对这个样本的知识，比如样本数据的分布，样本的均值或者方差等\(统计物理学中的系统的能量可以看成一种均值，统计物理学中有很多启发性的模型\)。加到模型中的就是我们的正则项，正则项就是一个先验，是我们需要模型满足的约束。  
一个问题是，正则化只能使我们的模型更稳定，不会更精确，那为啥还需要正则项？先验怎么作为正则化的形式加进模型，使得我们的模型更优越呢？实际上，优化不加正则项的损失函数就是求得极大似然；优化加正则项的损失函数就是最大后验概率估计。后者考虑了模型的风险，而前者没有考虑模型的风险。因此，加不加正则项，取决于我们是否需要考虑模型的风险，一般只有处理under-fitting(欠定)问题，也就是变量多余方程数目时，我们才加正则项，对于超定方程，我们根本就不需要加正则项，我们只需要用最小二乘就可以解决。  
最后还留下一个问题，就是损失函数怎么设计，也就是范数怎么选取，$$L_1,L_2,L_{\infty}$$ ，究竟选取哪一个？这个就涉及到模型预测误差的先验分布了，看是拉普拉斯分布($$L_1$$)，高斯分布($$L_2$$)还是其它的？nuclear 范数，无穷范数对应的先验是什么？   

###极大似然与最大后验概率估计
最大似然估计是求参数$$\theta$$，使似然函数$$P(y|\theta)$$最大。最大后验概率估计则是求$$\theta$$使得$$P(y|\theta)P(\theta)$$最大。求得的$$\theta$$不单单让似然函数最大，要$$\theta$$出现的概率也很大。其实对数化之后，就是加上一个有关模型$$\theta$$的正则项。也就是结构风险最小化就等价于最大后验概率估计。     
因此，加正则约束的优化问题，实际上是最大化后验概率估计，而不是最大化似然函数。比如加L2正则化，就是认为，参数先验的服从高斯分布。  
MAP实际上在最大化  
&emsp;&emsp;$$P(\theta|y) = \frac{P(y|\theta)P(\theta)}{P(y)}$$    
因为$$P(y)$$是个固定值，也就是y在实验中出现的次数，比如硬币正面朝上的次数。所以可以去掉分母$$P(y)$$。因此最大化后验概率估计就是最大化$$P(\theta|y)$$。最大化$$P(\theta|y)$$的意义很明确，y出现了，要求$$\theta$$取什么值使得后验概率$$P(\theta|y)$$最大。   
那最大化$$P(\theta|y)$$与最大化$$P(y|\theta)$$的区别是什么？  
MAP最大化$$P(\theta|y)$$:也就是在已知的数据中，求结构风险最小的模型参数。  
MLP最大化$$P(y|\theta)$$:也就是求使得数据出现概率最大的模型参数,它相当于不对模型做惩罚，结果可能是训练集上效果好，测试集合上效果差。  

###一些技术
这里，有些是技术，有些是先验知识，但更多的是先验知识。   
1. 误差反向传播，梯度下降方法
2. 梯度消失：Relu, leak-Relu
3. 循环网络，矩阵相乘与梯度爆炸： 
4. Pooling：特性提取，几何变换不变性  
5. Dropout与Bagging：  
6. 正则化  
7. 卷积  
8. 批量归一化与泛化能力  

##如何设计一个卷积神经网络来实现图像分类
####BP网络
在本章第一节，我们介绍了如何实现一个误差反向的神经网络。因此，我们可以把一张图转化为一个向量，输入到BP网络中，再根据总的类别来设置输出向量的长度，比如1000个分类就让输出的维度是1000.这样我们可以实现一个很简单的用于图像分类的神经网络。    
当然这只是一个很粗浅的模型，因为它没有利用到图像固有的一些特性，或者说图像识别这个问题背后的先验知识。

####先验与熵值
#####卷积[^1]
全连接认为下层的输出与上层的所有元素都有关系，上层的每个元素的作用是均等的。而卷积操作，基于这样一个假设，网络下一层的元素只与上一层网络的部分元素有关(稀疏连接，权值共享)。这相当于全连接来说，卷积相当于加了一个矩形窗，这是一个很强的先验。但正是由于有这个先验，使得我们的计算量远远小于全连接网络。  
卷积导致了参数共享，而参数共享的先验就是平移不变性。  
或者从另外一个角度来说，卷积，这种稀疏连接的方式，区别于全连接在于，它认为下一层每个神经元只与上一层特定区域的神经元有相互作用，或者说，与其它神经元的关联权重为0，也就是从全局作用变成局部作用，就像从一个方差很大的高斯分布变成一个方差很小的高斯分布。这是一个强先验，所谓的强先验就是具有小的熵值，弱先验具有大的熵值。强先验对应更确定的知识，未知性小，而弱先验就是未知性大，极端的就是没有先验，也就是说不对模型做任何假设，比如全连接网络。  
#####池化
能显著降低参数数量，还能保持平移，伸缩，旋转操作的不变性。  

[^1]: 《深度学习》9.6节



