# 决策树与集成学习

决策树是一种基本的回归与分类的方法。决策树由节点与边构成，节点分类内部节点与叶子节点；内部节点表示属性或者特征，叶子节点表示一个类。

## 决策树学习

训练集$$D = ((x_1, y_1),(x_2, y_2)...(x_N, y_N))$$其中$$x_i = (x_i^(1),x_i^(2)...x_i^(n))$$是n维变量，n表示特征的数目，$$y_i \in (1,2...K)$$类标签。训练的本质是基于一定标准得到一组分类规则。我们需要得到一组与训练数据矛盾较小并且泛化能力也好的分类规则，其中泛化能力说的是在测试集上其错误率也小，错误率有不同的定义方式，可以是均方误差，也可以是似然函数，一般回归问题选择均方误差，分类问题选择似然函数，**想想这是为什么？**。  
  分类规则就是特征选择的过程，特征选择是基于一些可以量化的函数，一般基于信息熵增益最大或者信息熵增益率最大或者Gini系数下降最大的规则。因此，接下来我们要定义如下概念。

1. 信息熵
2. 条件熵
3. 信息增益
4. 信息增益率
5. Gini系数

   ### 1. 信息熵

   熵的概念来自于统计物理，描述微观系统的混乱程度，由Rudolf Clausius提出，  
   $$S = k_B ln \Omega =-k_B\sum _i p_i \ln p_i $$  
   其中$$k_B$$是Boltzmann constant,$$\Omega$$表示系统的可能状态数。  
   它表示的就是$$ln(p_i)$$的期望值。香农把这个概念引入信息学，定义了信息熵。  
   考虑一个只能取有限n个值的系统，值对应的就是状态，比如投硬币，只能取两个值，就是说只能有2个状态，因此n=2，在统计物理中，状态对应的是系统的能级，也就是一个能级对应一个状态，系统处于不同能级的概率就是$$p_i$$  
   $$P(X = x_i) = p_i, i = 1,2..n$$  
   熵的定义如下：$$H(x) = -\sum_{i}^{i=n} p_i \ln p_i$$  
   若系统是确定的，则有一个$$p_i = 1$$，其他的$$p_i = 0$$等于0，因此代入熵的公式可知，熵为0.可以证明，对于n个状态的系统，系统的熵满足如下不等式：  
   $$0 \le H(x) = -\sum_{i}^{i=n} p_i ln p_i \le \ln n$$.

   ### 2. 条件熵

   联合概率说的是两个及两个系统随机变量共同发生的几率问题，条件熵$$H(Y|X)$$说的是随机变量X给定的情况下，随机变量Y的条件熵。  
   计算如下：  
   $$H(Y|X) = \sum_{i}^{i=n} p_i H(Y|X=x_i)$$  
   这里， $$p_i = P(X=x_i), i=1,2..n$$。  
   $$H(Y|X=x_i)$$计算与上面的信息熵一样，只是对于$$H(Y|X=x_i)$$，我们只计算$$X = x_i$$的那些样本的熵，因为我们会对所有X取不同值得样本进行求和，因此会遍历整个样本。

   ### 3. 信息增益

   信息增益是基于以上两个概念的，是针对于特征而言的，特征A对训练数据集D的信息增益g\(D,A\)定义成经验熵H\(D\)与特征A给定的条件下，D的经验条件熵H\(D\|A\)之差。即：  
   $$g(D,A) = H(D) - H(D|A)$$。  
   如果我们知道数据集D的信息熵计算，以及条件熵的计算，则信息增益的计算不难。

   ### 4. 信息增益率

   特征A对训数据集D的信息增益率$$g_R(D,A)$$定义成信息增益g\(D,A\)与特征A的经验熵H\(A\)之比：  
   $$g_R(D,A) = \frac{g(D,A)}{H(A}$$。 
   其中$$H(A) = \sum_{i=1}^{i=K} p_i \ln p_i$$,K是特征A能取不同值得数目，$$p_i$$是取不同值得概率。 
   他的计算基于信息增益与信息熵。

   ### 5. Gini系数

   基尼系数定义：对于分类问题，假设有K个类，样本点处于第k个类的概率是$$p_k$$,则概率分布的基尼系数定义成：  
   $$Gini(p) = \sum_{i}^{i=n} p_i(1 - p_i) = 1 - \sum_{i=1}^{i=K} p_i^2$$  
   基尼系数描述的是不同类之间的不一致程度，一个类与其它类的不一致概率和，也就是不一致程度程度，说的是从样本集合D中随便挑出两个样本，他们属于不同类的概率。 由下面函数刻画$$p_i(1 - p_i)$$.对于分类问题，我们希望最终的叶子节点里面，所有的样本点尽可能使同一个类里面的，也就是基尼系统尽可能趋近于0.  
   假设样本D中有K个类，$$C_k$$是第k类的集合，$$|C_k|$$是集合$$C_k$$的大小，因此样本集合的基尼系数定义成:  
   $$Gini(p) = 1 - \sum_{i=1}^{i=K} (\frac{|C_k|}{|D|})^2$$

基于不同的指标，比如信息增益，信息增益率，基尼系数，我们会得到不同的决策树生成算法，依次是ID3， C4.5, CART。

![](/assets/Entropy_Gini.png)

信息熵与基尼系数的函数大致一致，实际上在iris数据集中，生成决策树时，两者没有差别。

## ID3算法
ID3算法基于信息增益最大，代码实现如下，它的缺点在于，它倾向于选择取值很多的特征，因为当特征能取很多值得时候，此时系统的不确定度降低，极端情况是，特征能取N个不同的值，N个训练集的数量，此时特征A条件熵为0. 为了避免这种情况，而引入了C4.5算法。
假设特征A是信息增益最大的特征，当特征A取离散值时，假设特征A可以取K个不同值，我们选择A作为特征之后会生出K个节点，对于每个节点，我们要重复上面的步骤，继续寻找信息增益最大的特征，只是，这时候特征数目减小了1，减小的这个特征就是A特征本身。当一个节点，它的熵小于一定阈值的时候，我们就不再分割这个节点而是把他当成一个叶子节点。我们可以使用递归来实现ID3算法。
算法实现的时候要注意的一点就是，计算熵时，不能让概率等于0，否则会出错。
```py
import scipy
import numpy as np
from sklearn import tree
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

class DecisionTree(object):
    def __init__(self):
        self.training_steps = 50
        self.initial_x = [[1],[1]]#np.ones((2,1))

    def Entropy(self, Prob):
        entropy = 0
        for n in range(Prob.shape[0]):
            entropy -= Prob[n] * (np.log2(Prob[n])) ##Entropy
            #entropy += Prob[n] * (1 - Prob[n])  ##Gini
        return entropy

    def distribution(self, feature_data, throshold_n):
        entropy = 0
        prob = 0.001*np.ones((3, 1))
        for n in range(0, throshold_n):
            for index in range(3):
                if np.abs(feature_data[n, 1] - index) < 0.01:
                    prob[index] += 1
        prob = prob/throshold_n
        p_1 = (1.0*throshold_n)/feature_data.shape[0]
        H_1 = self.Entropy(prob)
        entropy += p_1*H_1

        prob_2 = 0.001*np.ones((3, 1))
        for n in range(throshold_n, feature_data.shape[0]):
            for index in range(3):
                if round(feature_data[n, 1]) == index:
                    prob_2[index] += 1
        prob_2 = prob_2/(feature_data.shape[0] - throshold_n)
        H_2 = self.Entropy(prob_2)
        entropy += (1 - p_1) * H_2
        return entropy

    def best_feature_threshold(self, data):
        best_threshold = 0
        best_feature = 0
        best_entropy = 100000
        feature_num = data.shape[1] - 1
        for feature_index in range(feature_num):
            a_arg = np.argsort(iris_data[:, feature_index])
            sorted_data = iris_data[a_arg]
            for n in range(1, sorted_data.shape[0]):
                threshold = (sorted_data[n-1, feature_index] + sorted_data[n, feature_index])/2
                featue_result_data = np.zeros((sorted_data.shape[0], 2))
                featue_result_data[:, 0] = sorted_data[:, feature_index]
                featue_result_data[:, -1] = sorted_data[:, -1]
                condition_entropy = self.distribution(featue_result_data, n)
                if condition_entropy < best_entropy:
                    best_threshold = threshold
                    best_entropy = condition_entropy
                    best_feature = feature_index
            print best_entropy, best_feature, best_threshold
        return best_entropy, best_feature, best_threshold

if __name__ == '__main__':
    DT = DecisionTree()
    iris = load_iris()
    clf = tree.DecisionTreeClassifier()
    clf = clf.fit(iris.data, iris.target)
    data = iris.data
    target = iris.target
    iris_data = np.zeros((data.shape[0], data.shape[1] + 1))
    iris_data[:, 0:data.shape[1]] = data
    iris_data[:, -1] = target
    best_entropy, best_feature, best_threshold = DT.best_feature_threshold(iris_data)
    print best_entropy, best_feature, best_threshold
    np.savetxt("C:\Data\Group\ShareFolder\Optimization\ML\iris_data.csv", data)
    np.savetxt("C:\Data\Group\ShareFolder\Optimization\ML\\target.csv", target)
    np.savetxt("C:\Data\Group\ShareFolder\Optimization\ML\\iris_data.csv", iris_data)
```

##C4.5
C4.5算法弥补了ID3算法的不足，通过使用信息增益率来作为特征的选择标准。
$$g_R(D,A) = \frac{g(D,A)}{H(A}$$。
其中$$H(A) = \sum_{i=1}^{i=K} p_i \ln p_i$$,K是特征A能取不同值得数目，$$p_i$$是取不同值得概率。 

##模型的剪枝
为什么需要剪枝：提高泛化能力，
我们可以一步步的细分节点而使得系统的分类误差很小，但是这只是训练数据集上的结果，当我们把模型运用到测试集时，误差率会很高，这是因为模型过拟合你，我们可以通过对决策树进行剪枝来提高模型的泛化能力。
为了进行剪枝，我们得定义剪枝的标准，也就是定义损失函数，损失函数至少要包括两项，一个是模型在训练集上的误差，一个是模型的复杂程度。模型的复杂程度可以定义成与叶子节点正相关。模型在训练集上的误差率可以如下定义。
$$C(T) = \sum_{t=1}^{t=|T|} N_t H_t$$
其中$$N_t$$是叶子节点t上的样本点数目，$$H_t$$是叶子节点t的熵。
叶子节点的熵定义为：
$$H_t = -\sum_{k=1}^{k=K}\frac{N_{tk}}{N_t}\ln \frac{N_{tk}}{N_t}$$,
其中$$k \in (1,2...K)$$是样本结果可以取不同值的数目，也就是标签有K类$$N_t$$是叶子节点t的样本数目， $$N_{tk}$$是叶子节点t中标签属于k类的数目。总体而言，C(T)刻画的是模型在训练集上的误差。结合这两部分，我们可以定义剪枝的损失函数：
$$C_{\alpha}(T) = C(T) + \alpha |T|$$
其中正数$$\alpha$$用来权衡模型的误差率与模型的复杂度。

##CART
决策树的生成就是递归的构建二叉决策树的过程。对回归树用平方误差最小的准则，对分类树用基尼指数最小化准则，进行特征选择，生成二叉树。

