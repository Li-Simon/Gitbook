# 降维与无监督学习
## 集成学习

集成学习就是组合一系列的弱分类器生成强分类器。组合的弱分类器之间的关系有两种:

一种是彼此间相互依赖,一系列学习器之间需要串行生成，代表算法是Boosting系列算法，代表算法是AdaBoost与GBDT.  
一种是学习器彼此间无关联，一些列算法可以并行的生成，代表算法是Bagging\(Bootstrap Aggregating\)和随机森林（Random Forest）系列。随机森林是时Bagging的进行版，随机表现在两个方面，一方面是取样本点是随机的，另一方面，选择特征也是随机的（比如总共有N个特征，随机选取小于N的K个特征）  
还有一种是两者的结合物：stacking

### PAC，弱可学习，强可学习

PAC\(Probably approximately correct\)   
强可学习：如果存在一个多项式的学习算法学习呀，学习的正确率很高  
弱可学习：如果存在一个多项式的学习算法学习呀，学习的正确率仅比随机猜测略好  
在PAC学习框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。

### 集成学习的理论基础

我们考虑多个不相干分类器叠加处理二分类的问题，$$y \in (-1,+1)$$和真实的函数f,假设基分类器的错误率都是$$\epsilon$$, 即对每个分类器都有  
$$\kern{10 em}P(h_i(x) \ne f(x)) = \epsilon$$  
假设集成通过简单的投票发结合T个基分类器，若半数的基分类器正确，则集成分类正确。  
       $$\kern{10 em}H(x) = sign(\sum _{i=1}^{T}h_i(x))$$  
 价格基分类器之间的错误率是相互独立的，则由Hoeffding不等式可知，集成的错误率为：  
 $$\kern{10 em}P(H(x) \ne f(x)) = \displaystyle\sum_{i=0}^{[T/2]}\begin{pmatrix}
   T \\
   k
\end{pmatrix}(1 - \epsilon)^k\epsilon ^{T-k} \le exp(-\frac{1}{2}T(1-2\epsilon)^2)$$  
上式表明，随着集成中个体分类器数目T的增大，集成的错误率将指数下降，最终趋向于0.

### Boosting:

Boosting系列方法的原理图如下：

![](/assets/Boosting.png)

#### AdaBoost

Boosting系列算法的代表就是AdaBoost。  
算法如下：  
输入：训练数据集 $$T = ((x_1, y_1),(x_2, y_2)...(x_N, y_N))$$,其中$$x_i \in X \subseteq R^{n}, y \in (-1, +1)$$  
输出： 最终的分类器G\(x\)  
\(1\)初始化训练数据集的权重分布  
$$\kern{10 em}D_1 = (w_{11},...,w_{1i},...,w_{1N})$$,  
$$\kern{10 em}w_{1i} = \frac{1}{N}, i =1, 2,...,N$$  
\(2\)对m=1,2,...,M  
使用具有权值分布$$D_{m}$$的训练数据集进行训练，得到基分类器  
$$\kern{10 em}G_m(x): \chi \in (-1, +1)$$  
\(b\)计算$$G_m(x)G_m(x)$$在训练集上的分类误差率：  
$$\kern{6 em}e_m = P(G_m(x_i) \ne y_i) = \displaystyle\sum_{i=1}^N w_{mi}I(G_m(x_i) \ne y_i)$$  
\(c\)计算$$G_m(x)$$的系数  
$$\kern{10 em}\alpha _m = \frac{1}{2}\log \frac{1-e_m}{e_m}$$  
这里用的是自然对数。  
\(d\)更新训练数据集的权值分布：  
$$\kern{10 em}D_1 = (w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N})$$,  
$$\kern{10 em}w_{m+1,i} = \frac{w_{mi}}{Z_m}exp(-\alpha _m y_i G_m(x_i)), i =1, 2,...,N$$  
这里，$$Z_m$$是归一化因子：   
$$\kern{10 em}Z_m =\displaystyle\sum_{i=1}^N w_{mi}exp(-\alpha _m y_i G_m(x_i))$$  
它使$$D_{m+1}$$成为一个概率分布。  
\(3\)构建基本分类器的线性组合：  
$$\kern{10 em} f(x) = \displaystyle\sum_{i=1}^N \alpha _m G_m(x)$$  
得到最终分类器  
$$\mkern{10 em} G(x) = sign(f(x)) = sign(\displaystyle\sum_{i=1}^N \alpha _m G_m(x))$$  
由$$\alpha _m$$的定义可以知道，第m个分类器的误差率越小，则$$\alpha _m$$的值越大，因此第m个分类器在总的分类器中占的比重越大。因此，AdaBoost会加大那些准确率很高的分类器的权重。  
同时，在第m+1个分类器求解时，对于上一轮被分错的样本的权值会变大。  
不改变所给的训练数据，而不断的改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用，这是AdaBoost的一个特点。  
可以参考《统计机器学习》8.1节的例子来加深理解。

##### AdaBoost算法的训练误差分析

定理：**\(AdaBoost的训练误差界\)**   
AdaBoost 算法最终分类器的训练误差界为：  
$$\kern{4 em}\frac{1}{N}\displaystyle\sum_{i=1}^N I(G_m(x_i) \ne y_i) \le \frac{1}{N} \displaystyle\sum_{i=1}^N exp(-y_if(x_i)) = \displaystyle \prod_{m} Z_m$$

定理**\(二分类问题AdaBoost的训练误差界\)**  
$$\kern{4 em} \displaystyle \prod_{m} Z\_m = \displaystyle \prod_{m=1}^{M}[2\sqrt{e_m(1-e_m)}] =   
\displaystyle \prod_{m=1}^{M}\sqrt{1 - 4\gamma _m^2} \le exp(-2\displaystyle \sum_{m=1}^{M}\gamma \_m^2)$$
其中： $$\gamma _m = \frac{1}{2} - e_m$$
#### GBDT

提升树是以分类树或者回归树为基本分类器的提升方法，提升树被认为是统计学习中性能最好的方法之一。  
  提升树实际采用加法模型\(即基函数的线性组合\)与前向分步算法。 以决策树为基函数的提升方法称为提升树。 对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。  
  决策树桩\(decsion stump\)：可以看成是一个根节点直接连接两个叶节点的简单决策树。  提升树模型可以表示为决策树的加法模型。  
  $$\kern{4 em} f_M(x) = \displaystyle \sum_{m=1}^{M}T(x;\Theta_m)$$  
  其中：$$T(x, \Theta_m)$$表示决策树；$$\Theta_m$$为决策树的参数；M为树的个数。

梯度提升算法：   
提升树算法采用前向分步算法。首先缺点初始提升树$$f_0(x)=0$$,第m步的模型是:  
$$\kern{4 em}f_m(x) = f_{m-1}(x) + T(x;\Theta_m)$$  
其中，$$f_{m-1}(x)$$为当前模型，通过经验风险极小化来确定下一刻决策树的参数$$\Theta_m$$  
$$\kern{4 em} \hat \Theta_m = arg min_{\Theta_m} \sum_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))$$.  
对于一个训练数据集 $$T=((x_1,y_1),(x_2,y_2),...,(x_N,y_N)),x_i \in \chi \in R^N$$, $$\chi$$为输入控件， $$y_i \in R$$,为输出空间。如果将输入控件$$\chi$$划分为J个互不相交的区域$$R_1,R_2,...,R_J$$，并且在每个区域上确定输出的常量$$c_j$$，那么树可以表示为：  
$$\kern{4 em} T(x;\theta)= \displaystyle \sum_{j=1}^J c_j I(x\in R_j)$$.  
其中，参数$$\Theta = ((R_1,c_1),(R_2,c_2),...,(R_J,c_J))$$表示树的区域划分和各区域上的常数。J是回归树的发杂都，即叶节点个数。  
回归为题提升树使用以下前向分步算法：  
$$\kern{4 em} f_0(x) = 0$$  
$$\kern{4 em}f_m(x) = f_{m-1}(x) + T(x;\Theta_m), m=1,2,...,M$$  
$$\kern{4 em} f_m(x) = \displaystyle \sum_{m=1}^M T(x;\Theta_m) $$  
在前向分步算法的第m步，给定当前模型$$f_{m-1}(x)$$，需求解：  
$$\kern{4 em} \hat \Theta_m = arg min_{\Theta_m} \sum_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))$$.  
得到$$\hat \Theta_m$$，即第m颗树的参数。  
采用平方误差损失函数时：  
$$\kern{4 em}  L(y,f(x)) = (y-f(x))^2$$  
其损失变成：  
$$\kern{4 em}  L(y,f_{m-1}(x) + T(x;\Theta_m)) = (y-f_{m-1}(x) - T(x;\Theta_m))^2 = [r - T(x;\Theta_m)]^2$$  
这里，  
  $$\kern{4 em}  r = y-f_{m-1}(x)$$  
  是当前模型拟合数据的残差。所以，对回归问题的提升树算法来说，只需要简单地你和当前模型的残差。

##### 梯度提升

当损失函数是平方损失和指数损失函数时，每一步优化是很简单的，但是对于一般损失函数而言，往往每一步优化并不容易。针对这一问题，Freidman提出了梯度梯度提升\(gradient boosting\)算法,这是利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值。  
$$\kern{4 em} -[\frac{\partial L(y, f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}$$  
作为回归问题提升算法中的残差的近似值，拟合一个回归树。  
梯度提升树算法  
输入： 训练数据集 $$T=((x_1,y_1),(x_2,y_2),...,(x_N,y_N)),x_i \in \chi \in R^N$$, $$\chi$$为输入， $$y_i \in R$$，损失函数为$$L(y,f(x))$$:  
输出： 回归树$$\hat f(x)$$.  
\(1\)初始化  
$$\kern{4 em} f_0(x) = arg min_{c} \displaystyle\sum_{i=1}^NL(y_i,c)$$.  
\(2\)对m=1,2,...,M  
\(a\)对i=1,2,...,N，计算  
$$\kern{4 em} r_{mi} = -[\frac{\partial L(y, f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}$$  
（b）对$$r_{mi}$$拟合一个回归树，得到第m课树叶节点区域$$R_{my},j=1,2,..,J$$  
\(c\)对j=1,2,..,J, 计算  
$$\kern{4 em}c_{mj} = arg min_{c} \displaystyle \sum_{x_i \in R_{mj}}L(y_i,f_{m-1}(x_i)+c)$$.  
\(d\)更新$$f_m(x) = f_{m-1}(x) + \displaystyle \sum_{j=1}^Jc_{mj}I(x \in R_{mj}), m=1,2,...,M$$  
\(3\)得到回归树  
$$\kern{4 em}\hat f(x) = f_M(x) = \displaystyle \sum_{m=1}^M\displaystyle \sum_{j=1}^Jc_{mj}I(x \in R_{mj})$$

### Bagging

Bagging系列方法的原理图如下：

#### Random Forest![](/assets/Bagging.png)



