# 决策树与集成学习

决策树是一种基本的回归与分类的方法。决策树由节点与边构成，节点分类内部节点与叶子节点；内部节点表示属性或者特征，叶子节点表示一个类。

## 决策树学习

训练集$$D = ((x_1, y_1),(x_2, y_2)...(x_N, y_N))$$其中$$x_i = (x_i^(1),x_i^(2)...x_i^(n))$$是n维变量，n表示特征的数目，$$y_i \in (1,2...K)$$类标签。训练的本质是基于一定标准得到一组分类规则。我们需要得到一组与训练数据矛盾较小并且泛化能力也好的分类规则，其中泛化能力说的是在测试集上其错误率也小，错误率有不同的定义方式，可以是均方误差，也可以是似然函数，一般回归问题选择均方误差，分类问题选择似然函数，**想想这是为什么？**。  
  分类规则就是特征选择的过程，特征选择是基于一些可以量化的函数，一般基于信息熵增益最大或者信息熵增益率最大或者Gini系数下降最大的规则。因此，接下来我们要定义如下概念。

1. 信息熵
2. 条件熵
3. 信息增益
4. 信息增益率
5. Gini系数
### 1. 信息熵
熵的概念来自于统计物理，描述微观系统的混乱程度，由Rudolf Clausius提出，
$$S = k_B ln \Omega =-k_B\sum _i p_i \ln p_i $$
其中$$k_B$$是Boltzmann constant,$$\Omega$$表示系统的可能状态数。
它表示的就是$$ln(p_i)$$的期望值。香农把这个概念引入信息学，定义了信息熵。
考虑一个只能取有限n个值的系统，值对应的就是状态，比如投硬币，只能取两个值，就是说只能有2个状态，因此n=2，在统计物理中，状态对应的是系统的能级，也就是一个能级对应一个状态，系统处于不同能级的概率就是$$p_i$$
$$P(X = x_i) = p_i, i = 1,2..n$$
熵的定义如下：$$H(x) = -\sum_{i}^{i=n} p_i \ln p_i$$
若系统是确定的，则有一个$$p_i = 1$$，其他的$$p_i = 0$$等于0，因此代入熵的公式可知，熵为0.可以证明，对于n个状态的系统，系统的熵满足如下不等式：
$$0 \le H(x) = -\sum_{i}^{i=n} p_i ln p_i \le \ln n$$.
###2. 条件熵
联合概率说的是两个及两个系统随机变量共同发生的几率问题，条件熵$$H(Y|X)$$说的是随机变量X给定的情况下，随机变量Y的条件熵。
计算如下：
$$H(Y|X) = \sum_{i}^{i=n} p_i H(Y|X=x_i)$$
这里， $$p_i = P(X=x_i), i=1,2..n$$。
$$H(Y|X=x_i)$$计算与上面的信息熵一样，只是对于$$H(Y|X=x_i)$$，我们只计算$$X = x_i$$的那些样本的熵，因为我们会对所有X取不同值得样本进行求和，因此会遍历整个样本。
###3. 信息增益
信息增益是基于以上两个概念的，是针对于特征而言的，特征A对训练数据集D的信息增益g(D,A)定义成经验熵H(D)与特征A给定的条件下，D的经验条件熵H(D|A)之差。即：
$$g(D,A) = H(D) - H(D|A)$$。
如果我们知道数据集D的信息熵计算，以及条件熵的计算，则信息增益的计算不难。

### 4. 信息增益率
特征A对训数据集D的信息增益率$$g_R(D,A)$$定义成信息增益g(D,A)与训练数据集D的经验熵H(D)之比：
$$g_R(D,A) = \frac{g(D,A)}{H(D}$$。
他的计算基于信息增益与信息熵。
### 5. Gini系数
基尼系数定义：对于分类问题，假设有K个类，样本点处于第k个类的概率是$$p_k$$,则概率分布的基尼系数定义成：
$$Gini(p) = \sum_{i}^{i=n} p_i(1 - p_i) = 1 - \sum_{i=1}^{i=K} p_i^2$$
基尼系数描述的是不同类之间的不一致程度，一个类与其它类的不一致概率和，也就是不一致程度程度，说的是从样本集合D中随便挑出两个样本，他们属于不同类的概率。 由下面函数刻画$$p_i(1 - p_i)$$.
假设样本D中有K个类，$$C_k$$是第k类的集合，$$|C_k|$$是集合$$C_k$$的大小，因此样本集合的基尼系数定义成:
$$Gini(p) = 1 - \sum_{i=1}^{i=K} (\frac{|C_k|}{|D|})^2$$




