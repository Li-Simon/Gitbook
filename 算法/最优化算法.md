# 最优化

[最速下降法，牛顿法，LBFGS](http://www.cnblogs.com/maybe2030/p/4751804.html)

1.梯度下降法（Gradient Descent）

梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。

**梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，梯度趋于0， 所以步长越小，前进越慢。**

梯度下降法的搜索迭代示意图如下图所示：

![](/assets/Algo_GradientDescent.png)

梯度下降法的缺点：

（1）越靠近极小值的地方收敛速度越慢，如下图所示

![](/assets/Algo_GradientDescentDisadv1.png)

从上图可以看出，梯度下降法在接近最优解的区域收敛速度明显变慢，利用梯度下降法求解需要很多次的迭代。

在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。

比如对一个线性回归（Linear Logistics）模型，假设下面的h\(x\)是要拟合的函数，J\(theta\)为损失函数，theta是参数，要迭代求解的值，theta求解出来了那最终要拟合的函数h\(theta\)就出来了。其中m是训练集的样本个数，n是特征的个数。  
$$h(\theta) = \sum^{n}_{j = 0}\theta_{k}x_{j} $$
$$\tag{1.2} J(\theta) = \frac{1}{2m}\sum_{i = 0}^{m}(y^i - h_{\theta}(x^i))^2$$  
//n是特征的个数, m的训练样本的数目

# 1\)批量梯度下降法（Batch Gradient Descent, BDG）

\(1\)将$$J(\theta)$$对$$\theta$$求偏导，得到每个$$\theta$$对应的梯度：  
$$\frac{\partial J(\theta)}{\partial \theta_{j}} = -\frac{1}{m}\sum_{i=1}^m(y^i - h_{\theta}(x^i))x_{j}^{i}\tag{1.3}$$  
\(2\)由于要最小化风险函数，所以按照每个参数$$\theta$$的负梯度方向来更新$$\theta$$  
$$\theta_{j}' = \theta{j} - \frac{\partial J(\theta)}{\partial \theta_{j}} = \theta{j} + \frac{1}{m}\sum_{i=1}^m(y^i - h_{\theta}(x^i))x_{j}^{i}\tag{1.4}$$

（3）从上面公式可以注意到，它得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果m很大，那么可想而知这种方法的迭代速度会相当的慢。所以，这就引入了另外一种方法——随机梯度下降。  
一个实验结果，说明随机梯度下降法能收敛到最终结果：

对于批量梯度下降法，样本个数m，x为n维向量，一次迭代需要把m个样本全部带入计算，迭代一次计算量为m\*n2。

# 2\)随机梯度下降（Stochastic Gradient Descent, SGD）

\(1\)上面的风险函数可以写成如下这种形式，损失函数对应的是训练集中每个样本的粒度，而上面批量梯度下降对应的是所有的训练样本

$$J(\theta) = \frac{1}{2m}\sum_{i = 0}^{m}(y^i - h_{\theta}(x^i))^2 = 
\frac{1}{m}\sum_{i=1}^{m}cost(\theta,(x^i,y^i))\tag{1.5}$$  
$$cost(\theta,(x^i,y^i)) = \frac{1}{2}(y^i - h_{\theta}(x^i))^2\tag{1.6}$$  
\(2\)每个样本的损失函数，对$$\theta$$求偏导得到对应的梯度，来更新$$\theta$$  
$$\theta_{j}' = \theta_{j} + (y^i - h_{\theta}(x^i))x_{j}^{i}\tag{1.7}$$  
\(3\)随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将$$\theta$$迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。  
随机梯度下降每次迭代只使用一个样本，迭代一次计算量为n2，当样本个数m很大的时候，随机梯度下降迭代一次的速度要远高于批量梯度下降方法。两者的关系可以这样理解：随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。

# 比较批量梯度下降法与随机梯度下降法：

用$$y = 2*x1 + x2 -1 + rand(0,1)$$产生1000组数，用这一组数据来反求产生函数中的系数$$(2,1,-1)^T$$.  
迭代停止条件就是，训练得到的相邻两次参数的差的范数小于0.000001\(严格来说，停止条件是损失函数一介导数等于0，也就是$$L'(\theta) = -\frac{1}{m}X^T(X\theta - Y) = 0$$\).  
1）批量梯度下降法：1000个样本，设置的learn rate = 0.0001, 一次迭代所有1000个样本，最终经过38000次迭代收敛到可以接受的标准。如果设置learn rate 是较大的值，比如为1，则发现结果马上发散了，因此批量梯度下降法对learn rate比较敏感，而下面的随机梯度下降法就不会出现这个问题，因为每次只对一个样本进行处理，即使这个样本对参数的梯度很大，但是下一个样本又可以马上把参数拉回来。  
![](/assets/BatchGradientDescentTest1.png)

2\)随机梯度下降法：

1000个样本，设置的learn rate = 1, 一次迭代处理一个样本，最终经过25000次迭代收敛到可以接受的标准。

![](/assets/StocGradientDescentTest1.png)  
3\)牛顿法

![](/assets/NetwonIterationMethodsTest1.png)

对批量梯度下降法和随机梯度下降法的总结：  
　　批量梯度下降---最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。  
　　随机梯度下降---最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。  
**从实验数据来看，批量梯度下降法收敛到的是全局最优值，而随机梯度下降法收敛到最优值附件的地方。随机梯度下降法的好处是收敛远快于批量梯度下降法。**

问题可以转化成寻找一组$$\lambda$$使得:$$L(\lambda) = (X*\lambda - Y)^T(X*\lambda - Y)$$极小  
利用$$L(\lambda)$$对向量$$\lambda$$[求导](https://zhuanlan.zhihu.com/p/24709748)可以得到[^1]：  
$$\frac{\partial L(\lambda)}{\partial \lambda} = 2X^T(X\lambda -Y) = 0$$  
我们也可以得到：  
$$\frac{\partial^2 L(\lambda)}{\partial \lambda^2} = 2X^TX\tag{Grad.8}$$

X\(n+1\) = X\(n\) - f'\(X\(n\)\)/f''\(X\(n\)\)

```cpp
MatrixXd Iteration::BathGradDescent(MatrixXd paras, int iterator_num)
{
    int rows = m_input_data.rows();
    int cols = m_input_data.cols();
    MatrixXd init_paras = paras;
    if (m_input_data.cols() == paras.rows())
    {

        MatrixXd grad = (1.0 / rows)*m_input_data.transpose()*(m_output_data - m_input_data*init_paras);
        paras += m_learn_rate*grad;
    }
    else
    {
        //error
    }
    return paras;
}

MatrixXd Iteration::StocGradDescent(MatrixXd paras, int index)
{
    int rows = m_input_data.rows();
    int cols = m_input_data.cols();
    MatrixXd input_data_i(1,cols);
    MatrixXd output_data_i(1, 1);
    output_data_i(0, 0) = m_output_data(index,0);

    for (int i = 0; i < cols; i++)
    {
        input_data_i(0, i) = m_input_data(index, i);
    }


    if (m_input_data.cols() == paras.rows())
    {
        MatrixXd grad = (1.0 / rows) *input_data_i.transpose()*(input_data_i*paras - output_data_i);    
        paras = paras - grad;
    }
    else
    {
        //error
    }
    return paras;
}
```

由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是"切线法"。牛顿法的搜索路径（二维情况）如下图所示：

牛顿法搜索动态示例图：

# 牛顿法和拟牛顿法（Newton's method &Quasi-Newton methods）

1\)牛顿法（Newton's method）  
牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f \(x\)的泰勒级数的前面几项来寻找方程f \(x\) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。[^1]  
步骤：  
首先，选择一个接近函数f\(x\)零点的$$x_{0}$$,计算相应的计算相应的 $$f(x_0)$$ 和切线斜率$$f'(x_0)$$。然后计算穿过点$$(x_0, f(x_0))$$并且斜率为$$f'(x_0)$$的直线和x轴的交点的x轴坐标，也就是如下方程：  
$$x*f'(x_{0}) + f(x_0) -x_{0}*f'(x_0) = 0\tag{N.1}$$  
我们将新求得的点的 x 坐标命名为$$x_{1}$$，通常$$x_{1}$$会比$$x_{0}$$更接近方程$$f(x)=0$$的解。因此我们现在可以利用$$x_{1}$$开始下一轮迭代。迭代公式可化简为如下所示：  
$$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}\tag{N.2}$$  
$$\mathbf{X_{n+1}} = \mathbf{X_{n}} - \frac{1}{1}$$  
如果X是向量，则可以写成向量形式：  
　已经证明，如果$$f'(x)$$是连续的，并且待求的零点x是孤立的，那么在零点x周围存在一个区域，只要初始值$$x_{0}$$位于这个邻近区域内，那么牛顿法必定收敛。 并且，如果$$f'(x)$$不为0, 那么牛顿法将具有平方收敛的性能. 粗略的说，这意味着每迭代一次，牛顿法结果的有效数字将增加一倍。下图为一个牛顿法执行过程的例子。

![](/assets/Algo_NewtonMethods1.png)

**关于牛顿法和梯度下降法的效率对比：**

**从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）**

**根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。**

![](/assets/Algo_NewtonMethods2.png)

注：红色的牛顿法的迭代路径，绿色的是梯度下降法的迭代路径。

**牛顿法的优缺点总结：**

**优点：二阶收敛，收敛速度快；**

**缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。**

2）拟牛顿法（Quasi-Newton Methods）

拟牛顿法是求解非线性优化问题最有效的方法之一，于20世纪50年代由美国Argonne国家实验室的物理学家W.C.Davidon所提出来。Davidon设计的这种算法在当时看来是非线性优化领域最具创造性的发明之一。不久R. Fletcher和M. J. D. Powell证实了这种新的算法远比其他方法快速和可靠，使得非线性优化这门学科在一夜之间突飞猛进。

拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。  
　　具体步骤：  
　　拟牛顿法的基本思想如下。首先构造目标函数在当前迭代$$x_k$$的二次模型：  
$$m_k(p) = f(x_k) + \bigtriangledown f(x_k)^Tp + \frac{p^TB_{k}p}{2}\tag{N.3}$$  
$$p_k = - B^{-1}_{k}\bigtriangledown f(x_k)\tag{N.4}$$  
这里$$B_k$$是一个对称正定矩阵，于是我们取这个二次模型的最优解作为搜索方向，并且得到新的迭代点：  
$$x_{k+1} = x_{k} + \alpha_kp_k\tag{N.5}$$  
其中我们要求步长$$\alpha_k$$满足Wolfe条件。这样的迭代与牛顿法类似，区别就在于用近似的Hesse矩阵$$B_k$$ 代替真实的Hesse矩阵。所以拟牛顿法最关键的地方就是每一步迭代中矩阵Bk 的更新。现在假设得到一个新的迭代$$x_k+1$$，并得到一个新的二次模型：

这个公式被称为割线方程。常用的拟牛顿法有DFP算法和BFGS算法。  
[最优化方法：牛顿迭代法和拟牛顿迭代法](https://blog.csdn.net/pipisorry/article/details/24574293)

# 共轭梯度法

共轭梯度法（Conjecture Gradient）是介于最速下降法与牛顿法之间的一个方法，它仅需要一阶导数信息，但克服了最速下降法收敛慢的缺点，同时又避免了牛顿法需要储存和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化问题最有效的算法之一。
向量共轭的定义：
若$$\mathbf{A}$$是正定对称阵，若非0矢量$$\mathbf{u,v}$$满足，$$\mathbf{u^{T}Av} = 0$$，则称矢量$$\mathbf{u,v}$$是共轭的。
假设：
$$P = \mathbf{[p_1,p_2,...,p_n]}$$是一组基于$$\mathbf{A}$$的共轭矢量，则$$\mathbf{Ax = b}$$的解$$\mathbf{x^*}$$可以表示为：
$$\mathbf{x^*} = \sum_{i=1}^{n}{\alpha}_i\mathbf{P_i} $$,因此基于基矢量展开，我们有：
$$\mathbf{Ax_*} = \sum_{i=1}^{n}{\alpha}_i\mathbf{AP_i} $$
左乘$$\mathbf{P^T_{k}}$$：
$$\mathbf{P^T_{k}Ax_*} = \sum_{i=1}^{n}{\alpha}_iP^T_{k}\mathbf{AP_i} \tag{CG.0}$$
代入：$$\mathbf{\mathbf{P^T_{k}Ax_*} = b}$$,$$\mathbf{u^TAv = \left\langle u,v \right\rangle_A}$$,利用$$i != k$$有$$\mathbf{\left\langle p_k,p_i \right\rangle _A = 0}$$ 就得到：

如果我们的小心的选择$$\mathbf{p_k}$$,为了获得解$$\mathbf{x_*}$$的一个好的近似，我们并不需要所有的基向量。因此，我们把共轭梯度算法当成一种迭代算法，它也允许我们近似n很大，以至于直接求解需要花费太多时间的系统。
我们给$$\mathbf{x_*}$$一个初始猜测值$$\mathbf{x_0}$$，假设$$\mathbf{x_0 = 0}$$,在求解的过程中，我们需要一种标准来告诉我们是否我们的解更靠近真实的解$$\mathbf{x_*}$$,实际上，求解$$\mathbf{Ax = b}$$等价于求解如下二次函数的极小值：
$$\mathbf{f(x) = \frac{1}{2}x^TAx - x^Tb}\tag{CG.1}$$,
他存在唯一的最小值，因为他的二阶导是正定的：
$$\mathbf{D^2f(x) = A}\tag{CG.2}$$,
解就是一阶导数等于0的地方
$$\mathbf{Df(x) = Ax - b}\tag{CG.3}$$,
假设第一个基矢$$\mathbf{p_0}$$就是$$f(x)$$在$$\mathbf{x = x_0}$$处的负梯度，我们可以假设$$\mathbf{x_0 = 0}$$.因此：
$$\mathbf{p_0 = b - Ax_0}\tag{CG.4}$$.
令$$\mathbf{r_0}$$表示第k步的残差：
$$\mathbf{r_k = b - Ax_k}$$,$$\mathbf{r_k}$$是$$\mathbf{f(x)}$$在$$\mathbf{x = x_k}$$处的负梯度。
为了让$$\mathbf{p_k}$$与前面的所有的$$\mathbf{p_i}$$相互共轭，$$\mathbf{p_k}$$可以如下构造：
$$\mathbf{p_k = r_k - \sum _{i < k}\frac{p_i^TAr_{k}}{p_i^TAp_i}}\tag{CG.5}$$
沿着这个方向，因此下一步的优化方向就是：
$$\mathbf{x_{k+1} = x_k + {\alpha}_kp_k}$$,满足：
$$g'({\alpha}_k = 0)$$
因此：
$$\mathbf{f(x_{k+1}) = f(x_k + {\alpha}_kp_k) = :g({\alpha}_k)}\tag{CG.6}$$,
$$\mathbf{\alpha _k= \frac{p_k^Tb}{p_k^TAp_k} = \frac{p_k^T(r_k + Ax_k)}{p_k^TAp_k} = \frac{p_k^Tr_k}{p_k^TAp_k}}\tag{CG.7}$$
算法：
$$\mathbf{r_0 := b - Ax_0}$$
$$\mathbf{p_0 := r_0}$$
$$\mathbf{k := 0}$$
repeat:
$$\mathbf{\alpha _k := r_0}$$
$$\mathbf{\alpha _k := \frac{r_k^Tr_k}{p_k^TAp_k}}$$
$$\mathbf{x_{k+1} := x_k + \alpha _kp_k}$$
$$\mathbf{r_{k+1} := r_k + \alpha _kAp_k}$$
if $$r_{k+1}$$ is sufficiently small, then exit loop.
$$\mathbf{\beta _k := \frac{r_{k+1}^Tr_{k+1}}{r_k^Tr_k}}$$
$$\mathbf{p_{k+1} := r_{k+1} + \beta _kp_k}$$
$$\mathbf{k := k+1}$$



protected Vector FillDataAbsolute(Vector f, Matrix J, Variable variable)










































