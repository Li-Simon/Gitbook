# 逻辑回归
在这里我们以多项逻辑斯蒂回归为例，讨论怎么用最大化似然函数来求解这一类问题。  
假设离散性随机变量Y的取值是(1,2,...,K),输入变量$$\mathbf{X} \in R^n$$  
则，模型如下：  
$$\kern{4 em} P(Y=k|x) = \frac{exp(\mathbf{w_k}\mathbf{x})}{1+\displaystyle \sum_{k=1}^{K-1}exp(\mathbf{w_k}\mathbf{x})}, k=1,2,...,K-1$$  
$$\kern{4 em} P(Y=K|x) = \frac{1}{1+\displaystyle \sum_{k=1}^{K-1}exp(\mathbf{w_k}\mathbf{x})}$$   
这里$$\mathbf{x} \in \mathbf{R^{n+1}};\mathbf{w_k} \in \mathbf{R^{n+1}}$$.  
因此，总的似然函数可以表示为：  
$$\kern{4 em}L(\mathbf{W}) = \displaystyle \prod _{n=1}^NP(Y=Y_n|X=X_n)$$ 
这样，我们需要求的参数是$$W = (\mathbf{w_1},\mathbf{w_2},...,\mathbf{w_{n+1}})$$.

最简单的就是二项逻辑斯蒂回归，在这里K=2:
似然函数可以写成如下简单的形式：  
$$\kern{4 em}L(\mathbf{W}) = \displaystyle \prod _{n=1}^N[\pi(x_i)^{y_i}][1-\pi(x_i)]^{1-y_i}$$  
其中：
$$\kern{4 em}P(Y=1|x) =\pi(x) = \frac{exp(\mathbf{w*x})}{1+exp(\mathbf{w*x})}$$   
$$\kern{4 em} P(Y=0|x) =1-\pi(x)$$   
当$$y_i = 0$$时:  
$$\kern{4 em} [\pi(x_i)^{y_i}][1-\pi(x_i)]^{1-y_i}=1-\pi(x_i)$$ 
当$$y_i = 1$$时:  
$$\kern{4 em} [\pi(x_i)^{y_i}][1-\pi(x_i)]^{1-y_i}=\pi(x_i)$$  
实际上分别是$$y_i = 0$$,$$y_i = 1$$发生的概率。  
一般连乘形式的似然函数容易发生值溢出，故而一般求对数似然函数：  
$$\kern{4 em}L(\mathbf{W}) = \displaystyle \sum _{n=1}^N[y_i\log\pi(x_i) + (1-y_i)\log[1-\pi(x_i)]$$  
$$\kern{7 em} = \displaystyle \sum _{n=1}^N[y_i(w*x_i) + \log(1+exp(w*x_i))$$   
求$$L(\mathbf{W})$$的极大值，得到$$\mathbf{w}$$的估计值。  
问题转化为以对数似然函数为目标函数的最优化问题，一般采用梯度下降法或者拟牛顿法求解。(为啥不使用牛顿法？因为求Hassian矩阵比较麻烦，而拟牛顿法只需要构造Hassian阵就可以了，用迭代法求)  

#最大熵模型
##最大熵原理
最大熵原理是概率模型学习的一个准则，最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合。所以，最大熵原理也可以表述为在满足约束条件的模型集合中，选取熵最大的模型。   
怎么建立起最大熵原理与最大似然原理之间的联系？  
最大熵模型如下：  
对于给定的训练数据集：
$$T=((x_1,y_1),(x_2,y_2),...,(x_N,y_N))$$  
以及特征函数$$f_i(\mathbf{x},\mathbf{y}),i=1,2,...,n$$，最大熵模型的学习等价于约束最优化问题：  
$$\kern{4 em} \displaystyle \max_{P \in C} H(P) = \displaystyle \sum_{x,y}\displaystyle \hat P(x)P(y|x)\log P(y|x)$$  
$$\kern{4 em}  S.t. E_P(f_i) = E_{\hat P}(f_i), i=1,2,...,n$$
$$\kern{4 em}  \displaystyle \sum _y P(y|x) = 1$$  
可以转化为求极小问题：  
$$\kern{4 em} \displaystyle \min_{P \in C} H(P) = -\displaystyle \sum_{x,y}\displaystyle \hat P(x)P(y|x)\log P(y|x)$$  
$$\kern{4 em}  S.t. E_P(f_i) - E_{\hat P}(f_i) = 0, i=1,2,...,n$$
$$\kern{4 em}  \displaystyle \sum _y P(y|x) = 1$$  

一般，带约束的优化问题可以通过引入拉格朗日乘子与KKT乘子来求解，也就是把原始问题转化为对偶问题来求解。在满足KKT条件时，对偶问题与原始问题等价：  
首先引入拉格朗日乘子：$$w_0, w_1,...,w_n$$,定义拉格朗日函数$$L(P,\mathbf{w})$$;  
$$\kern{4 em}L(P,\mathbf{w}) = -H(P) + w_0(1-\displaystyle \sum _y P(y|x)) + \displaystyle \sum _{i=1}^n w_i(E_P(f_i) - E_{\hat P}(f_i))$$  
$$\kern{8 em}= -\displaystyle \sum_{x,y}\displaystyle \hat P(x)P(y|x)\log P(y|x) + w_0(1-\displaystyle \sum _y P(y|x)) $$   
$$\kern{9 em} +\displaystyle \sum _{i=1}^n w_i(\displaystyle \sum_{x,y}\displaystyle \hat P(x,y)f_i(x,y) - \displaystyle \sum_{x,y}\displaystyle \hat P(x)P(y|x)f_i(x,y))$$  
最优化的原始问题是：  
$$\kern{8 em} \displaystyle \min_{P \in C}\displaystyle \max_{\mathbf{w}}L(P,\mathbf{w})$$   
最优化的对偶问题是：  
$$\kern{8 em} \displaystyle \max_{\mathbf{w}}\displaystyle \min_{P \in C}L(P,\mathbf{w})$$  
由于拉格朗日函数$$L(P,\mathbf{w})$$是P的凸函数，因此原始问题与对偶问题的解释等价的。（为什么？，用KKT条件能解析吗？）  
令： $$\kern{2 em}  \Psi(\mathbf{w}) = \displaystyle \min_{P \in C}L(P,\mathbf{w})$$  
$$\Psi(\mathbf{w})$$称为对偶函数。 同时，将其解记为：  
$$\kern{8 em} P_{\mathbf{w}} = arg \displaystyle \min_{P \in C}L(P,\mathbf{w})$$.  
具体计算就是$$L(P,\mathbf{w})$$对P求导，并让其等于0.    
   
$$\kern{4 em} \frac{\partial L(P,W)}{\partial P(y|x)} = \displaystyle \sum_{x,y}\hat P(x)(\log P(y|x) + 1) - \displaystyle \sum_{y}w_0 - \displaystyle \sum_{x,y}(\hat P(x)\displaystyle \sum_{i=1}^nw_if_i(x,y))$$  
$$\kern{8 em} = \displaystyle \sum_{x,y}\hat P(x)(\log P(y|x) + 1 - w_0 - \displaystyle \sum_{i=1}^nw_if_i(x,y))$$  
令其偏导数等于0，在$$\hat P(x)>0$$的情况下，解得：  
$$\kern{4 em} P(y|x) = \exp(\displaystyle \sum_{i=1}^nw_if_i(x,y)) + w_0 - 1)$$  
由于： $$\displaystyle \sum _y P(y|x) = 1$$,得：  
$$\kern{4 em} P_w(y|x) = \frac{1}{Z_w(x)} \exp(\displaystyle \sum_{i=1}^nw_if_i(x,y)))$$  
其中：   
$$\kern{4 em} Z_w(x) = \displaystyle \sum_{y} \exp(\displaystyle \sum_{i=1}^nw_if_i(x,y)))$$  
其中：$$Z_w(x)$$称为规范化因子，$$f_i(x,y)$$称为特征函数，$$w_i$$是特征函的权重。  
其实这些就可以和统计物理学中的波尔兹曼分布联系在一起了，上面的推导也适用于波尔兹曼分布的推导。这也不奇怪，因为最大熵原理的理论就是脱胎于统计物理学。  
之后再求解对偶问题外部的极大化问题：  
$$\kern{8 em}\displaystyle \max_w \Psi(\mathbf{w})$$  
将其解记作$$w^*$$,即：  
$$\kern{8 em} w^* = arg \displaystyle \max_w \Psi(\mathbf{w})$$  
因为最大熵模型是一个带约束的优化问题，因此可任意通过定于拉格朗日函数，通过对变量求极值得到对偶函数，再通过对偶函数来求解。又因为在这里，对偶函数等价于对数似然函数，因此可以求对数似然函数的最大化。  
###模型学习的最优化算法
我们已知模的概率分布：  
$$\kern{4 em} P_w(y|x) = \frac{1}{Z_w(x)} \exp(\displaystyle \sum_{i=1}^nw_if_i(x,y)))$$  
因此模型的对数似然函数是：  
$$\kern{4 em} L(w) = \displaystyle \sum_{x,y}P(x,y)\displaystyle \sum_{i=1}^nw_if_i(x,y) - \displaystyle \sum_{x}P(x)\log Z_w(x)$$  
$$\kern{4 em} L(w) = \displaystyle \sum_{x,y}P(x,y)\displaystyle \sum_{i=1}^nw_if_i(x,y) - \displaystyle \sum_{x}P(x)\log \displaystyle \sum_{y} \exp(\displaystyle \sum_{i=1}^nw_if_i(x,y)))$$  
求解上面函数的最大值，可以通过拟牛顿法或者改进的迭代尺度法来求解。(有时间就加上这两个算法)
####BFGS
输入：特征函数$$f_1,f_2,...,f_n$$；经验分布P(x,y),目标函数f(w),梯度$$g(w)=\bigtriangledown f(w)$$,精度要求$$e$$




