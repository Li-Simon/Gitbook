# RNN

建模序列化数据的一种主流深度学习模型。

## 设计目的

捕获长距离输入之间的依赖。

### 背景：

传统的前馈神经网络一般输入的都市一个定长的向量，无法处理变长的序列信息，即使通过一些方法把序列处理成定长的向量，模型也很难捕捉序列中的长距离依赖关系。RNN则通过将神经元串行起来处理序列化的数据。由于每个神经元能用它的内部变量保存之前输入的序列信息，因此整个序列被浓缩成抽象的表示，并可以据此进行分类或生成新的序列。近年来，得益于计算能力的大幅度提升和模型的改进，RNN在很多领域取得了突破性的进展--机器翻译，图像描述，推荐系统，智能聊天机器人，自动作词作曲等。
 
###RNN, LSTM, GRU比较
#####传统RNN vs 门控RNN
传统RNN的问题 ：长期依赖会造成梯度消失（多数情况下） 或梯度爆炸（少数情况下）。  
解决以上问题的方案：  
1. 以新的方法改善或者代替传统的SGD方法，如Bengio提出的clipped gradient;   
2. 设计更为精妙的activation function或recurrent unit，如LSTM和GRU。原因：LSTM和GRU都能通过各种Gate将重要特征保留，保证其在long-term 传播的时候也不会被丢失；还有一个作用就是有利于BP的时候不容易梯度消失。   

传统RNN和门控RNN的不同点：  
1. 传统RNN会每一步都重写”记忆“，而门控RNN可以在某些步骤保持原有”记忆“；  
2. 门控RNN收敛速度更快，泛化能力更好。   

#####LSTM vs GRU  
相同点； 都属于”加模型“  
不同点：   
1. LSTM可控”记忆“的曝光度，而GRU完全暴露”记忆“，是不可控的；
2. LSTM的新”记忆“是与forget gate独立的，而GRU的新”记忆“受update gate控制
 
从LSTM和GRU的公式里面可以看出，都会有门操作，决定是否保留上时刻的状态和是否接收此时刻的外部输入，LSTM是用遗忘门和输入门来做到的，GRU则是只用一个更新门\($$z_t$$\)。
这种设计有两种解释，一种解释是说，网络是能很容易记住长依赖问题。即前面很久之前出现过一个重要的特征，如果遗忘门或者更新门选择不重写内部的memory，那么网络就会一直记住之前的重要特征，那么会对当前或者未来继续产生影响。另一点是，这种设计可以不同状态之间提供一条捷径，那么梯度回传的时候不会消失的太快，因此减缓了梯度消失带来的训练难问题。  
LSTM 和GRU的不同点。首先LSTM有一个输出门来控制memory content的曝光程度，而GRU则是直接输出。另外一点是要更新的new memory content的来源不同, $$\hat h_t$$会通过重置门控制从$$h_{t-1}$$ 中得到信息的力度，而$$\hat c_t$$则没有，而是直接输入$$h_{t-1}$$。  


## 注意力机制

[^1] RNN-LSTM-GRU-最小GRU  [https://www.jianshu.com/p/166db8ab3cef](https://www.jianshu.com/p/166db8ab3cef)  
[^2] RNN LSTM与GRU深度学习模型学习笔记  [https://blog.csdn.net/softee/article/details/54292102](https://blog.csdn.net/softee/article/details/54292102)  
[^3] RNN以及LSTM的介绍和公式梳理  https://blog.csdn.net/Dark_Scope/article/details/47056361  

 