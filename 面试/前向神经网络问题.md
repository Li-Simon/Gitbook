# 前向神经网络问题
####为啥Sigmoid和Tanh激活函数会导致梯度消失的现象？
因为在参数很大或者很小的时候，他们的导数会趋于0，因此造成了梯度消失。  
####ReLU系列激活函数相对于Sigmoid和Tanh激活函数的有点事什么？它们有什么局限性以及如何改进？  
#####优点
(1)计算梯度简单，因为Sigmod和Tanh涉及到指数运算  
(2)ReLU能有效解决后两者的题都消失问题，提供相对宽的激活边界   
(3)ReLU的单侧抑制提供了网络的稀疏表达能力  
#####缺点
ReLU的局限性在于其训练过程中会导致神经元死亡的问题，由于激活函数和是$$mf(x) = \max(0,z)$$导致梯度在经过该ReLU单元时，被置为0。且之后再也不能被任何数据激活，因为流经这个神经元的梯度永远为0，因此不对任何数据产生响应。因此会导致一定比例的神经元不可逆的死亡，进而参数梯度永远无法更新，整个训练过程失败。  
为了解决这一问题，人们设计了Leaky ReLU(LReLU)其表达式是：  
$$\kern{8 em}f(z) = z; z> 0$$    
$$\kern{8 em}f(z) = az; z\le 0$$    
一般a是一个很小的正数，但是怎么选取a也是一个问题。为解决这个问题，人们发明了参数化的PReLU(Parametric ReLU),它与LReLU的主要区别是，斜率参数a作为网络中一个可学习的参数。而另一个LReLU的变种增加了"随机化"机制，具体的，在训练过程中，斜率a作为一个满足某种分布的随机采样；测试时再固定下来。Random ReLU(RReLU)在一定程度上能起到正则化的作用。  
####平方误差损失函数和交叉熵损失函数分别适合什么场景？  
平方损失函数:回归问题，输出是连续值，并且最后一层不含Sigmoid或者Softmax激活函数的NN。因为会导致梯度很小而发生梯度消失的问题  
交叉熵：分类问题，
###神经网络训练技巧
主要涉及的问题是“过拟合”：解决方法包括数据集增强，正则化，模型集成。其中dropout是模型集成方法中最高效与常用的技巧。此外，DNN训练中设计的手动调参，如学习率，权重衰减系数，Dropout比例等，这些参数的选择会显著影响模型最终的训练效果。Batch Normalization(BN)方法有效规避了这些复杂参数对网络训练产生的影响，在加速训练收敛的同时也提升了网络的泛化能力。  
####神经网络训练时是否可以将全部参数初始化为0？  
因为所有参数都是相同的值，所有神经元都是对称的，因此没有办法打破这种对称，导致无论学习多久，所有参数依然是相同的。因此我们需要随机地初始化NN参数的值，来打破这种对称性。  
####为啥Dropout可以抑制过拟合？它的工作原理和实现？  
还得先介绍Dropout的实际步骤，也就是训练的时候是多少个神经元？预测的时候是多少个？   
Dropout作用于小批量训练数据，由于其随机丢弃部分神经元的机制，相当于每次迭代都是在训练不同结构的神经网络，类比于Bagging方法，Dropout可被认为书一种实用的大规模深度神经网络的模型集成算法。这是由于传统意义上的Bagging涉及多个模型的同时训练和测试评估，当网络与参数规模庞大时，这种集成方式需要消耗大量的运算时间与空间。Dropout在小批量级别上的操作，提供了一种轻量级的Bagging集成近似，能够实现指数级数量神经网络的训练与评测。  
Dropout在具体实现中，某个神经元以概率p被丢弃，因此N个神经元在Dropout下相当于是$$2^N$$个模型的集成。这$$2^N$$个模型可认为是原始网络的子网络，它们共享部分权值，并且具有相同的网络层数，而模型的整体参数数目不变，这就大大简化了运算。对于任意神经元，每次训练中都与一组随机挑选的不同的神经元集合共同进行优化，这个过程会减弱全体神经元之间的联合适应性，减小过拟合的风险，增强泛化能力。
#####批量归一化的基本动机与原理是什么？在卷积神经网络中如何使用？
神经网络训练过程的本质是学习数据分布，如果训练数据与测试数据的分布不同将大大降低网络的泛化能力，因此我们需要在训练开始前对所有输入数据进行归一化处理。  
归一化方法是针对每一批数据（比如m个样本），在网络的每一层的输入之前增加归一化处理（均值为0，方差为1），将所有批处理数据强制在统一的数据分布下，即对该层的任一神经元（假设是第k维）$$\hat x^{(k)}$$采用如下公式：  
$$\kern{8 em} \hat x^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}$$  
其中$$x^{(k)}$$为该层第k个神经元的原始输入数据，$$E[x^{(k)}]$$为这一批输入数据在第k个神经元的均值，$$\sqrt{Var[x^{(k)}]}$$为这一批数据在k个审计元的标准差。  
####卷积操作的本质特性包括系数交互和参数共享，具体解释这两种特性以及作用。
######稀疏交互
对于全连接网络，如果第k层有m个神经元，第k+1层有n个神经元，怎两层间的权重数目是mn,对于卷积网络，如果卷积核大小为k,则这两层间的权重个数是kn,因为后面一层，每一个神经元只与前一层k个神经元相连。一般k远远小于m，因此相对于全连接网络而言，参数数量大大减少了，因此具有稀疏交互的特征。  
稀疏交互的物理意义是，通常图像，文本，语音等现实世界中的数据都具有局部的特征结构，我看可以先学习局部的特征，再将局部的特征组合起来形成复杂和抽象的特征。以人脸识别为例，最底层的神经元可以检测出各个角度的边缘特征，中间层的神经元可以将边缘组合起来得到眼睛，鼻子，嘴巴等复特征；最后，位于上层的神经元可以根据各个器官的组合检测出人脸的特征。  
######参数共享
参数共享是指在同一个模型的不同模块中使用相同的参数，它是卷积运算的固有属性。在学习的过程中，我们学习到的就是卷积核（的参数）。根据参数共享的思想，我们只需要学习一组参数集合，而不需要针对每个位置的每个参数都进行优化，从而大大降低了模型的储存需求。  
参数共享的物理意义是使得卷积具有平移不变性。假如图像中有一只猫，那么无论它出现图像中的什么位置，都应该将它识别为猫，也就是说神经网络的输出对于平移变换具有不变性。  
####常用的池化操作有哪些？池化的作用是什么？   
常用的池化操作主要针对非重叠区域，包括均值（mean pooling）,最大池（max pooling）。mean pooling是用一个区域的平均值代替这个区域，能够抑制由于领域大小受限造成估计值方差增大的现象，特点是对背景的保留效果更好；max pooling是用一个区域的最大值代替这个区域本身，能够抑制网络参数误差造成估计均值偏移现象，特点是更好地提取纹理信息。  
池化操作的本质是降采样。除了能保显著降低参数数量，还能够保持对平移，伸缩，旋转操作的不变性。平移不变性是对小量平移而言，尺度一般是pooling的尺寸。伸缩（尺度）不变性，因为pooling就是一个能保持区域主要信息（如最大值）的压缩变换，因此具有伸缩不变性。旋转不变性，在配合多重pooling以后，对于max pooling，对于旋转的图像，我们仍能得到相同的结果，比如图像数字5。