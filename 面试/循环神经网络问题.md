# 循环神经网络问题

建模序列化数据的一种主流深度学习模型。

###### 背景：

传统的前馈神经网络一般输入的都市一个定长的向量，无法处理变长的序列信息，即使通过一些方法把序列处理成定长的向量，模型也很难捕捉序列中的长距离依赖关系。RNN则通过将神经元串行起来处理序列化的数据。由于每个神经元能用它的内部变量保存之前输入的序列信息，因此整个序列被浓缩成抽象的表示，并可以据此进行分类或生成新的序列。近年来，得益于计算能力的大幅度提升和模型的改进，RNN在很多领域取得了突破性的进展--机器翻译，图像描述，推荐系统，智能聊天机器人，自动作词作曲等。

#### 处理文本数据时，循环神经网络与前馈神经网络相比有什么特点？

Softmax function:  
$$\kern{8 em} softmax(\mathbf{x}_i) = \frac{\exp(x_i)}{\displaystyle \sum_{j=1}^n \exp(x_j)}$$

### 展开图计算

![](/assets/RNN_Unfold_map.png)  
当前神经元的输入是，系统上一个的状态$$h^{(t-1)}$$，以及外部信号$$x^{(t)}$$,因此当前神经元的输出状态是：  
$$\kern{8 em} h^{(t)} = f(h^{(t-1)},x^{(t)};\mathbf{\theta})$$  
可以看出，当前状态包含整个过去序列的信息。  
相对于循环图，展开图有如下两个优点：  
\(1\)无论序列长度，学成的模型始终具有相同的输入大小；因为它指的是从一种状态到另外一种状态的转移，而不是在可变长度的历史状态上操作。  
\(2\)我们可以在每个时间步使用相同参数的相同转移函数f。  
这两个因素使得学习在所有时间步和所有序列长度上操作单一的模型f是可能的，而不需要在所有时间步学习独立的模型$$g^{(t)}$$.  
$$\kern{8 em} h^{(t)} =  g^{(t)}(x^{(t)},x^{(t-1)},...,x^{(2)},x^{(1)})$$  
$$ \kern{10 em}= f(h^{(t-1)},x^{(t)};\mathbf{\theta})$$

### 循环神经网络

循环神经网络中一些重要的设计模式包含如下几种：  
\(1\)每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络，如下图所示。  
![](/assets/RNN_General.png)  
\(2\)每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络。如下图所示：  
![](/assets/RNN_DesignPattern_2.png)  
\(3\)隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络，如下图所示：  
![](/assets/RNN_DesignPattern_3.png)
这三种结构分别用于什么情况？  
