# 逻辑回归
在这里我们以多项逻辑斯蒂回归为例，讨论怎么用最大化似然函数来求解这一类问题。  
假设离散性随机变量Y的取值是(1,2,...,K),输入变量$$\mathbf{X} \in R^n$$  
则，模型如下：  
&emsp;&emsp;$$ P(Y=k|x) = \frac{exp(\mathbf{w_k}\mathbf{x})}{1+\displaystyle \sum_{k=1}^{K-1}exp(\mathbf{w_k}\mathbf{x})}, k=1,2,...,K-1$$  
&emsp;&emsp;$$P(Y=0|x) = \frac{1}{1+\displaystyle \sum_{k=1}^{K-1}exp(\mathbf{w_k}\mathbf{x})}$$   
这里$$\mathbf{x} \in \mathbf{R^{n+1}};\mathbf{w_k} \in \mathbf{R^{n+1}}$$.  
因此，总的似然函数可以表示为：  
&emsp;&emsp;$$L(\mathbf{W}) = \displaystyle \prod _{n=1}^NP(Y=Y_n|X=X_n)$$ 
这样，我们需要求的参数是$$W = (\mathbf{w_1},\mathbf{w_2},...,\mathbf{w_{n+1}})$$.

最简单的就是二项逻辑斯蒂回归，在这里K=2:
似然函数可以写成如下简单的形式：  
&emsp;&emsp;$$L(\mathbf{W}) = \displaystyle \prod _{n=1}^N[\pi(x_i)^{y_i}][1-\pi(x_i)]^{1-y_i}$$  
其中：
&emsp;&emsp;$$P(Y=1|x) =\pi(x) = \frac{exp(\mathbf{w*x})}{1+exp(\mathbf{w*x})}$$   
&emsp;&emsp;$$P(Y=0|x) =1-\pi(x)$$   
当$$y_i = 0$$时:  
&emsp;&emsp;$$ [\pi(x_i)^{y_i}][1-\pi(x_i)]^{1-y_i}=1-\pi(x_i)$$ 
当$$y_i = 1$$时:  
&emsp;&emsp;$$[\pi(x_i)^{y_i}][1-\pi(x_i)]^{1-y_i}=\pi(x_i)$$  
实际上分别是$$y_i = 0$$,$$y_i = 1$$发生的概率。  
一般连乘形式的似然函数容易发生值溢出，故而一般求对数似然函数：  
&emsp;&emsp;$$L(\mathbf{W}) = \displaystyle \sum _{n=1}^N[y_i\log\pi(x_i) + (1-y_i)\log[1-\pi(x_i)]$$  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$= \displaystyle \sum _{n=1}^N[y_i(w*x_i) - \log(1+exp(w*x_i))$$   
因此：  
&emsp;&emsp;$$\frac{\partial L(\mathbf{w})}{\partial \mathbf{w}} = \displaystyle \sum _{n=1}^N[y_i*x_i - x_i\pi_i] = \displaystyle \sum _{n=1}^Nx_i[y_i - \pi_i]$$  
所以：    
&emsp;&emsp;$$w := w - \alpha \displaystyle \sum _{n=1}^Nx_i[y_i - \pi_i]$$
求$$L(\mathbf{W})$$的极大值，得到$$\mathbf{w}$$的估计值。  
问题转化为以对数似然函数为目标函数的最优化问题，一般采用梯度下降法或者拟牛顿法求解。(为啥不使用牛顿法？因为求Hassian矩阵比较麻烦，而拟牛顿法只需要构造Hassian阵就可以了，用迭代法求)  

#最大熵模型
##最大熵原理
最大熵原理是概率模型学习的一个准则，最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合。所以，最大熵原理也可以表述为在满足约束条件的模型集合中，选取熵最大的模型。   
怎么建立起最大熵原理与最大似然原理之间的联系？  
最大熵模型如下：  
对于给定的训练数据集：
&emsp;&emsp;$$T=((x_1,y_1),(x_2,y_2),...,(x_N,y_N))$$  
以及特征函数$$f_i(\mathbf{x},\mathbf{y}),i=1,2,...,n$$，最大熵模型的学习等价于约束最优化问题：  
&emsp;&emsp;$$ \displaystyle \max_{P \in C} H(P) = -\displaystyle \sum_{x,y}\displaystyle \hat P(x)P(y|x)\log P(y|x)$$  
&emsp;&emsp;s.t.&emsp;$$  E_P(f_i) = E_{\hat P}(f_i), i=1,2,...,n$$      
&emsp;&emsp;&emsp;$$ \displaystyle \sum _y P(y|x) = 1$$  
可以转化为求极小问题：  
&emsp;&emsp;$$\displaystyle \min_{P \in C} H(P) = -\displaystyle \sum_{x,y}\displaystyle \hat P(x)P(y|x)\log P(y|x)$$  
&emsp;&emsp;s.t. &emsp;$$ E_P(f_i) - E_{\hat P}(f_i) = 0, i=1,2,...,n$$    
&emsp;&emsp;$$ \displaystyle \sum _y P(y|x) = 1$$  

一般，带约束的优化问题可以通过引入拉格朗日乘子与KKT乘子来求解，也就是把原始问题转化为对偶问题来求解。在满足KKT条件时，对偶问题与原始问题等价：  
首先引入拉格朗日乘子：$$w_0, w_1,...,w_n$$,定义拉格朗日函数$$L(P,\mathbf{w})$$;  
&emsp;&emsp;$$L(P,\mathbf{w}) = -H(P) + w_0(1-\displaystyle \sum _y P(y|x)) + \displaystyle \sum _{i=1}^n w_i(E_P(f_i) - E_{\hat P}(f_i))$$  
&emsp;&emsp;&emsp;&emsp;$$= -\displaystyle \sum_{x,y}\displaystyle \hat P(x)P(y|x)\log P(y|x) + w_0(1-\displaystyle \sum _y P(y|x)) $$   
&emsp;&emsp;&emsp;&emsp;$$ +\displaystyle \sum _{i=1}^n w_i(\displaystyle \sum_{x,y}\displaystyle \hat P(x,y)f_i(x,y) - \displaystyle \sum_{x,y}\displaystyle \hat P(x)P(y|x)f_i(x,y))$$  
最优化的原始问题是：  
&emsp;&emsp;$$\displaystyle \min_{P \in C}\displaystyle \max_{\mathbf{w}}L(P,\mathbf{w})$$   
最优化的对偶问题是：  
&emsp;&emsp;$$ \displaystyle \max_{\mathbf{w}}\displaystyle \min_{P \in C}L(P,\mathbf{w})$$  
由于拉格朗日函数$$L(P,\mathbf{w})$$是P的凸函数，因此原始问题与对偶问题的解释等价的。（为什么？，用KKT条件能解析吗？）  
令： &emsp;&emsp;$$\Psi(\mathbf{w}) = \displaystyle \min_{P \in C}L(P,\mathbf{w})$$  
$$\Psi(\mathbf{w})$$称为对偶函数。 同时，将其解记为：  
&emsp;&emsp;$$P_{\mathbf{w}} = arg \displaystyle \min_{P \in C}L(P,\mathbf{w})$$.  
具体计算就是$$L(P,\mathbf{w})$$对P求导，并让其等于0.    
   
&emsp;&emsp;$$\frac{\partial L(P,W)}{\partial P(y|x)} = \displaystyle \sum_{x,y}\hat P(x)(\log P(y|x) + 1) - \displaystyle \sum_{y}w_0 - \displaystyle \sum_{x,y}(\hat P(x)\displaystyle \sum_{i=1}^nw_if_i(x,y))$$  
&emsp;&emsp;&emsp;&emsp;$$= \displaystyle \sum_{x,y}\hat P(x)(\log P(y|x) + 1 - w_0 - \displaystyle \sum_{i=1}^nw_if_i(x,y))$$  
令其偏导数等于0，在$$\hat P(x)>0$$的情况下，解得：  
&emsp;&emsp;$$P(y|x) = \exp(\displaystyle \sum_{i=1}^nw_if_i(x,y)) + w_0 - 1)$$  
由于： $$\displaystyle \sum _y P(y|x) = 1$$,得：  
&emsp;&emsp;$$ P_w(y|x) = \frac{1}{Z_w(x)} \exp(\displaystyle \sum_{i=1}^nw_if_i(x,y)))$$  
其中：   
&emsp;&emsp;$$Z_w(x) = \displaystyle \sum_{y} \exp(\displaystyle \sum_{i=1}^nw_if_i(x,y)))$$  
其中：$$Z_w(x)$$称为规范化因子，$$f_i(x,y)$$称为特征函数，$$w_i$$是特征函的权重。  
其实这些就可以和统计物理学中的波尔兹曼分布联系在一起了，上面的推导也适用于波尔兹曼分布的推导。这也不奇怪，因为最大熵原理的理论就是脱胎于统计物理学。  
之后再求解对偶问题的极大化问题：  
&emsp;&emsp;$$\displaystyle \max_w \Psi(\mathbf{w})$$  
将其解记作$$w^*$$,即：  
&emsp;&emsp;$$w^* = arg \displaystyle \max_w \Psi(\mathbf{w})$$  
因为最大熵模型是一个带约束的优化问题，因此可任意通过定于拉格朗日函数，通过对变量求极值得到对偶函数，再通过对偶函数来求解。又因为在这里，对偶函数等价于对数似然函数，因此可以求对数似然函数的最大化。  
###模型学习的最优化算法
我们已知模的概率分布：  
&emsp;&emsp;$$P_w(y|x) = \frac{1}{Z_w(x)} \exp(\displaystyle \sum_{i=1}^nw_if_i(x,y)))$$  
因此模型的对数似然函数是：  
&emsp;&emsp;$$L(w) = \displaystyle \sum_{x,y}P(x,y)\displaystyle \sum_{i=1}^nw_if_i(x,y) - \displaystyle \sum_{x}P(x)\log Z_w(x)$$  
&emsp;&emsp;$$ L(w) = \displaystyle \sum_{x,y}P(x,y)\displaystyle \sum_{i=1}^nw_if_i(x,y) - \displaystyle \sum_{x}P(x)\log \displaystyle \sum_{y} \exp(\displaystyle \sum_{i=1}^nw_if_i(x,y)))$$  
求解上面函数的最大值，可以通过拟牛顿法或者改进的迭代尺度法来求解，当然最典型的求解方法是IIS(Improved Iterative Scaling)。(有时间就加上这两个算法)
####BFGS
输入：特征函数$$f_1,f_2,...,f_n$$；经验分布P(x,y),目标函数f(w),梯度$$g(w)=\bigtriangledown f(w)$$,精度要求$$\epsilon$$。  
输出：最优化参数$$w^*$$;最优模型$$P_{w^*}(y|x)$$。    
(1)选定初始点$$w^{0}$$,取$$B_0$$为正定对称函数，置k=0;    
(2)计算$$g_k = g(w^{k})$$,若$$||g_k||< \epsilon$$，则停止计算，得到$$w^* = w^{k}$$；否则转到(3)    
(3)由$$B_kp_k=-g_k$$得出p_k;    
(4)一维搜索，求$$\lambda _k$$使得:  
&emsp;&emsp;$$(w^{(k)}+\lambda _k p_k) = \displaystyle \min_{\lambda \ge 0} f(w^{(k)}+\lambda p_k)$$    
(5)置： $$w^{(k+1)} = w^{(k)} + \lambda _k p_k$$    
(6)计算$$g_{k+1} = g(w^{(k+1)})$$，若$$||g_{k+1}||< \epsilon$$，则停止计算，得$$w^* = w^{(k+1)}$$,否则按下面方式求$$B_{k+1}$$。    
&emsp;&emsp;$$ B_{k+1} = B_{k} + \frac{y_ky_k^T}{y_k^T\delta_k} - \frac{B_k\delta_k \delta_k^TB_k^T}{\delta_k^TB_k\delta_k}$$   
其中：   
&emsp;&emsp;$$y_k = g_{k+1} - g_k$$    
&emsp;&emsp;$$\delta_k = w^{(k+1)} - w^{(k)}$$    
(7)置k=k+1,转到(3)   

