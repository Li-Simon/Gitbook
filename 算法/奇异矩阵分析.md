# 奇异矩阵分析

1. SVD

2. PCA

3. Condition Number

## SVD

对于任何一个DxM的矩阵X可以做如下分解：  
&emsp;&emsp;$$ X = UDV^T$$  
其中$$U^TU=I_D,VTV=I_N$$,D是对角矩阵。  
&emsp;&emsp;$$XX^T = UDV^TVDU^T=UD^2U^T$$  
&emsp;&emsp;$$X^TX = VDU^TUDV^T=VD^2V^T$$  
是矩阵的本征值分解。因此可以通过上面两式来求解U，V。主要用到的是矩阵本征值，本征矢求解技巧。

## PCA

我们可以通过SVD来实现PCA,具体就是只取X的SVD中的前M个其一分量。  
&emsp;&emsp;$$ X = UDV^T \approx U_MD_MV_M^T$$  
其中$$U_M,D_M,V_M$$对于矩阵X的前M个奇异值分量。
##SVD的作用
SVD 将矩阵分解成累加求和的形式，其中每一项的系数即是原矩阵的奇异值。这些奇异值，按之前的几何解释，实际上就是空间超椭球各短轴的长度。现在想象二维平面中一个非常扁的椭圆（离心率非常高），它的长轴远远长于短轴，以至于整个椭圆看起来和一条线段没有什么区别。这时候，如果将椭圆的短轴强行置为零，从直观上看，椭圆退化为线段的过程并不突兀。回到 SVD 分解当中，较大的奇异值反映了矩阵本身的主要特征和信息；较小的奇异值则如例中椭圆非常短的短轴，几乎没有体现矩阵的特征和携带的信息。因此，若我们将 SVD 分解中较小的奇异值强行置为零，则相当于丢弃了矩阵中不重要的一部分信息。

因此，SVD 分解至少有两方面作用：

* 分析了解原矩阵的主要特征和携带的信息（取若干最大的奇异值），这引出了主成分分析（PCA）；
* 丢弃忽略原矩阵的次要特征和携带的次要信息（丢弃若干较小的奇异值），这引出了信息有损压缩、矩阵低秩近似等话题。

## Condition Number

### 条件数的定义

Let's linear equations:  
&emsp;&emsp;$$ \mathbf{Ax = b}$$,  
Let us investigate first, how a small change in the $$\mathbf{b}$$ vector changes the solution vector. $$\mathbf{x}$$ is the solution of the original system and let $$\mathbf{x + {\Delta}x}$$ is the solution when $$\mathbf{b}$$ changes from $$\mathbf{b}$$ to $$\mathbf{b + {\Delta}b}$$ .

Then we can write:  
&emsp;&emsp;$$ \mathbf{A(x + {\Delta}x) = b + {\Delta}b}$$  
or &emsp;&emsp;$$ \mathbf{Ax + A{\Delta}x = b + {\Delta}b}$$  
But because $$\mathbf{Ax = b}$$, it follows that  
&emsp;&emsp;$$ \mathbf{A{\Delta}x = {\Delta}b}$$  
So：  
&emsp;&emsp;$$ \mathbf{\delta x=A^{-1}{\Delta}b}$$  
Using the matrix norm properties:  
&emsp;&emsp;$$ \mathbf{||Ax|| \leq ||A||||x||} $$  
We can get:  
&emsp;&emsp;$$ \mathbf{||A^{-1}{\Delta}b|| \leq ||A^{-1}||||{\Delta}b||}$$,  
Also, we can get:  
&emsp;&emsp;$$ \mathbf{||b|| =  ||Ax|| \leq ||A||||x||}$$  
Using equations 2.5 and 2.6 we can get:  
&emsp;&emsp;$$\mathbf{\frac{||{\Delta}x||}{||A||\cdot||x||} \leq  \frac{||A^{-1}||\cdot||{\Delta}b||}{||b||}}$$  
Let's define condition number as: $$\mathbf{K(A)=||A||\cdot||A^{-1}||}$$  
we can rewrite equation 2.7 as:  
&emsp;&emsp;$$ \mathbf{\frac{||{\Delta}x||}{||x||} \leq  K(A)\cdot\frac{||{\Delta}b||}{||b||}}$$

Now, let us investigate what happens if a small change is made in the coefficient matrix  
$$\mathbf{A}$$. Consider $$\mathbf{A}$$ is changed to $$\mathbf{A+\Delta{A}}$$ and the solution changes from $$\mathbf{x}$$ to $$\mathbf{x+\Delta{x}}$$  
&emsp;&emsp;$$\mathbf{(A + \Delta A)(x + \Delta x) = b} $$,    
we can obtain:   
&emsp;&emsp;$$ \mathbf{\frac{||{\Delta}x||}{||x + \Delta x||} \leq  K(A)\cdot\frac{||\Delta A||}{||A||}}$$

$$\mathbf{K(A)}$$ is a measure of the relative sensitivity of the solution to changes in the right-hand  
side vector b.When the condition number $$\mathbf{K(A)}$$  becomes large, the system is regarded as being illconditioned.  
Matrices with condition numbers near 1 are said to be well-conditioned.

### 条件数对梯度下降法收敛的影响

## 例子

### 线性方程组解的稳定性分析

系统解的稳定性定义成系统小的扰动对系统解的影响，。  
[Example 1](http://engrwww.usask.ca/classes/EE/840/notes/ILL_Conditioned Systems.pdf):  
$$\begin{bmatrix}400&-201\\ 
\\-800&401\end{bmatrix}\begin{bmatrix} x_1\\ x_2\end{bmatrix}=\begin{bmatrix} 200\\ -200\end{bmatrix}$$  
Solution:  
$$x_1 = -100, x_2 = -200$$  
If we give small change to matrix A, change $$\mathbf{A_{11}}$$from 400 to 401 then:  
$$\begin{bmatrix}401&-201\\ 
\\-800&401\end{bmatrix}\begin{bmatrix} x_1\\ x_2\end{bmatrix}=\begin{bmatrix} 200\\ -200\end{bmatrix}$$  
This time the solution is:  
$$x_1 = 40000, x_2 = 79800$$

Ill-conditiond:  
When the solution is highly sensitive to the values of the coefficient matrix A or the righthand  
side constant vector b, the equations are called to be ill-conditioned.

## SVD在推荐系统中的应用

假设我们现在有评分矩阵$$V \in \mathbb{R}^{n \times m}$$,SVD实际上就是去找到两个矩阵：$$U \in \mathbb{R}^{f \times n}$$，$$M \in \mathbb{R}^{f \times m}$$，其中矩阵U表示 User 和 feature 之间的联系，矩阵V表示 Item 和 feature 之间的联系。  
大家这时候肯定会想，我们的评分矩阵里面一般会有很多缺失值，那要怎么去得到 U 和 M 呢？实际上，这两个矩阵是通过学习的方式得到的，而不是直接做矩阵分解。我们定义如下的损失函数：    
&emsp;&emsp;$$E = \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{m}I_{ij}(V_{ij} - p(U_i,M_j))^2+\frac{k_u}{2}\sum_{i=1}^{n}\lVert U_i \rVert^2 + \frac{k_m}{2}\sum_{j=1}^{m}\lVert M_j \rVert^2$$  
其中 $$p(U_i,M_j)$$ 表示我们对用户 i 对 物品 j 的评分预测：  
&emsp;&emsp;$$p(U_i,M_j) = U_i^TM_j$$

梯度下降：  
&emsp;&emsp;$$-\frac{\partial E}{\partial U_i}  = \sum_{j=1}^{M}I_{ij}((V_{ij}-p(U_i,M_j))M_j) - k_uU_i $$  
&emsp;&emsp;$$-\frac{\partial E}{\partial M_j}  = \sum_{i=1}^{n}I_{ij}((V_{ij}-p(U_i,M_j))U_i) - k_mM_j $$


实际上，用户的评分与用户的习惯，以及该物品的总体评分水平有关，因此可以加上用户的均值与物品的均值。  
&emsp;&emsp;$$\hat{r}_{ui} = \mu + b_i + b_u + \mathbf{q}_i^T\mathbf{p}_u $$  
μ： 训练集中所有记录的评分的全局平均数。在不同网站中，因为网站定位和销售的物品不同，网站的整体评分分布也会显示出一些差异。比如有些网站中的用户就是喜欢打高分，而另一些网站的用户就是喜欢打低分。而全局平均数可以表示网站本身对用户评分的影响。  
$$b_u$$： 用户偏置（user bias）项。这一项表示了用户的评分习惯中和物品没有关系的那种因素。比如有些用户就是比较苛刻，对什么东西要求都很高，那么他的评分就会偏低，而有些用户比较宽容，对什么东西都觉得不错，那么他的评分就会偏高。  
$$b_i$$： 物品偏置（item bias）项。这一项表示了物品接受的评分中和用户没有什么关系的因素。比如有些物品本身质量就很高，因此获得的评分相对都比较高，而有些物品本身质量很差，因此获得的评分相对都会比较低。  
这个时候我们的损失函数变为：  
&emsp;&emsp;$$E = \sum_{(u,i)\in \mathcal{k}}(r_{ui}-\mu - b_i - b_u - \mathbf{q}_i^T\mathbf{p}_u) + \lambda (\lVert p_u \rVert_2) + \lVert q_i \rVert_2 + b_u^2 + b_i^2) $$  



[^1] [SVD在推荐系统中的应用](http://charleshm.github.io/2016/03/SVD-Recommendation-System/)

