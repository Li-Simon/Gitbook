# 最优化的优缺点及其改进方案
##常见的损失函数
###分类问题
####0-1损失函数
y是标签，f是预测结果。 
对于二分类问题$$Y = [-1,1]$$   
0-1损失函数：  
$$\kern{4 em} L_{0-1}(f,y) =1_{fy \le 0}$$  
Hinge损失函数：  
$$\kern{4 em} L_{hinge}(f,y) = \max(0,1-fy)$$  
Logistic损失函数：  
$$\kern{4 em} L_{logistic}(f,y) = \log_2(1+\exp(-fy))$$  
当预测值$$f \in [-1, 1]$$时，可以定义如下损失函数：  
交叉熵（Cross Entropy）:  
$$\kern{4 em} L_{cross entropy} = -\log_2(\frac{1 + fy}{2})$$  
###回归问题
对于回归问题，Y=R,我们希望$$f(x_i, \theta) \approx y_i$$,最常用的损失函数是平方损失函数：  
$$\kern{4 em} L_{square}(f,y) = (f-y)^2$$  
平方损失函数对异常点惩罚很大，因此对异常点比较敏感。为了解决这个问题，引入了绝对损失函数：  
$$\kern{4 em} L_{square}(f,y) = |f-y|$$  
但是绝对损失函数在0点不可以求导。  
一个方法是在0点进行函数光滑化也就是在0点附件用平方函数代替。实际上就是Huber损失函数：  
Huber损失函数：  
$$\kern{4 em} L_{square}(f,y) = (f-y)^2, |f-y|>\delta$$  
$$\kern{4 em} L_{square}(f,y) = 2\delta|f-y| - \delta^2, |f-y|>\delta$$  
还得补充与思考文本处理，信号处理中的损失函数。  
##梯度下降
####问题1：怎样验证自己代码中梯度公式的正确性？

####问题2：当数据量特别大的时候经典梯度下降法存在什么问题，需要怎么改进？
####问题3：深度学习中常用的优化方法是随机梯度下降法，但是SGD偶尔也会失效，无法给出满意的训练结果，这是为什么？
####问题4：为了改进SGD，研究者都做了哪些改动？提出了哪些方法？他们各有哪些特点？
####问题5：L1正则化使得模型参数具有稀疏性的原理是什么？
