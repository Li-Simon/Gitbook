# 多元回归分析
求解线性方程组，如果问题是为满秩，或者超定问题，可以通过矩阵求伪逆来解决。当然如果矩阵求逆太费时间，则可以通过共轭梯度法来求解。   这一节不分析欠定问题。  
多元回归方程  
&emsp;&emsp;$$\hat y = \lambda_{0} + \lambda_{1}x_{1}+\lambda_{2}x_{2}+...+\lambda_{m}x_{m}$$  
其中$$b_j$$是$$x_j$$的偏回归系数，$$\hat y$$是样本的估计值。  
根据最小值原理，可以求得偏回归系数。  
&emsp;&emsp;$$L(b) = \sum_{i=0}^{n-1}[y_i-(\lambda_{0} + \lambda_{1}x_{1i}+\lambda_{2}x_{2i}+...+\lambda_{m}x_{mi})]^2$$  
变量数为m,样本数为n. 最小二乘法的目的就是找到一组偏回归系数使得损失函数L极小，极端情况就是L=0，则所有样本估计值就是样本的观测值。这只在满秩的情况下存在。一般只是求得L的极小值点。对损失函数求偏导，可以得到：
###正则方程
实际问题可以转化成矩阵分析  
&emsp;&emsp;$$\begin{bmatrix}x_{11}&x_{12}&\cdots&x_{1m}&1
\\ x_{21}&x_{22}&\cdots&x_{2m}&1
\\\cdots&\cdots&\cdots&\cdots&\cdots
\\x_{n1}&x_{n2}&\cdots&x_{nm}&1
\end{bmatrix}
\begin{bmatrix} \lambda_1\\ \lambda_2\\\cdots\\\lambda_n\\\lambda_0\end{bmatrix}=\begin{bmatrix} y_{1o}\\ y_{2o}\\\cdots\\y_{no}\end{bmatrix} + \begin{bmatrix} \epsilon_{1}\\ \epsilon_{2o}\\\cdots\\\epsilon_{no}\end{bmatrix} $$  
即：  
&emsp;&emsp;$$X*\lambda = Y + \epsilon$$  
寻找一组$$\lambda$$使得:$$L(\lambda) = (X*\lambda - Y)^T(X*\lambda - Y)$$极小  
利用$$L(\lambda)$$对向量$$\lambda$$[求导](https://zhuanlan.zhihu.com/p/24709748)可以得到[^1]：  
&emsp;&emsp;$$\frac{\partial L(\lambda)}{\partial \lambda} = 2X^T(X\lambda -Y) = 0$$  
我们也可以得到：  
&emsp;&emsp;$$\frac{\partial^2 L(\lambda)}{\partial \lambda^2} = 2X^TX$$  
求得：$$\lambda = (X^TX)^{-1}X^TY$$  
$$(X^TX)^{-1}X^T$$也称为X的伪逆。
###共轭梯度法
当矩阵X非常大的时候，时间复杂度是$$O(n^3)$$的矩阵求逆方法太费时间，一般采用共轭梯度法来求解。这在矩阵计算那一章会讨论。  
#多项式拟合 
函数的多项式表示方式：  
&emsp;&emsp;$$y = \sum_{k=0}^{m}\lambda_{k}x^{k}$$  
对于n组数据$$(x_i,y_i)$$若我们想用多项式去拟合这组数据，就是寻找使下列函数值极小的一组系数$$\lambda$$  
&emsp;&emsp;$$L(\lambda) = \frac{1}{2m}\sum_{i=0}^{n-1}(y_i - \sum_{k=0}^{m}\lambda_{k}x_{i}^{k})^2$$  
令：
&emsp;&emsp;$$\lambda = [\lambda_0,\lambda_1,\cdots,\lambda_m]^T$$  
s.t.&emsp;$$X_i = [x_{i}^{0},x_{i}^{1},\cdots,x_{i}^{m}]$$  
&emsp;&emsp;$$Y = [Y_0,Y_1,\cdots,Y_{n-1}]^T$$  

则上面损失函数可以表示为：  
&emsp;&emsp;$$L(\lambda) = (X*\lambda - Y)^T(X*\lambda - Y)$$  

对其求一阶导，结果为0；   
&emsp;&emsp;$$\frac{\partial L(\lambda)}{\partial \lambda} = 0$$   
最终方程可以化简成如下形式：  

&emsp;&emsp;$$\begin{bmatrix}\sum_{j=0}^{n-1}x_j^{0}&\sum_{j=0}^{n-1}x_j^{1}&\cdots&\sum_{j=0}^{n-1}x_j^{m}
\\ \sum_{j=0}^{n-1}x_j^{1}&\sum_{j=0}^{n-1}x_j^{2}&\cdots&\sum_{j=0}^{n-1}x_j^{m+1}
\\\cdots&\cdots&\cdots&\cdots
\\\sum_{j=0}^{n-1}x_j^{m}&\sum_{j=0}^{n-1}x_j^{m+1}&\cdots&\sum_{j=0}^{n-1}x_j^{2m}
\end{bmatrix}
\begin{bmatrix} \lambda_0\\ \lambda_1\\\cdots\\\lambda_m\end{bmatrix}=\begin{bmatrix} \sum_{j=0}^{n-1}y_j*x_j^{0}\\ \sum_{j=0}^{n-1}y_j*x_j^{1}\\\cdots\\\sum_{j=0}^{n-1}y_j*x_j^{m}\end{bmatrix}$$   
即:$$X*\lambda = Y$$ 得到 $$\lambda = X^{-1}Y$$  
若在方程上加上一个正则项，  
&emsp;&emsp;$$L(\lambda) = \frac{1}{2m}\sum_{i=0}^{n-1}(y_i - \sum_{k=0}^{m}\lambda_{k}x_{i}^{k})^2 + \sum_{k=0}^{m}\lambda_{k}^{2}$$  
得到如下的矩阵：  

&emsp;&emsp;$$\begin{bmatrix}\sum_{j=0}^{n-1}x_j^{0} - 2n&\sum_{j=0}^{n-1}x_j^{1}&\cdots&\sum_{j=0}^{n-1}x_j^{m}
\\ \sum_{j=0}^{n-1}x_j^{1}&\sum_{j=0}^{n-1}x_j^{2} - 2n&\cdots&\sum_{j=0}^{n-1}x_j^{m+1}
\\\cdots&\cdots&\cdots&\cdots
\\\sum_{j=0}^{n-1}x_j^{m}&\sum_{j=0}^{n-1}x_j^{m+1}&\cdots&\sum_{j=0}^{n-1}x_j^{2m}-2n
\end{bmatrix}
\begin{bmatrix} \lambda_0\\ \lambda_1\\\cdots\\\lambda_m\end{bmatrix}=\begin{bmatrix} \sum_{j=0}^{n-1}y_j*x_j^{0}\\ \sum_{j=0}^{n-1}y_j*x_j^{1}\\\cdots\\\sum_{j=0}^{n-1}y_j*x_j^{m}\end{bmatrix}$$ 

类似于Ridge回归：  
但是对于加噪声参数的多项式数据，发现不加正则项能给出正确的结果，加正则项反倒不能，因为这是满秩问题，没必要加正则项加正则项的目的是为了解的稳定性而不是解的精确性，对于欠定问题，才可以加正则项来对解进行约束。  

#迭代法求解
求解上面的方程也可以使用迭代法求解，实际上就是最优化方法。



[^1]: 矩阵求导术（上），https://zhuanlan.zhihu.com/p/24709748

