## 循环神经网络（RNN）

#### 最简单的RNN

网上最易懂得结构图[^1]：  
![](/assets/RNN_simple_2.png)     
一个简单的RNN内部的结构是：  

![](/assets/RNN_sim_2.png)     
展开图：  
 
![](/assets/RNN_unfold.png)    
RNN主要由矩阵Ｕ,W，V,以及激活函数f,g决定。  
图中左边是RNN的一个基本模型，右边是模型展开之后的样子。展开是为了与输入样本匹配。假若输入时汉语句子，每个句子最长不超过20（包含标点符号），则把模型展开20次[^2]。  
1. $$x_t$$ 代表输入序列中的第tt步元素，例如语句中的一个汉字。一般使用一个one-hot向量来表示，向量的长度是训练所用的汉字的总数（或称之为字典大小），而唯一为1的向量元素代表当前的汉字。   
2. $$s_t$$代表第t步的隐藏状态，其计算公式为$$s_t=tanh(Ux_t+Ws_{t-1})$$。也就是说，当前的隐藏状态由前一个状态和当前输入计算得到。考虑每一步隐藏状态的定义，可以把$$s_t$$视为一块内存，它保存了之前所有步骤的输入和隐藏状态信息。$$s_{-1}$$是初始状态，被设置为全0。  
3. $$o_t$$是第t步的输出。可以把它看作是对第t+1步的输入的预测，计算公式为：$$o_t=softmax(Vs_t)$$。可以通过比较$$o_t$$和$$x_{t+1}$$之间的误差来训练模型。

U,V,W是RNN的参数，并且在展开之后的每一步中依然保持不变。这就大大减少了RNN中参数的数量。

##### 一个例子

假设我们要训练的中文样本中一共使用了3000个汉字，每个句子中最多包含50个字符，则RNN中每个参数的类型可以定义如下。  
1. $$x_t \in R^{3000}$$,第t步的输入，是一个one-hot向量，代表3000个汉字中的某一个。  
2. $$o_t\in R^{3000}$$,第t步的输出，类型同$$x_t$$。  
3. $$s_t\in R^{50}$$,第t步的隐藏状态，是一个包含50个元素的向量。RNN展开后每一步的隐藏状态是不同的。  
4. $$U\in R^{50\times 3000}$$, 在展开后的每一步都是相同的。  
5. $$V\in R^{3000\times 50}$$, 在展开后的每一步都是相同的。  
6. $$W\in R^{50\times 50}$$, 在展开后的每一步都是相同的。

其中$$x_t$$是输入，U,V,W是参数，$$s_t$$是由输入和参数计算所得到的隐藏状态，而$$0_t$$则是输出。$$s_t$$和$$o_t$$的计算公式已经给出，为清晰起见，重新写出。  
$$s_t=tanh(Ux_t + Ws_{t-1})$$  
$$o_t=\text{softmax}(Vs_t)$$  
要训练的就是参数矩阵U,V,W

### RNN Goodfellow

循环神经网络中一些重要的设计模式包含如下几种：  
\(1\)每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络，如下图所示。  
RNN的输入到隐藏的连接由权值矩阵U参数化，隐藏到隐藏的循环由权重矩阵W参数化，隐藏到输出层的连接由权重矩阵V参数化。$$L^{(t)}$$是损失函数。  
![](/assets/RNN_General.png)  
前向传播公式：假设激活函数是tanh，我们有如下的更新方程：  
  $$ \mathbf{a}^{(t)} = \mathbf{b} + \mathbf{W}\mathbf{h}^{(t-1)} + \mathbf{U}x^{(t)}$$  
  $$ \mathbf{h}^{(t)} = \tanh(\mathbf{a})$$  
  $$ \mathbf{o}^{(t)} = \mathbf{c} + \mathbf{V}\mathbf{h}^{(t)} $$  
  $$\mathbf{\hat y}^{(t)} = softmax(\mathbf{o}^{(t)})$$  
其中$$\mathbf{b},\mathbf{c}$$是偏置向量,$$\mathbf{U},\mathbf{V},\mathbf{W}$$是权重矩阵。  
这一循环网络将一个输入序列映射到相同长度的输出序列。与$$\mathbf{x}$$序列配对的$$\mathbf{y}$$的总的损失就是所有时间步的损失之和。例如，$${L}^{(t)}$$为给定的$$x^{(1)},x^{(2)},...,x^{(t)}$$后$$y^{(t)}$$的负对数似然，则：  
  $$L((x^{(1)},x^{(2)},...,x^{(\tau)}),(y^{(1)},y^{(2)},...,y^{(\tau)})) = \displaystyle \sum_{t} L^{(t)}$$  
  $$ = -\displaystyle \sum_{t} \log p_{model}(y^{(t)}|(x^{(1)},x^{(2)},...,x^{(t)})$$  
其中$$p_{model}(y^{(t)}|(x^{(1)},x^{(2)},...,x^{(t)})$$需要读取模型输出向量$$\mathbf{\hat y}^{(t)}$$对应于$$\mathbf{y}^{(t)}$$的项。  
应用于展开图且代价为$$O(\tau)$$的反向传播算法是通过时间反向传播\(back-propagation through time, BPTT\)。  
\(2\)每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络。如下图所示：  
该图中，RNN被训练将特定输出值放入o中，并且o是被允许传播到未来的唯一信息。此处没有从h前向传播的直接连接。之前的h仅通过产生的预测间接地连接到当前。o通常缺乏过去的重要信息，除非它非常高维并且内容丰富。这使得该图中的RNN不那么强大，但是它更容易训练，因为每个时间步可以与其他时间步分离训练，允许训练期间更多的并行化。

![](/assets/RNN_DesignPattern_2.png)  
\(3\)隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络，如下图所示：  
这样的网络可以用于概括序列并产生用于进一步处理的固定大小的表示。在结束处可能存在目标，或者通过更下游模块的反向传播来获得输出$$o^{(t)}$$上的梯度。  
![](/assets/RNN_DesignPattern_3.png)

## Clockwork RNNs

CW-RNNs，时钟频率驱动循环神经网络  
![](/assets/CW_RNN_LSTM_result.png)  
上图中，绿色实线是预测结果，蓝色散点是真实结果。每个模型都是对前半部分进行学习，然后预测后半部分。LSTMs模型类似滑动平均，但是CW-RNNs效果更好。其中三个模型的输入层、隐藏层、输出层的节点数都相同，并且只有一个隐藏层，权值都使用均值为0，标准差为0.1的高斯分布进行初始化，隐藏层的初始状态都为0，每一个模型都使用Nesterov-style  momentum SGD\(Stochastic Gradient Descent，随机梯度下降算法\)进行学习与优化

