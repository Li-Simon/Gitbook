# 自己动手写神经网络

这里需要实现NN，CNN,RNN，区别在于是否加卷积。对于NN，要保证层数不说限制，每层的神经元不受限制，只处理一维输入,输出也就一维,也就是分类问题；CNN处理二维输入\(图像\)，设计卷积和卷积核，实现的例子就是LeNet-5。RNN就是实现最简单的RNN。三者通用的东西是什么？可以提出来，我们需要用到哪些模块？这个心里是要清楚的。  
![](/assets/LeNet.png)

![](/assets/WriteNNByHand.png)

## NN

## CNN

![](/assets/BP_CNN1.png)  

$$n_l$$是层神经元数目；  
$$W^{(l)} \in R^{n_l \times n_{l-1}}$$是第l-1层到第l层的的权重矩阵。  
$$W^{(l)}_{ij}$$表示第l-1层的j神经元到l层的i神经元之间的权重。  
$$f()$$是激活函数。  
$$\mathbf{b^{(l)}}=[b^{(l)}_1,b^{(l)}_2,...,b^{(l)}_{n_l}] \in R^{n_l}$$表示第l-1层到第l层的的偏置。
$$\mathbf{z^{(l)}}=[z^{(l)}_1,z^{(l)}_2,...,z^{(l)}_{n_l}]\in R^{n_l}$$表示第l层神经元的的状态。    
$$\mathbf{a^{(l)}}=[a^{(l)}_1,a^{(l)}_2,...,a^{(l)}_{n_l}]\in R^{n_l}$$表示第l层神经元的的激活值。  
##信息前向传播
因此我们根据下面的公式计算每层的神经元状态与输出值，计算从第2层到最后一层L。  
&emsp;&emsp;$$\mathbf{z^{(l)}}=\mathbf{W^{(l)}a^{(l-1)} + b^{(l)}}$$   
&emsp;&emsp;$$\mathbf{a^{(l)}}=f(\mathbf{z^{(l)}})$$   
对于第L层，最终输出$$\mathbf{a^{(L)}}$$。信息传播如下：  
&emsp;&emsp;$$\mathbf{x = a^{(1)} \to z^{(2)}\to a^{(2)} \to ...\to a^{(L-1)} \to z^{(L)} \to a^{(L)} = y}$$  
##误差反向传播
训练数据为$$\mathbf{(x^{(1)},y^{(1)}),...,(x^{(N)},y^{(N)})}$$,即N个数据，假设输出是$$n_L$$维的，也就是：  
&emsp;&emsp;$$\mathbf{y^{(i)}}=(y^{(i)}_1,y^{(i)}_2,...,y^{(i)}_{n_L})^T\in R^{n_L \times 1}$$  
对于第i个数据，假设误差是均方误差，$$\mathbf{o^{(i)}}$$是输出。   
&emsp;&emsp;$$E_i = \frac{1}{2} (\mathbf{y^{(i)}-o^{(i)}})^2 = \frac{1}{2} \displaystyle \sum_{k=1}^{n_L}(y_k^{(i)}-o_k^{(i)})^2$$
因此对总的N个数据。总的误差表示如下：    
&emsp;&emsp;$$E =\frac{1}{N} \displaystyle \sum_{i=1}^{N}E_i$$
##对输出层权重参数更新
&emsp;&emsp;$$\frac{\partial E}{\partial w_{ij}^{(n_L)}} = \frac{\partial E}{\partial z_{i}^{(n_L)}} \frac{\partial z_{i}^{(n_L)}}{\partial w_{ij}^{(n_L)}} = \delta_i^{(n_L)}a_j^{(n_L-1)}$$  
其中：  
&emsp;&emsp;$$\delta_i^{(n_L)} = \frac{\partial E}{\partial z_{i}^{(n_L)}} = -(y_i - a_i^{(n_L)})f'(z_i^{(n_L)})$$  
因此：  
&emsp;&emsp;$$\mathbf{\delta^{(n_L)} = -(y - a^{(n_L)})*f'(z^{(n_L)})}$$   
其中*表示向量对应元素相乘。    

##对隐含层权重参数的更新
对于隐藏层有：   
&emsp;&emsp;$$\frac{\partial E}{\partial w_{ij}^{(l)}} = \frac{\partial E}{\partial z_{i}^{(l)}} \frac{\partial z_{i}^{(l)}}{\partial w_{ij}^{(l)}} = \delta_i^{(l)}a_j^{(l-1)}$$    
 
&emsp;&emsp;$$\delta_i^{(l)} = \frac{\partial E}{\partial z_{i}^{(l)}} =\displaystyle \sum_{j=1}^{n_{l+1}} \frac{\partial E}{\partial z_{j}^{(l+1)}} \frac{\partial z_{j}^{(l+1)}}{\partial z_{i}^{(l)}} =\displaystyle \sum_{j=1}^{n_{l+1}} \delta_j^{(l+1)} \frac{\partial z_{j}^{(l+1)}}{\partial z_{i}^{(l)}} $$    
&emsp;&emsp;$$\frac{\partial z_{j}^{(l+1)}}{\partial z_{i}^{(l)}} = \frac{\partial z_{j}^{(l+1)}}{\partial a_{i}^{(l)}}\frac{\partial a_{i}^{(l)}}{\partial z_{i}^{(l)}} = w_{ji}^{(l+1)}f'(z_{i}^{(l)})$$    
因此有：   
&emsp;&emsp;$$\delta_i^{(l)} = \displaystyle \sum_{j=1}^{n_{l+1}} \delta_j^{(l+1)} w_{ji}^{(l+1)}f'(z_{i}^{(l)}) = (\displaystyle \sum_{j=1}^{n_{l+1}} \delta_j^{(l+1)} w_{ji}^{(l+1)})f'(z_{i}^{(l)})$$    
因此写成矢量想成形式：    
&emsp;&emsp;$$\mathbf{\delta^{(l)} = -((W^{(l+1)})^T\delta^{(l+1)})*f'(z^{(l)})}$$    
因此：    
&emsp;&emsp;$$\frac{\partial E}{\partial w_{ij}^{(l)}} = (\displaystyle \sum_{j=1}^{n_{l+1}} \delta_j^{(l+1)} w_{ji}^{(l+1)})f'(z_{i}^{(l)})a_j^{(l-1)}$$    
###对输出层与隐含偏置求导数
&emsp;&emsp;$$\frac{\partial E}{\partial b_{i}^{(l)}} = \frac{\partial E}{\partial z_{i}^{(l)}} \frac{\partial z_{i}^{(l)}}{\partial b_{i}^{(l)}} = \delta_i^{(l)}$$   
##BP四个核心公式
整理起来，BP算法有如下四个核心公式：  
&emsp;&emsp;$$\delta_i^{(L)}= -(y_i - a_i^{(L)})f'(z_i^{(L)})$$    
&emsp;&emsp;$$\delta_i^{(l)} = (\displaystyle \sum_{j=1}^{n_{l+1}} \delta_j^{(l+1)} w_{ji}^{(l+1)})f'(z_{i}^{(l)})$$    
&emsp;&emsp;$$\frac{\partial E}{\partial b_{i}^{(l)}} = \delta_i^{(l)}$$   
&emsp;&emsp;$$\frac{\partial E}{\partial w_{ij}^{(l)}} = \delta_i^{(l)}a_j^{(l-1)}$$   
###两个辅助公式
&emsp;&emsp;$$\mathbf{z^{(l)}}=\mathbf{W^{(l)}a^{(l-1)} + b^{(l)}}$$   
&emsp;&emsp;$$\mathbf{a^{(l)}}=f(\mathbf{z^{(l)}})$$   
##参数更新
&emsp;&emsp;$$W^{(l)} = W^{(l)} - \frac{u}{N}\displaystyle \sum_{i=1}^{N} \frac{\partial E_i}{\partial W^{(l)}}$$  
&emsp;&emsp;$$\mathbf{b^{(l)} = b^{(l)}} - \frac{u}{N}\displaystyle \sum_{i=1}^{N} \frac{\partial E_i}{\partial b^{(l)}}$$  
 






  

