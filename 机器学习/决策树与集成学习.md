# 决策树与集成学习

决策树是一种基本的回归与分类的方法。决策树由节点与边构成，节点分类内部节点与叶子节点；内部节点表示属性或者特征，叶子节点表示一个类。

## 决策树学习

训练集$$D = ((x_1, y_1),(x_2, y_2)...(x_N, y_N))$$其中$$x_i = (x_i^{(1)},x_i^{(2)}...x_i^{(n)})$$是n维变量，n表示特征的数目，$$y_i \in (1,2...K)$$类标签。训练的本质是基于一定标准得到一组分类规则。我们需要得到一组与训练数据矛盾较小并且泛化能力也好的分类规则，其中泛化能力说的是在测试集上其错误率也小，错误率有不同的定义方式，可以是均方误差，也可以是似然函数，一般回归问题选择均方误差，分类问题选择似然函数，**想想这是为什么？**。  
  分类规则就是特征选择的过程，特征选择是基于一些可以量化的函数，一般基于信息熵增益最大或者信息熵增益率最大或者Gini系数下降最大的规则。因此，接下来我们要定义如下概念。

1. 信息熵
2. 条件熵
3. 信息增益
4. 信息增益率
5. Gini系数

   ### 1. 信息熵

   熵的概念来自于统计物理，描述微观系统的混乱程度，由Rudolf Clausius提出，  
   $$S = k_B ln \Omega =-k_B\sum _i p_i \ln p_i $$  
   其中$$k_B$$是Boltzmann constant,$$\Omega$$表示系统的可能状态数。  
   它表示的就是$$ln(p_i)$$的期望值。香农把这个概念引入信息学，定义了信息熵。  
   考虑一个只能取有限n个值的系统，值对应的就是状态，比如投硬币，只能取两个值，就是说只能有2个状态，因此n=2，在统计物理中，状态对应的是系统的能级，也就是一个能级对应一个状态，系统处于不同能级的概率就是$$p_i$$  
   $$P(X = x_i) = p_i, i = 1,2..n$$  
   熵的定义如下：$$H(x) = -\sum_{i}^{i=n} p_i \ln p_i$$  
   若系统是确定的，则有一个$$p_i = 1$$，其他的$$p_i = 0$$等于0，因此代入熵的公式可知，熵为0.可以证明，对于n个状态的系统，系统的熵满足如下不等式：  
   $$0 \le H(x) = -\sum_{i}^{i=n} p_i ln p_i \le \ln n$$.

   ### 2. 条件熵

   联合概率说的是两个及两个系统随机变量共同发生的几率问题，条件熵$$H(Y|X)$$说的是随机变量X给定的情况下，随机变量Y的条件熵。  
   计算如下：  
   $$H(Y|X) = \sum_{i}^{i=n} p_i H(Y|X=x_i)$$  
   这里， $$p_i = P(X=x_i), i=1,2..n$$。  
   $$H(Y|X=x_i)$$计算与上面的信息熵一样，只是对于$$H(Y|X=x_i)$$，我们只计算$$X = x_i$$的那些样本的熵，因为我们会对所有X取不同值得样本进行求和，因此会遍历整个样本。

   ### 3. 信息增益

   信息增益是基于以上两个概念的，是针对于特征而言的，特征A对训练数据集D的信息增益g\(D,A\)定义成经验熵H\(D\)与特征A给定的条件下，D的经验条件熵H\(D\|A\)之差。即：  
   $$g(D,A) = H(D) - H(D|A)$$。  
   如果我们知道数据集D的信息熵计算，以及条件熵的计算，则信息增益的计算不难。

   ### 4. 信息增益率

   特征A对训数据集D的信息增益率$$g_R(D,A)$$定义成信息增益g\(D,A\)与特征A的经验熵H\(A\)之比：  
   $$g_R(D,A) = \frac{g(D,A)}{H(A}$$。  
   其中$$H(A) = \sum_{i=1}^{i=K} p_i \ln p_i$$,K是特征A能取不同值得数目，$$p_i$$是取不同值得概率。  
   他的计算基于信息增益与信息熵。

   ### 5. Gini系数

   基尼系数定义：对于分类问题，假设有K个类，样本点处于第k个类的概率是$$p_k$$,则概率分布的基尼系数定义成：  
   $$Gini(p) = \sum_{i}^{i=n} p_i(1 - p_i) = 1 - \sum_{i=1}^{i=K} p_i^2$$  
   基尼系数描述的是不同类之间的不一致程度，一个类与其它类的不一致概率和，也就是不一致程度程度，说的是从样本集合D中随便挑出两个样本，他们属于不同类的概率。 由下面函数刻画$$p_i(1 - p_i)$$.对于分类问题，我们希望最终的叶子节点里面，所有的样本点尽可能使同一个类里面的，也就是基尼系统尽可能趋近于0.  
   假设样本D中有K个类，$$C_k$$是第k类的集合，$$|C_k|$$是集合$$C_k$$的大小，因此样本集合的基尼系数定义成:  
   $$Gini(p) = 1 - \sum_{i=1}^{i=K} (\frac{|C_k|}{|D|})^2$$

基于不同的指标，比如信息增益，信息增益率，基尼系数，我们会得到不同的决策树生成算法，依次是ID3， C4.5, CART。

![](/assets/Entropy_Gini.png)

信息熵与基尼系数的函数大致一致，实际上在iris数据集中，生成决策树时，两者没有差别。

## ID3算法

ID3算法基于信息增益最大，代码实现如下，它的缺点在于，它倾向于选择取值很多的特征，因为当特征能取很多值得时候，此时系统的不确定度降低，极端情况是，特征能取N个不同的值，N个训练集的数量，此时特征A条件熵为0. 为了避免这种情况，而引入了C4.5算法。  
假设特征A是信息增益最大的特征，当特征A取离散值时，假设特征A可以取K个不同值，我们选择A作为特征之后会生出K个节点，对于每个节点，我们要重复上面的步骤，继续寻找信息增益最大的特征，只是，这时候特征数目减小了1，减小的这个特征就是A特征本身。当一个节点，它的熵小于一定阈值的时候，我们就不再分割这个节点而是把他当成一个叶子节点。我们可以使用递归来实现ID3算法。  
对于特征值取连续的情况，我们通过计算特征值大于阈值与小于阈值的两部分熵之和来计算熵。  
算法实现的时候要注意的一点就是，计算熵时，不能让概率等于0，否则会出错。

```py
import scipy
import numpy as np
from sklearn import tree
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

class DecisionTree(object):
    def __init__(self):
        self.training_steps = 50
        self.initial_x = [[1],[1]]#np.ones((2,1))

    def Entropy(self, Prob):
        entropy = 0
        for n in range(Prob.shape[0]):
            entropy -= Prob[n] * (np.log2(Prob[n])) ##Entropy
            #entropy += Prob[n] * (1 - Prob[n])  ##Gini
        return entropy

    def distribution(self, feature_data, throshold_n):
        entropy = 0
        prob = 0.001*np.ones((3, 1))
        for n in range(0, throshold_n):
            for index in range(3):
                if np.abs(feature_data[n, 1] - index) < 0.01:
                    prob[index] += 1
        prob = prob/throshold_n
        p_1 = (1.0*throshold_n)/feature_data.shape[0]
        H_1 = self.Entropy(prob)
        entropy += p_1*H_1

        prob_2 = 0.001*np.ones((3, 1))
        for n in range(throshold_n, feature_data.shape[0]):
            for index in range(3):
                if round(feature_data[n, 1]) == index:
                    prob_2[index] += 1
        prob_2 = prob_2/(feature_data.shape[0] - throshold_n)
        H_2 = self.Entropy(prob_2)
        entropy += (1 - p_1) * H_2
        return entropy

    def best_feature_threshold(self, data):
        best_threshold = 0
        best_feature = 0
        best_entropy = 100000
        feature_num = data.shape[1] - 1
        for feature_index in range(feature_num):
            a_arg = np.argsort(iris_data[:, feature_index])
            sorted_data = iris_data[a_arg]
            for n in range(1, sorted_data.shape[0]):
                threshold = (sorted_data[n-1, feature_index] + sorted_data[n, feature_index])/2
                featue_result_data = np.zeros((sorted_data.shape[0], 2))
                featue_result_data[:, 0] = sorted_data[:, feature_index]
                featue_result_data[:, -1] = sorted_data[:, -1]
                condition_entropy = self.distribution(featue_result_data, n)
                if condition_entropy < best_entropy:
                    best_threshold = threshold
                    best_entropy = condition_entropy
                    best_feature = feature_index
            print best_entropy, best_feature, best_threshold
        return best_entropy, best_feature, best_threshold

if __name__ == '__main__':
    DT = DecisionTree()
    iris = load_iris()
    clf = tree.DecisionTreeClassifier()
    clf = clf.fit(iris.data, iris.target)
    data = iris.data
    target = iris.target
    iris_data = np.zeros((data.shape[0], data.shape[1] + 1))
    iris_data[:, 0:data.shape[1]] = data
    iris_data[:, -1] = target
    best_entropy, best_feature, best_threshold = DT.best_feature_threshold(iris_data)
    print best_entropy, best_feature, best_threshold
    np.savetxt("C:\Data\Group\ShareFolder\Optimization\ML\iris_data.csv", data)
    np.savetxt("C:\Data\Group\ShareFolder\Optimization\ML\\target.csv", target)
    np.savetxt("C:\Data\Group\ShareFolder\Optimization\ML\\iris_data.csv", iris_data)
```

## C4.5

C4.5算法弥补了ID3算法的不足，通过使用信息增益率来作为特征的选择标准。  
$$g_R(D,A) = \frac{g(D,A)}{H(A}$$。  
其中$$H(A) = \sum_{i=1}^{i=K} p_i \ln p_i$$,K是特征A能取不同值得数目，$$p_i$$是取不同值得概率。

## 模型的剪枝

为什么需要剪枝：提高泛化能力，  
我们可以一步步的细分节点而使得系统的分类误差很小，但是这只是训练数据集上的结果，当我们把模型运用到测试集时，误差率会很高，这是因为模型过拟合你，我们可以通过对决策树进行剪枝来提高模型的泛化能力。  
为了进行剪枝，我们得定义剪枝的标准，也就是定义损失函数，损失函数至少要包括两项，一个是模型在训练集上的误差，一个是模型的复杂程度。模型的复杂程度可以定义成与叶子节点正相关。模型在训练集上的误差率可以如下定义。  
$$C(T) = \sum_{t=1}^{t=|T|} N_t H_t$$  
其中$$N_t$$是叶子节点t上的样本点数目，$$H_t$$是叶子节点t的熵。  
叶子节点的熵定义为：  
$$H_t = -\sum_{k=1}^{k=K}\frac{N_{tk}}{N_t}\ln \frac{N_{tk}}{N_t}$$,  
其中$$k \in (1,2...K)$$是样本结果可以取不同值的数目，也就是标签有K类$$N_t$$是叶子节点t的样本数目， $$N_{tk}$$是叶子节点t中标签属于k类的数目。总体而言，C\(T\)刻画的是模型在训练集上的误差。结合这两部分，我们可以定义剪枝的损失函数：  
$$C_{\alpha}(T) = C(T) + \alpha |T|$$  
其中正数$$\alpha$$用来权衡模型的误差率与模型的复杂度。  
输入：计算生成的整个树T，参数$$\alpha$$  
输出：修剪后的子树$$T_{\alpha}$$  
\(1\)计算每个节点的经验熵  
\(2\)递归地从树的叶节点向上回溯  
设一组叶节点回溯到父节点之前与之后的整个树为$$T_B, T_A$$,对应的损失函数值分别为$$C_{\alpha}(T_{B})$$和$$C_{\alpha}(T_{A})$$,如果：  
$$C_{\alpha}(T_{A}) \le C_{\alpha}(T_{B})$$  
则进行剪枝，将父节点变成新的叶节点。  
\(3\)返回\(2\),直到不能继续为止，得到损失函数最小的子树$$T_{\alpha}$$

#### 局部加权平均

$$L(\theta) = \frac{1}{N}\sum _i w_i(\hat y_i - y_i)^2$$  
其中：$$w_i = exp(\frac{x_i - x}{2\tau^2})$$

## CART

决策树的生成就是递归的构建二叉决策树的过程。对回归树用平方误差最小的准则，对分类树用基尼指数最小化准则，进行特征选择，生成二叉树。  
CART包含分类树与回归树，下面分别来讨论分类树与回归树

#### 回归树的生成

假设X和Y分别为输入与输出变量，并且Y是连续变量，给定训练数据集  
$$D = ((x_1, y_1),(x_2, y_2)...(x_N, y_N))$$.  
假设已将输入空间划分为M个单元$$R_1, R_2,...,R_M$$,并且每个单元$$R_m$$上有一个固定的输出值$$c_m$$,于是回归树模型可以表示为：  
$$f(x) = \sum_{m=1}^{m=M} c_m I(x \in R_m)$$  
当输入空间的划分确定时，可以用平方误差  
$$\sum _{x_i \in R_m} (y_i - f(x_i))^2$$  
来表示回归书对于训练数据的预测误差，用平方误差最小的准则来求解每个单元上的最优输出值。  
可以求得  
$$\hat c_m = ave(y_i | x_i \in R_m)$$。  
问题是，怎么对输入的空间进行划分？  
回归树可以处理离散特征与连续特征，对于连续特征，若这里按第j个特征的取值s进行划分，切分后的两个区域分别为：  
$$R_1(j,s) = (x_i|x_i^j \le s)$$  
$$R_2(j,s) = (x_i|x_i^j \gt s)$$  
若为离散特征，则找到第j个特征下的取值s:  
$$R_1(j,s) = (x_i|x_i^j = s)$$  
$$R_2(j,s) = (x_i|x_i^j \ne s)$$  
分别计算$$R_1, R_2$$的类别估计$$c_1, c_2$$,然后计算按照\(j,s\)切分后的损失:  
$$min_{j,s}[\sum_{x_i \in R_1}(y_i - c_1)^2 + \sum_{x_i \in R_2}(y_i - c_2)^2]$$,找到是的损失最小的\(j,s\)对即可，也就是说找到最优特征$$j^*$$，并找到最优特征的分割值$$s^*$$s。递归执行\(j,s\)的选择过程，知道满足停止条件为止。递归的意思是，对分割出的两个区域$$R_1, R_2$$分别进行如上的步骤，再分割下去。

总结：  
回归树算法：  
输入：训练集 $$D = ((x_1, y_1),(x_2, y_2)...(x_N, y_N))$$.  
输出：回归树T  
 1.求解选择的切分特征j与切分特征取值s, j将训练集D划分成两部分，  $$R_1, R_2$$,  
$$R_1(j,s) = (x_i|x_i^j \le s)$$  
$$R_2(j,s) = (x_i|x_i^j \gt s)$$  
其中：  
$$c_1 = \frac{1}{N_1}\sum_{x_i \in R_1}y_i$$  
$$c_2 = \frac{1}{N_2}\sum_{x_i \in R_2}y_i$$  
2.遍历所有的可能解\(j,s\),找到最优的$$(j^*,s^*)$$,最优解使得如下损失函数最小，  
$$min_{j,s}[\sum_{x_i \in R_1}(y_i - c_1)^2 + \sum_{x_i \in R_2}(y_i - c_2)^2]$$  
按照最优特征$$(j^*,s^*)$$来切分即可  
3.递归调用1.，2.步骤，直到满足停止条件  
4.将输入空间划分为M个区域$$R_1, R_2,...,R_M$$,返回决策树T  
$$f(x) = \sum_{m=1}^{m=M} \hat c_m I(x \in R_m)$$

回归树采用了分治策略，对于无法用唯一的全局线性回归来优化的目标进行分而治之，进而可以取得比较准确的结果，但分段取均值并不是一明智的选择，可以考虑将叶节点设置为一个线性函数，这就是所谓的分段线性树模型。

#### 分类树

分类树是CART中用来分类的，不同于ID3与C4.5，分类树采用基尼系数来选择最优的切分特征，而且每次都是二分。  
输入：训练集 $$D = ((x_1, y_1),(x_2, y_2)...(x_N, y_N))$$.  
输出：回归树T  
 1\)利用特征A的取值a将数据分为两部分，计算A=a时的基尼系数  
$$Gini(D,A) = \frac{|D_1|}{D}Gini(D_1) + \frac{|D_2|}{D}Gini(D_2)$$  其中  
$$D_1(A,a) = (x_i|x_i^A = a)$$  
$$D_2(A,a) = (x_i|x_i^A \ne a)$$  
2\)对整个数据集中所有的可能特征A以及其可能取值a选取基尼系数最小的特征$$A^*$$与特征下的取值$$a^*$$，来将数据集切分，将数据集$$D_1, D_2$$分到两个字节点中去。  
3.对子节点（$$D_1,D_2$$）递归调用1\)，2\)步骤，直到满足停止条件  
4.返回CART分类树T

#### CART的剪枝[^1]

剪枝基于如下的损失函数  
$$C_{\alpha}(T) = C(T) + \alpha |T|$$  
其中T为任意子树，\|T\|为子树的叶子结点个数，C\(T\)为对训练数据的预测误差\(如基尼系数\)，正数$$\alpha$$用来权衡模型的误差率与模型的复杂度。  
对于固定的$$\alpha$$，存在唯一的最有子树$$C_{\alpha}(T)$$.  
Breiman等人证明，可以用递归的方法对树进行剪枝，将$$\alpha$$从0开始增大，$$0 \le \alpha _1\le \alpha _2\le... \le \alpha _n$$，得到一系列最优子树系列 $$(T_0, T_1,T_n)$$,序列中的子树是嵌套的（？）。  
具体的， 从整棵树$$T_0$$开始剪枝，对$$T_0$$的任意内部节点t,以t为单节点书的损失函数是\(也就是把t下面的所有节点减掉之后的损失函数\)：  
  $$C_{\alpha}(t) = C(t) + \alpha $$  
  以t为根结点的字数$$T_t$$的损失函数是\(不进行剪枝时的损失函数\)：  
  $$C_{\alpha}(T_t) = C(T_t) + \alpha |T_t| $$  
当\alpha 小于某个值时，不剪枝会使得整体的损失函数较小，也就是：  
$$C_{\alpha}(T_t) \lt C_{\alpha}(t)$$  
当$$\alpha$$增大时，在某一$$\alpha$$有：  
$$C_{\alpha}(T_t) = C_{\alpha}(t)$$  
当$$\alpha$$再增大时，不等式反向，只要$$\alpha = \frac{C(t) - C(T_t)}{|T_t|-1}$$, $$T_t$$与t有相同的损失函数。  
  为此， 对$$T_0$$中每一个内部节点t,计算：  
  $$g(t) = \frac{C(t) - C(T_t)}{|T_t|-1}$$  
  它表示剪枝之后整体损失函数减小的程度，在$$T_0$$中减去$$g(t)$$最小的$$T_t$$,将得到的子树作为$$T_1$$,同时将最小的g\(t\)设为$$\alpha _1$$, $$T_1$$为区间$$[\alpha _1, \alpha _2)$$的最优子树。  
  如此剪枝下去，在这一过程中，不断的增加$$\alpha$$的取值，产生新的区间，最终会的到一组决策树$$(T_0,T_1, T_n )$$，对应于椅子确定的权衡参数$$(a_0,a_1,a_n)$$，通过验证集合中每颗树的总体误差，也就得到了最终的最优决策树$$T^*$$.

T是整颗决策树吗？

输入：CART 生成树 $$T_0$$

输出：剪枝后的最优树 $$T^*$$

1）设 k=0 ， $$T=T_0$$ ，$$a=+\infty$$

3\) 自下而上的对内部节点 t 计算 ：

$$g(t) = \frac{C(t) - C(T_t)}{|T_t|-1}$$  
a=min\(a,g\(t\)\)  
4\) 自上而下的访问内部节点 t ， 对最小的 g\(t\)=a 进行剪枝，并对叶节点 tt 以多数表决形式决定其类别，得到树 TT

5\) k=k+1,$$a_k=a$$, $$T_k=T$$

6\) 如果 T 为非单节点树，回到 4\).

7\) 对于产生的子树序列 $$(T_0,T_1, T_n)$$ 分别计算损失，得到最优子树$$T^*$$并返回.

![](/assets/CART_pruning.png)

### CART总结

CART\(classification and regression tree\)即分类回归树，意思是它既可以用来做分类页可以用来做回归。  
对分类问题，我们基于基尼系数构造分类树，其做法与ID3，C4.5一致。

对于回归问题，我们基于平方误差最小，构造回归树。构造就是，基于一个特征\(输入样本的一个维度\)，选择最佳的切分点，把当前数据集分成两部分，分别求两部分输出值的质心（平均值），再求两部分中的每一部分的输出值到该区域质心的平方误差，对平方误差求和，选择最佳切分点，使得这两部分的误差和最小，这就是该特征的最佳切分点，再在该数据集中的所有特征中这个误差，找到最佳的切分特征，最后得到最佳特征以及最佳特征的切分点。  基于这两个值，把数据集分成两部分，再对这两部分分别进行如上的操作（求最佳特征与该特征下的最佳切分点）

## 集成学习

集成学习就是组合一系列的弱分类器生成强分类器。组合的弱分类器之间的关系有两种:  

一种是彼此间相互依赖,一系列学习器之间需要串行生成，代表算法是Boosting系列算法，代表算法是AdaBoost与GBDT.  
一种是学习器彼此间无关联，一些列算法可以并行的生成，代表算法是Bagging\(Bootstrap Aggregating\)和随机森林（Random Forest）系列。随机森林是时Bagging的进行版，随机表现在两个方面，一方面是取样本点是随机的，另一方面，选择特征也是随机的（比如总共有N个特征，随机选取小于N的K个特征）
还有一种是两者的结合物：stacking
###集成学习的理论基础
我们考虑多个不相干分类器叠加处理二分类的问题，$$y \in (-1,+1)$$和真实的函数f,假设基分类器的错误率都是$$\epsilon$$, 即对每个分类器都有
$$P(h_i(x) \ne f(x)) = \epsilon$$  
假设集成通过简单的投票发结合T个基分类器，若半数的基分类器正确，则集成分类正确。
       $$H(x) = sign(\sum _{i=1}^{T}h_i(x))$$
 价格基分类器之间的错误率是相互独立的，则由Hoeffding不等式可知，集成的错误率为：
 $$P(H(x) \ne f(x)) = \displaystyle\sum_{i=0}^{[T/2]}\begin{pmatrix}
   T \\
   k
\end{pmatrix}(1 - \epsilon)^k\epsilon ^{T-k} \le exp(-\frac{1}{2}T(1-2\epsilon)^2)$$   
上式表明，随着集成中个体分类器数目T的增大，集成的错误率将指数下降，最终趋向于0.
###Boosting:
![](/assets/Boosting.png)
Boosting系列算法的代表就是AdaBoost。

###Bagging
![](/assets/Bagging.png)

[^1]: [决策树之 CART](https://www.cnblogs.com/ooon/p/5647309.html) [.](https://www.cnblogs.com/ooon/p/5647309.html)

