#PCA
机器学习一般面对的是一个高维的问题，样本稀疏。容易面临严重的维数灾难。处理维数灾难的一个重要方法就是降维。确
切的来说，有两种情形：  
1. 基于距离的聚类问题。对于M个数据，定义距离后，M个数据张成一个MxM的距离矩阵，而且是对称矩阵。  
2. 基于数据的低秩分解。对于M个数据，每个数据是N维，我们是对这个数据集张成的MxN维矩阵进行降维。  
这就可以用到我们在矩阵计算中的一些方法，比如主成分分析\(PCA\),实际上我们是通过SVD来是实现PCA的，就是取最大的K个奇异值与奇异向量来实现矩阵的低秩分解。  
下面我们主要考虑第一种情况，也就是处理聚类问题。

## SVD

对于任何一个MxD的矩阵X可以做如下分解：  
&emsp;&emsp;$$ X = UDV^T$$  
其中$$U^TU=I_D,VTV=I_N$$,D是MxD的对角矩阵。  
&emsp;&emsp;$$XX^T = UDV^TVD^TU^T=UDD^TU^T$$  
&emsp;&emsp;$$X^TX = VD^DU^TUDV^T=VD^TDV^T$$  
是矩阵的本征值分解。因此可以通过上面两式来求解U，V。主要用到的是矩阵本征值，本征矢求解技巧。这都在矩阵计算\(第五章\)那一章讨论的内容。  
对于我们的情况M=D, 因此  
&emsp;&emsp;$$D^T=D$$  
令$$B = X^TX$$，对B做特征值分解，得到  
&emsp;&emsp;$$B = V\Lambda V^T$$，  
假设$$\Lambda$$的矩阵元按如下方式排序：  
&emsp;&emsp;$$\lambda_1 \ge \lambda_2 \ge ... \ge \lambda_M$$

## PCA

我们可以通过SVD来实现PCA,具体就是只取X的SVD中的前M个其一分量。  
&emsp;&emsp;$$X = UDV^T \approx U_MD_MV_M^T$$  
其中$$U_M,D_M,V_M$$对于矩阵X的前M个奇异值分量。  
取前d个奇异向量。选择一个比例t，一般取t = 95%，使得。  
&emsp;&emsp;$$\frac{\displaystyle \sum_{i=1}^d \lambda_i}{\displaystyle \sum_{i=1}^M \lambda_i} \ge t$$

#### PCA的好处

1. 通过降维，使得样本的采样密度变大，这是降维的主要目的 [^1]
2. 当数据收噪声影响时，小的特征值所对应的特征向量一般受噪声影响，因此去掉这些量在一定程度上能起到去噪的作用。  

##PCA成立的条件
PCA依赖于以线性合并为基础的经验数据集(这里不讨论非线性PCA)，PCA另外一个重要的假设是原始数据集是从高斯分布中抽取出来的。当这个假设不正确时，就无法保证主要成分的有效性。  
这里我是有两个疑问的：  
1. 高斯分布的矩阵元，而产生的对称矩阵，可以通过随机矩阵分析，得到其展平的能谱时服从高斯正交分布，进而简并的能级概率为0，而保证最大的一些能级占的能量可以占到整个系统能量的很大比，比如95%。但是非高斯分布就可以吗？比如t-分布(在极端情况下就是高斯分布)  
2. 一般推荐系统中的UI矩阵都是非负矩阵，明显就不是服从高斯分布的，为啥还可以用SVD?能从正交变换来考虑吗？(解析就是，只要高斯分布的均值就是数据的均值就可以了。但是购物的均值是有限的，最大值确实无限的，因此也不是完美的高斯分布。不是完美的高斯分布也没关系，大致符合就可以了)。还是我们需要讨论服从泊松分布的随机矩阵的本征值问题？当然，可以直接通过数值实验来验证。之所以要研究泊松分布，使用时因为元素为0的概率最大。  
3. 高斯分布是大自然中最常见的，因为大数定理使得。二项分布

###二项分布，泊松分布于高斯分布
二项分布的期望是$$np$$, 方差是$$np(1-p)$$。  
当$$n \to \infty$$时，p很小，二项分布就趋近于泊松分布，也就是期望与方差一样都是np；p不接近于0或者1时，就趋近于均值为np，方差是$$np(1-p)$$的高斯分布。   
因此，即使是二项分布，大量独立事件的总体效果就是高斯分布。  
###中心极限定理
中心极限定理：设随机变量序列$$X_i$$相互独立，具有相同的期望与方差，也就是$$E(X_i) = u, D(X_I) = \sigma^2$$,令：  
&emsp;&emsp;$$Y_n = X_1+...+X_n$$，  
&emsp;&emsp;$$Z_n = \frac{Y_n - E(Y_n)}{\sqrt{n}\sigma}$$，     
则$$Z_n \to N(0,1)$$   
因此独立同分布产生的数据的总的效果就是高斯分布。  

[^1]: 周志华《机器学习》

