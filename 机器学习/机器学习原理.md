# 机器学习原理

## 偏差，方差，噪声

偏差-方差分解可以用来对学习算法的期望泛化错误率进行拆分。同时，偏差-方差分解为我们设计模型与分析提供了一个比较清晰的方向。  
假设测试样本是$$\mathbf{x}$$，令$$\mathbf{y_D}$$为$$\mathbf{x}$$在数据集上的标记\(可能存在标记错误的情况\)，y为$$\mathbf{x}$$的真实标记，$$f(\mathbf{x};D)$$为训练集D上学得模型f在$$\mathbf{x}$$上的预测输出，以回归模型为例，学习算法的期望预测为：  
$$\kern{4 em} \hat f(\mathbf{x}) = E_D[f(\mathbf{x},D) ]$$  
方差是：  
$$\kern{4 em} var(\mathbf{x}) = E_D[f(\mathbf{x},D) - \hat f(\mathbf{x})]^2$$  
噪声是：  
$$\kern{4 em} \epsilon ^2= E_D[y_D - y]^2$$  
输出期望与真实标记之间的差别成为偏差（Bias），即：  
$$\kern{4 em} bias ^2(\mathbf{x}))= E_D[\hat f(\mathbf{x}) - y]^2$$  
为方便讨论，假设噪声的期望为0； 即：$$E_D[y_D - y] = 0$$. 下面来对模型的期望泛化误差进行分解：  
$$\kern{4 em} E(f;D) = E_D[(f(\mathbf{x},D) - y_D)^2]$$  
$$\kern{8 em} = E_D[(f(\mathbf{x},D) - \hat f(\mathbf{x})+ \hat f(\mathbf{x})- y_D)^2]$$  
$$\kern{8 em} = E_D[(f(\mathbf{x},D) - \hat f(\mathbf{x}))^2]+ E_D[(\hat f(\mathbf{x})- y_D)2] + E_D[2(f(\mathbf{x},D) - \hat f(\mathbf{x}))(\hat f(\mathbf{x})- y_D)]$$  
$$\kern{8 em} = E_D[(f(\mathbf{x},D) - \hat f(\mathbf{x}))^2]+ E_D[(\hat f(\mathbf{x})- y_D)^2]$$  
$$\kern{8 em} = E_D[(f(\mathbf{x},D) - \hat f(\mathbf{x}))^2]+ E_D[(\hat f(\mathbf{x})- y + y -y_D)^2]$$  
$$\kern{8 em} = E_D[(f(\mathbf{x},D) - \hat f(\mathbf{x}))^2]+ E_D[(\hat f(\mathbf{x})- y)^2] + E_D[(y -y_D)^2] + E_D[2(\hat f(\mathbf{x})- y)(y-y_D)]$$  
$$\kern{8 em} = E_D[(f(\mathbf{x},D) - \hat f(\mathbf{x}))^2]+ E_D[(\hat f(\mathbf{x})- y)^2] + E_D[(y -y_D)^2] $$   
因此就得到：  
$$\kern{4 em} E(f;D)= bias^2(\mathbf{x}) + var(\mathbf{x}) + \epsilon ^2 $$   
也就是泛化误差分成偏差，方差与噪声之和。我们设计模型分析时，可以从这三方面去考虑。

### 噪声

为了消除噪声的影响，我们需要对数据进行清洗，进行预处理。在很大程度上就是为了得到更干净的数据。

### Bias

偏差小，说明的是模型很准，可能有过拟合的倾向，增加模型的复杂度可以使得bias减小。但是泛化能力变差，也就是variance会大，模型对数据很敏感。 如果模型bias大，可以通过对简单的模型进行boost，来提升模型的准确性，也就是Boost系列算法做的事情,如：AdaBoost, GBDT, XGBoost。

### 方差

方差小，说明模型很稳定，也就是说模型的泛化能力好，可能测试集上的数据相对于训练集来说有一些变化，但是也不影响模型的输出结果，此时，可能模型欠拟合，因此泛化能力好。如果模型variance大，  
可以通过训练多个模型，并让各个模型之间的关联性很小，通过求平均来减小Variance，这就是集成模型中bagging系列算法做的事情，如Bagging,Random Forest.

## 最大似然函数

输入：$$X \in R^n$$  
输出： $$Y \in (1,2,...,K)$$  
条件概率：  $$P(X=x|Y=C_k) = P(X^{(1)} = x^{(1)},X^{(2)} = x^{(2)},...,X^{(n)} = x^{(n)}|Y=C_k)$$,  k=1,2,...,K  
假设有N个样本点$$(X_i,Y_i), i=1,2,...,N$$  
条件概率： $$P(Y=y|X=x,\mathbf{\theta})$$，其中$$\mathbf{\theta}$$是模型的参数。  
似然函数定义为：每个样本点发生概率的乘积：   
$$L(\mathbf{\theta}) = \displaystyle \prod _{n=1}^NP(Y=Y_n|X=X_n,\mathbf{\theta})$$  
我们需要求得一个模型，使得在所有样本点在该模型发生的几率极大，几率是联合几率，是每个样本发生几率的乘积。也就是极大化似然函数。  
分类与回归问题似乎都可以转化成求解似然函数。

### 用极大似然函数求解回归问题

输入：$$\mathbf{X} \in R^n$$  
输出： $$Y \in R$$  
我们的模型是$$f(\mathbf{X}, \mathbf{\Theta})$$,其中$$\mathbf{\Theta}$$是模型参数，X是输入变量。  
假设我们的损失函数是平方误差，因此我们的cost function可以表示如下：  
$$\kern{4 em} L(\mathbf{X},\mathbf{\Theta}) = \frac{1}{2}\displaystyle \sum _{n=1}^N(f(X_n, \mathbf{\Theta}) - Y_n)^2$$  
等价于：  
$$\kern{4 em} e^{L(\mathbf{X},\mathbf{\Theta})}= e^{\displaystyle \prod _{n=1}^N \frac{1}{2}(f(X_n, \mathbf{\Theta}) - Y_n)^2} = \displaystyle \prod _{n=1}^Ne^{ \frac{1}{2}(f(X_n, \mathbf{\Theta}) - Y_n)^2}$$  
实际上，最小化$$L(\mathbf{X},\mathbf{\Theta})$$等价于最小化$$e^{L(\mathbf{X},\mathbf{\Theta})}$$,等价于最大化$$e^{-L(\mathbf{X},\mathbf{\Theta})}$$,也就是等价于最大化  
$$\kern{4 em}\Gamma(\Theta) = \displaystyle \prod _{n=1}^Ne^{ -\frac{1}{2}(f(X_n, \mathbf{\Theta}) - Y_n)^2} = \displaystyle \prod _{n=1}^N P(X_n, \Theta)$$  
其中  
$$\kern{4 em} P(X_n, \Theta) = e^{ -\frac{1}{2}(f(X_n, \mathbf{\Theta}) - Y_n)^2}$$  
定义成回归模型中样本$$(X_n, Y_n)$$发生的几率，这样我们就把回归问题与几率联系起来了。我们把求解损失函数最小，转化成极大化似然函数的求解。  
对于不同的损失函数，我们都可以转化成对应的概率。  
对于分类问题，怎么用一个统一的框架来解析？可以是基于概率的。

### 用极大似然函数求解多类分类问题

在这里我们以多项逻辑斯蒂回归为例，讨论怎么用最大化似然函数来求解这一类问题。  
假设离散性随机变量Y的取值是\(1,2,...,K\),输入变量$$\mathbf{X} \in R^n$$  
则，模型如下：  
$$\kern{4 em} P(Y=k|x) = \frac{exp(\mathbf{w_k}\mathbf{x})}{1+\displaystyle \sum_{k=1}^{K-1}exp(\mathbf{w_k}\mathbf{x})}, k=1,2,...,K-1$$  
$$\kern{4 em} P(Y=K|x) = \frac{1}{1+\displaystyle \sum_{k=1}^{K-1}exp(\mathbf{w_k}\mathbf{x})}$$  
这里$$\mathbf{x} \in \mathbf{R^{n+1}};\mathbf{w_k} \in \mathbf{R^{n+1}}$$.  
因此，总的似然函数可以表示为：  
$$\kern{4 em}L(\mathbf{W}) = \displaystyle \prod _{n=1}^NP(Y=Y_n|X=X_n)$$   
这样，我们需要求的参数是$$W = (\mathbf{w_1},\mathbf{w_2},...,\mathbf{w_{n+1}})$$.

### 用极大似然与SVM来做多类分类问题

核心，定义出到超平面的距离，每一个类，一个超平面，然后定义距离，然后转化成概率，最终转化成极大似然函数求解。

## 极大似然与最大熵原理的等价性

## 最大熵原理

## 奥卡姆剃刀

### Evaluation Metrics

二分类： AUC，  
多分类： 交叉熵\(log loss\)  
回归： 均方差  
For any kind of machine learning problem, we must know how we are going to evaluate our results, or what the evaluation metric or objective is. For example in case of a skewed binary classification problem we generally choose area under the receiver operating characteristic curve \(ROC AUC or simply AUC\). In case of multi-label or multi-class classification problems, we generally choose categorical cross-entropy or multiclass log loss and mean squared error in case of regression problems.

### NO FREE LUNCH
![](/assets/NO_FREE_LUNCH.png)
黑点是训练样本，白点是测试样本点。根据训练样本，我们得到模型A，B。如果测试点不同，则A，B的相对表现不同。问题的关键在于，我们先验的假设了(测试)样本点在所有空间是均匀分布的，因此，在训练集上训练出的所有模型的效果是一样的，因为你总有样本集是落在你模型预测的曲线之上的。  
因此，如果样本点在整个数据空间是均匀分布的前提下，则不存在一个模型或算法在所有问题上比其它算法更优越。  
但是现实遇到的问题是，数据的分布是受限制的，数据存在一个先验分布，因此会存在一些模型比其它模型更有效。这里NFL不再有是因为问题的前提--数据是均匀分布的--不再成立。  

