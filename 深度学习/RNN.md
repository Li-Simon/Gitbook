# RNN

建模序列化数据的一种主流深度学习模型。

## 设计目的

捕获长距离输入之间的依赖。

###### 背景：

传统的前馈神经网络一般输入的都市一个定长的向量，无法处理变长的序列信息，即使通过一些方法把序列处理成定长的向量，模型也很难捕捉序列中的长距离依赖关系。RNN则通过将神经元串行起来处理序列化的数据。由于每个神经元能用它的内部变量保存之前输入的序列信息，因此整个序列被浓缩成抽象的表示，并可以据此进行分类或生成新的序列。近年来，得益于计算能力的大幅度提升和模型的改进，RNN在很多领域取得了突破性的进展--机器翻译，图像描述，推荐系统，智能聊天机器人，自动作词作曲等。

### 循环神经网络（RNN）

#### 最简单的RNN

网上最易懂得结构图[^1]：  
![](/assets/RNN_simple_2.png)  
一个简单的RNN内部的结构是：  
![](/assets/RNN_sim_2.png)  
展开图：  
![](/assets/RNN_unfold.png)  
RNN主要由矩阵Ｕ,W，V,以及激活函数f,g决定。  
图中左边是RNN的一个基本模型，右边是模型展开之后的样子。展开是为了与输入样本匹配。假若输入时汉语句子，每个句子最长不超过20（包含标点符号），则把模型展开20次[^2]。  
1. $$x_t$$ 代表输入序列中的第tt步元素，例如语句中的一个汉字。一般使用一个one-hot向量来表示，向量的长度是训练所用的汉字的总数（或称之为字典大小），而唯一为1的向量元素代表当前的汉字。  
2. $$s_t$$代表第t步的隐藏状态，其计算公式为$$s_t=tanh(Ux_t+Ws_{t-1})$$。也就是说，当前的隐藏状态由前一个状态和当前输入计算得到。考虑每一步隐藏状态的定义，可以把$$s_t$$视为一块内存，它保存了之前所有步骤的输入和隐藏状态信息。$$s_{-1}$$是初始状态，被设置为全0。  
3. $$o_t$$是第t步的输出。可以把它看作是对第t+1步的输入的预测，计算公式为：$$o_t=softmax(Vs_t)$$。可以通过比较$$o_t$$和$$x_{t+1}$$之间的误差来训练模型。

U,V,W是RNN的参数，并且在展开之后的每一步中依然保持不变。这就大大减少了RNN中参数的数量。

##### 一个例子

假设我们要训练的中文样本中一共使用了3000个汉字，每个句子中最多包含50个字符，则RNN中每个参数的类型可以定义如下。  
1. $$x_t \in R^{3000}$$,第t步的输入，是一个one-hot向量，代表3000个汉字中的某一个。  
2. $$o_t\in R^{3000}$$,第t步的输出，类型同$$x_t$$。  
3. $$s_t\in R^{50}$$,第t步的隐藏状态，是一个包含50个元素的向量。RNN展开后每一步的隐藏状态是不同的。  
4. $$U\in R^{50\times 3000}$$, 在展开后的每一步都是相同的。  
5. $$V\in R^{3000\times 50}$$, 在展开后的每一步都是相同的。  
6. $$W\in R^{50\times 50}$$, 在展开后的每一步都是相同的。

其中$$x_t$$是输入，U,V,W是参数，$$s_t$$是由输入和参数计算所得到的隐藏状态，而$$0_t$$则是输出。$$s_t$$和$$o_t$$的计算公式已经给出，为清晰起见，重新写出。  
$$s_t=tanh(Ux_t + Ws_{t-1})$$  
$$o_t=\text{softmax}(Vs_t)$$  
要训练的就是参数矩阵U,V,W

### RNN Goodfellow

循环神经网络中一些重要的设计模式包含如下几种：  
\(1\)每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络，如下图所示。  
RNN的输入到隐藏的连接由权值矩阵U参数化，隐藏到隐藏的循环由权重矩阵W参数化，隐藏到输出层的连接由权重矩阵V参数化。$$L^{(t)}$$是损失函数。  
![](/assets/RNN_General.png)  
前向传播公式：假设激活函数是tanh，我们有如下的更新方程：  
  $$ \mathbf{a}^{(t)} = \mathbf{b} + \mathbf{W}\mathbf{h}^{(t-1)} + \mathbf{U}x^{(t)}$$  
  $$ \mathbf{h}^{(t)} = \tanh(\mathbf{a})$$  
  $$ \mathbf{o}^{(t)} = \mathbf{c} + \mathbf{V}\mathbf{h}^{(t)} $$  
  $$\mathbf{\hat y}^{(t)} = softmax(\mathbf{o}^{(t)})$$  
其中$$\mathbf{b},\mathbf{c}$$是偏置向量,$$\mathbf{U},\mathbf{V},\mathbf{W}$$是权重矩阵。  
这一循环网络将一个输入序列映射到相同长度的输出序列。与$$\mathbf{x}$$序列配对的$$\mathbf{y}$$的总的损失就是所有时间步的损失之和。例如，$${L}^{(t)}$$为给定的$$x^{(1)},x^{(2)},...,x^{(t)}$$后$$y^{(t)}$$的负对数似然，则：  
  $$L((x^{(1)},x^{(2)},...,x^{(\tau)}),(y^{(1)},y^{(2)},...,y^{(\tau)})) = \displaystyle \sum_{t} L^{(t)}$$  
  $$ = -\displaystyle \sum_{t} \log p_{model}(y^{(t)}|(x^{(1)},x^{(2)},...,x^{(t)})$$  
其中$$p_{model}(y^{(t)}|(x^{(1)},x^{(2)},...,x^{(t)})$$需要读取模型输出向量$$\mathbf{\hat y}^{(t)}$$对应于$$\mathbf{y}^{(t)}$$的项。  
应用于展开图且代价为$$O(\tau)$$的反向传播算法是通过时间反向传播\(back-propagation through time, BPTT\)。  
\(2\)每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络。如下图所示：  
该图中，RNN被训练将特定输出值放入o中，并且o是被允许传播到未来的唯一信息。此处没有从h前向传播的直接连接。之前的h仅通过产生的预测间接地连接到当前。o通常缺乏过去的重要信息，除非它非常高维并且内容丰富。这使得该图中的RNN不那么强大，但是它更容易训练，因为每个时间步可以与其他时间步分离训练，允许训练期间更多的并行化。

![](/assets/RNN_DesignPattern_2.png)  
\(3\)隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络，如下图所示：  
这样的网络可以用于概括序列并产生用于进一步处理的固定大小的表示。在结束处可能存在目标，或者通过更下游模块的反向传播来获得输出$$o^{(t)}$$上的梯度。  
![](/assets/RNN_DesignPattern_3.png)

## Clockwork RNNs

CW-RNNs，时钟频率驱动循环神经网络  
![](/assets/CW_RNN_LSTM_result.png)  
上图中，绿色实线是预测结果，蓝色散点是真实结果。每个模型都是对前半部分进行学习，然后预测后半部分。LSTMs模型类似滑动平均，但是CW-RNNs效果更好。其中三个模型的输入层、隐藏层、输出层的节点数都相同，并且只有一个隐藏层，权值都使用均值为0，标准差为0.1的高斯分布进行初始化，隐藏层的初始状态都为0，每一个模型都使用Nesterov-style  momentum SGD\(Stochastic Gradient Descent，随机梯度下降算法\)进行学习与优化

## LSTM

由于Vanilla RNN具有梯度消失问题，对长关系的依赖的建模能力不够强大，也就是很长时刻以前的输入，对现在的网络影响非常小，后向传播那些梯度也很难影响很早以前的输入，即会出现题都消失的问题。而LSTM通过构建一些门，让网络能记住那些非常重要的信息，而这个核心的结构就是cell state。比如遗忘门，来选择性清空过去的记忆和更新较新的信息。  
两种常见的LSTM结构。  
第一种是带遗忘门的Traditional LSTM。  

![](/assets/LSTM_1.png)   
公式如下：  
  $$ f_t = \sigma_g(W_fx_t + U_fh_{t-1} + b_f)$$  
  $$ i_t = \sigma_g(W_ix_t + U_ih_{t-1} + b_i)$$  
  $$ o_t = \sigma_g(W_ox_t + U_oh_{t-1} + b_o)$$  
  $$ c_t = f_t*c_{t-1} + i_t*\sigma_c(W_cx_t + U_ch_{t-1} + b_c)$$  
  $$ h_t = o_t*\sigma_h(c_t)$$    
*表示向量中对应元素相乘。_$$c_t$$_是cell中t时刻保存的信息，_$$h_t$$_表示t时刻的输出。 $$f_t,i_t, o_t, c_t, h_t$$都是向量。  
前三行是三个门，分别是遗忘门f,输入门i,输出门o,输入都是_$$x_t,h_{t-1}$$,只是参数不同，然后要经过一个激活函数把值缩放到\[0,1\]之间。  
第四行_$$c_t$$_是cell state,由上一时刻的_$$c_{t-1}$$_和输入得到,两者相互独立。如果遗忘门_$$f_t$$_取0的话，也就是调节参数_$$W_f, U_f$$_使得对于任意的输入_$$x_t$$_与上一次的输出_$$h_{t-1}$$_都有_$$f_t$$_趋于0向量，那么上一时刻的状态就会全部被清空，然后只关注此时刻的输入，这与传统的RNN相似，只是多了一个_$$\sigma_h$$_操作。输入门_$$i_t$$_决定是否接收此时刻的输入。  最后输出门_$$o_t$$_决定是否输出cell state。  
$$\hat c = \sigma_c(W_cx_t + U_ch_{t-1} + b_c)$$
因此有$$c\_t = f_t*c_{t-1} + i_t *\hat c$$.  
这样一来cell state的更新来源就明显了，一部分是上时刻的自己，一部分是new memory content，而且两个来源是相互独立地由两个门控制的。遗忘门控制是否记住以前的那些特征，输入们决定是否接受当前的输入。后面可以看到GRU其实把这两个门合二为一了。

#### 总结

加入输入门与遗忘门，就是为了控制上一时刻与当前时刻的内容在cell state $$c_t$$中的比列，$$f_t, i_t$$的值介于\[0,1\],当取0的时候，就表示对应的内容不添加在cell state中。 因此，当前cell state的内容由上一时刻cell state中的内容以及这一时刻的输入决定[^2]。

### Peephole LSTM

第二种是带遗忘门的Peephole LSTM，公式如下：  
  $$ f_t = \sigma_g(W_fx_t + U_fc_{t-1} + b_f)$$  
  $$i_t = \sigma_g(W_ix_t + U_ic_{t-1} + b_i)$$  
  $$ o_t = \sigma_g(W_ox_t + U_oc_{t-1} + b_o)$$  
  $$c_t = f_t*c_{t-1} + i_t*\sigma_c(W_cx_t + b_c)$$  
  $$ h_t = o_t*\sigma_h(c_t)$$  
和上面的公式比较，发现只是把$$h_{t-1}$$换成了$$c_{t-1}$$，即三个门的输入都改成了\[$$x_t,c_{t-1}$$\],因为是从cell state里取得信息，所以叫窥视管\(peephole\)。  
把这两种结构结合起来，可以用如下图描述：  
![](/assets/LSTM_Structure.png)

图中连着门的那些虚线都市peephole。三个输入都是\[$$x_t,h_{t-1},c_{t-1}$$\]

## [GRU](https://blog.csdn.net/zhangxb35/article/details/70060295)

参考[循环神经网络\(RNN, Recurrent Neural Networks\)介绍](https://blog.csdn.net/heyongluoyao8/article/details/48636251)  
GRU这个结构2014年才出现，结构与LSTM类似，效果一样，但是精简一些，参数更少。公式如下：  
  $$ z_t = \sigma(W_zx_t + U_zh_{t-1})$$  
  $$ r_t = \sigma(W_rx_t + U_rh_{t-1})$$  
  $$ \hat h_t = \tanh(Wx_t + U(r_t*h_{t-1}))$$  
  $$h_t = (1-z_t)*h_{t-1} + z_t*\hat h_t$$  
四行的解释如下：  
  $$z_t$$是update gate，更新activation时的逻辑门。  
  $$r_t$$是reset gate，决定candidate activation时，是否要放弃以前的activate $$h_t$$  
  $$\hat h_t$$是candidate activation，接收\[$$x_t,h_{t-1}$$\]  
  $$h_t$$是activation，是GRU的隐层，接收\[$$h_{t-1},\hat h_{t}$$\].  
GRU相对于LSTM，它取消了输出门。由更新门与重置门共同控制怎么从上一刻的的隐藏状态$$h_{t-1}$$到当前的隐藏状态$$h_t$$。  
其中$$r_t$$ 用来控制需要 保留多少之前的记忆，比如如果 $$r_t$$ 为0，那么 $$\tilde{h}_t$$只包含当前词的信息。    
$$z_t$$控制需要从前一时刻的隐藏层 $$h_{t−1}$$ 中遗忘多少信息，需要加入多少当前时刻的隐藏层信息$$\tilde{h}_t$$,最后得到 $$h_t$$。   
如果reset门取1而update门取1，则退化到RNN。      

![](/assets/GRU_LSTM_Structure.png)  
现在看这图，就清晰很多，i,o,f都是门向量，作用就是控制流过这些门的流量，使得通过向量各个分量的通过门之后，其分量的值变为原来的x倍，$$x \in [0,1]$$是门的系数。就连输出也有一个输出门向量来控制。

从LSTM和GRU的公式里面可以看出，都会有门操作，决定是否保留上时刻的状态和是否接收此时刻的外部输入，LSTM是用遗忘门和输入门来做到的，GRU则是只用一个更新门\($$z_t$$\)。  
这种设计有两种解释，一种解释是说，网络是能很容易记住长依赖问题。即前面很久之前出现过一个重要的特征，如果遗忘门或者更新门选择不重写内部的memory，那么网络就会一直记住之前的重要特征，那么会对当前或者未来继续产生影响。另一点是，这种设计可以不同状态之间提供一条捷径，那么梯度回传的时候不会消失的太快，因此减缓了梯度消失带来的训练难问题。  
LSTM 和GRU的不同点。首先LSTM有一个输出门来控制memory content的曝光程度，而GRU则是直接输出。另外一点是要更新的new memory content的来源不同，$$\hat h_t$$会通过重置门控制从$$h_{t-1}$$中得到信息的力度，而$$\hat c_t$$则没有，而是直接输入$$h_{t-1}$$。

###RNN, LSTM, GRU比较
#####传统RNN vs 门控RNN
传统RNN的问题 ：长期依赖会造成梯度消失（多数情况下） 或梯度爆炸（少数情况下）。  
解决以上问题的方案：  
1. 以新的方法改善或者代替传统的SGD方法，如Bengio提出的clipped gradient;   
2. 设计更为精妙的activation function或recurrent unit，如LSTM和GRU。原因：LSTM和GRU都能通过各种Gate将重要特征保留，保证其在long-term 传播的时候也不会被丢失；还有一个作用就是有利于BP的时候不容易梯度消失。   

传统RNN和门控RNN的不同点：  
1. 传统RNN会每一步都重写”记忆“，而门控RNN可以在某些步骤保持原有”记忆“；  
2. 门控RNN收敛速度更快，泛化能力更好。   

#####LSTM vs GRU  
相同点； 都属于”加模型“  
不同点：   
1. LSTM可控”记忆“的曝光度，而GRU完全暴露”记忆“，是不可控的；
2. LSTM的新”记忆“是与forget gate独立的，而GRU的新”记忆“受update gate控制
 


## 注意力机制

[^1] RNN-LSTM-GRU-最小GRU  [https://www.jianshu.com/p/166db8ab3cef](https://www.jianshu.com/p/166db8ab3cef)  
[^2] RNN LSTM与GRU深度学习模型学习笔记  [https://blog.csdn.net/softee/article/details/54292102](https://blog.csdn.net/softee/article/details/54292102)  
[^3] RNN以及LSTM的介绍和公式梳理  https://blog.csdn.net/Dark_Scope/article/details/47056361  

 