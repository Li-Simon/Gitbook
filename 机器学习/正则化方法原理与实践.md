# 正则化方法原理

## Ridge回归，Shrink与SVD

&emsp;&emsp;$$\hat \beta ^{ridge} =  \arg min_{\beta}{\displaystyle \sum_{i=1}^N(y_i -\beta_0-\displaystyle \sum_{j=1}^px_{ij}\beta_j)^2 + \lambda \displaystyle \sum_{j=1}^p\beta_j^2}$$  
等价于:  
&emsp;&emsp;$$\hat \beta ^{ridge} =  \arg min_{\beta}{\displaystyle \sum_{i=1}^N(y_i -\beta_0-\displaystyle \sum_{j=1}^px_{ij}\beta_j)^2} $$  
&emsp;&emsp;$$subject to:  \displaystyle \sum_{j=1}^p\beta_j^2 \le t$$.  
上面的问题也等价于假定，$$y_i,\beta_i$$服从如下分布：  
&emsp;&emsp;$$ y_i \in N(\beta_0 + x_i^T\beta,\sigma^2)$$  
&emsp;&emsp;$$ \beta_i \in N(0,\tau^2)$$  
其中：$$\lambda = \sigma^2/\tau^2$$  
因此RRS\(Root sum square\)可以写成如下形式：  
&emsp;&emsp;$$RRS(\lambda) =(\mathbf{y} - \mathbf{X\beta})^T(\mathbf{y} - \mathbf{X\beta}) + \lambda \beta^T\beta $$  
通过对$$\beta$$求导并令其为0，可以得到：  
&emsp;&emsp;$$\hat \beta ^{ridge} = (\mathbf{X}^T\mathbf{X} + \lambda I)^{-1}\mathbf{X}^T\mathbf{y}$$  
为了建立起L2与SVD之间的联系， 我们对X进行SVD分解：  
&emsp;&emsp;$$ \mathbf{X} = \mathbf{UDV^T}$$  
我们重写没有正则化的最小二乘拟合：  
&emsp;&emsp;$$\mathbf{X}\beta^{ls} = (\mathbf{X}^T\mathbf{X} )^{-1}\mathbf{X}^T\mathbf{y} =\mathbf{UU^T}\mathbf{y} $$  
对于L2正则化的最小二乘法：  
&emsp;&emsp;$$ \mathbf{X}\beta^{ls} = (\mathbf{X}^T\mathbf{X} + \lambda I)^{-1}\mathbf{X}^T\mathbf{y} =\mathbf{UD}(\mathbf{D^2+\lambda I})^{-1}\mathbf{DU^T}\mathbf{y} $$  
&emsp;&emsp;&emsp;&emsp;&emsp;$$= \displaystyle \sum_{j=1}^p \mathbf{u_j}\frac{d_j^2}{d_j^2 + \lambda}\mathbf{u_j}^T\mathbf{y}$$  
 从上面的分式$$\frac{d_j^2}{d_j^2 + \lambda}$$可以知道，对于$$d_j \ll \lambda$$,则相应的方向会收缩到0。可以认为L2就是对数据进行了SVD分解后，只保留了$$d_j \gt \lambda$$的分量。

## Lasso\(L1\)回归

&emsp;&emsp;$$\hat \beta ^{ridge} =  \arg min_{\beta}{\displaystyle \sum_{i=1}^N(y_i -\beta_0-\displaystyle \sum_{j=1}^px_{ij}\beta_j)^2 + \lambda \displaystyle \sum_{j=1}^p|\beta_j|}$$  
等价于:  
&emsp;&emsp;$$\hat \beta ^{ridge} =  \arg min_{\beta}{\displaystyle \sum_{i=1}^N(y_i -\beta_0-\displaystyle \sum_{j=1}^px_{ij}\beta_j)^2} $$  
&emsp;&emsp;$$ subject to:  \displaystyle \sum_{j=1}^p|\beta_j| \le t$$.  
L1更容易产生稀疏性,因为椭圆与菱形更易在菱形顶点相交，而与圆形不容易在坐标轴上相切。
![](/assets/L1_L2_regulation.png)
###L1稀疏性的数学解析
一个数学上更严格的解析。。
上面的问题也等价于假定，$$y_i,\beta_i$$服从如下分布：  
&emsp;&emsp;$$ y_i \in N(\beta_0 + x_i^T\beta,\sigma^2)$$  
&emsp;&emsp;$$\beta_i \in (1/2\tau)exp(-|\beta|/\tau)$$  
其中： $$\tau = 1/\lambda$$  
因此$$\beta_i$$更容易分布在0的附件，也就是能级排斥（量子混沌里面的概念，L1是可积系统，L2是GOE）   
最小二乘一般通过Cholesky分解（$$p^3+Np^2/2$$）或者QR分解($$NP^2$$)来实现，前者一般更快，但是没有后者稳定。
