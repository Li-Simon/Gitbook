# 随机坐标下降法

随机梯度法是对样本进行随机采样，当然我们也可以对模型的维度进行随机采样，这就是我们要介绍的随机坐标下降法。它的更新公式如下：   
&emsp;&emsp;$$w_{t+1,j_t} = w_{t,j_t} - \eta_t \nabla_{j_t}f(w_t)$$  
其中$$j_t$$表示第t次迭代中随机选取的维度标号，$$\nabla_{j_t}f(w_t)$$是损失函数对于模型$$w_t$$中第$$j_t$$个维度的偏导数。

### 随机坐标下降法流程

Initialize: $$w_0$$  
&emsp;Iterate: for t = 0,1,...,T-1  
&emsp;&emsp;1. 随机选取一个维度$$j_t \in [1,2,...,d]$$  
&emsp;&emsp;2. 计算梯度$$\nabla_{j_t}f(w_t)$$  
&emsp;&emsp;3. 更新参数：  
&emsp;&emsp;&emsp;$$w_{t+1,j_t} = w_{t,j_t} - \eta_t \nabla_{j_t}f(w_t)$$  
end

##### 有放回抽样

一方面，如果采样方法是有放回的，那么可以得到:       
&emsp;&emsp;$$E_{j_t}[\nabla_{j_t}f(w_t)] = \frac{1}{d}\nabla f(w_t)$$  
此处$$\nabla_{j_t}f(w_t)$$是d维向量，其中第$$j_t$$个维度是$$\nabla_{j_t}f(w_t)$$,其它维度是0。因此在期望方面，随机坐标梯度与梯度方面是一致的。  
另一方面，对于线性模型，计算一个维度的梯度所需要的计算量只是计算整个梯度向量的1/d。因此，随机坐标梯度可以作为原始梯度的高效替代品，尤其在维度较高时。

#### 偏导数

定义：如果对任意模型$$w\in R^d$$,对于维度j存在常数$$\beta_j$$，使得对任意$$\delta \in R$$有下面不等式成立：    
&emsp;&emsp;$$|\nabla_{j}f(w+\delta e_j) -\nabla_{j}f(w)| \le \beta_j |\delta|$$  
则称目标函数f对于维度j具有$$\beta_j$$-Lipschitz连续的偏导数。  
如果f对每个维度的偏导数都是Lipschitz连续的，我们记$$\beta_{max} = \displaystyle \max_{j=1,...,d}\beta_j$$。  
定理：假设目标函数f是$$R^d$$上的凸函数，并且具有$$\beta_j$$-Lipschitz连续的偏导数，记：  
&emsp;&emsp;$$w^*= \displaystyle \arg \min_{||w||\le D} f(w)$$,  
当步长$$\eta = 1/\beta_{max}$$时，随机坐标下降法具有如下的次线性收敛速率：     
&emsp;&emsp;$$E f(w_T) - f(w^*) \le \frac{2d\beta_{max}D^2}{T}$$  
定理： 假设目标函数f是$$R^D$$上的$$\alpha$$-强凸函数，并且具有$$\beta_j$$-Lipschitz连续的偏导数，当步长$$\eta = 1/\beta_{max}$$时，随机坐标下降法具有如下的次线性收敛速率：     
&emsp;&emsp;$$E f(w_T) - f(w^*) \le (1-\frac{\alpha}{d\beta_{max}})^T(f(w_0)-f(w^*))$$  
对比梯度下降法的收敛速率，我们发现随机坐标下降法的收敛速度关于迭代次数的阶数与梯度下降法是一致的。从这个意义上讲，随机坐标下降法的理论性质优于随机梯度下降法。[^1]  
####可分离情况
这种偏导数的计算量小于梯度的计算量的情形一般称为"可分离"情形，对于线性模型，常用损失函数都是可分离的。
##随机块坐标下降法
Initialize: $$w_0$$  
&emsp;将d个维度均等分为J块。  
&emsp;Iterate: for t = 0,1,...,T-1  
&emsp;&emsp;1. 随机选取一个维度$$J_t \in [1,2,...,J]$$  
&emsp;&emsp;2. 计算梯度$$\nabla_{J_t}f(w_t)$$  
&emsp;&emsp;3. 更新参数：  
&emsp;&emsp;&emsp;$$w_{t+1,J_t} = w_{t,J_t} - \eta_t \nabla_{J_t}f(w_t), j\in J_t$$  
end

[^1]: 《分布式机器学习--算法，理论与实践》刘铁岩

