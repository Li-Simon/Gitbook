# 优化收敛位置

[参考知乎上面](https://www.zhihu.com/question/68109802)  
对于N个参数的系统（神经网络），对应的Hassian阵是N阶的，我们假定其本征值的正负概率都是0.5.因此，要保证是局部最小值，则Hassian正定，意味着所有的本征值都为正。其概率是$$\frac{1}{2^N}$$。同样，是局部最大值，则Hassian阵负定，概率也为$$\frac{1}{2^N}$$。因此，最有可能的是本征值有正有负，也就是鞍点的情形。但是即使是普通的鞍点，函数不会震荡，而是马上逃离。  
实际的情形是，函数收敛到了一个大的平坦区域，其表现就是一阶导数很小，以至于在训练结束的时候还没有逃离出这个平坦区域。那么，我们得从数学上讨论这种平坦区域（平坦马鞍面）具有什么样的性质。可以看下面一个方程：  
$$f(x,y) = \frac{\alpha}{2}x^2 - \frac{\beta}{2}y^2$$  
$$f_{x} = \alpha x; f_{y} = -\beta y$$.假设$$\alpha > 0, \beta > 0$$  
\(0, 0\)点是唯一的鞍点。在x方向，该点是局部最小值，但在y方向，是局部极大值，如果$$\beta$$很小，很小是它与学习率r相乘结果与1来说的。假设学习率是r一开始$$y = \Delta _0$$,则一次迭代后  
$$\Delta _1 = \Delta _0 + r*\beta*\Delta _0 = (1 + r\beta)\Delta $$  
$$\Delta _2 = \Delta _1 + r*\beta*\Delta _1 = (1 + r\beta)\Delta _1$$  
n次迭代后：   

如果训练总的次数为N， $$r\beta \leq \frac{1}{N}$$,那N次迭代之后 $$（1 + r\beta）^N \Delta _0 \leq (1 + \frac{1}{N})^N \approx e\Delta _0 = 2.71828 \Delta _0$$，这定量的给出了在训练结束的时候，函数能多大程度的逃离（0，0）点。  
这个模型可以推广到N维情形。假设Hassian阵有N-K个本征值非负，K个小于零。  
考虑通过平移后，鞍点处于（0，0....0）点附近的二阶展开：  
$$L(X) = L(0) + \sum _{i=K + 1} ^{i=N}\frac{\alpha _i}{2}x_i^2 - \sum _{i=1} ^{i=K}\frac{\alpha _i}{2}x_i^2$$  
如果满足所有的$$\alpha _i \ll 1, i \in [1,K]$$，这时候，函数会在很长时间内，局域在鞍点附件。  
可以考虑的时，怎么通过设计Cost Function来避免产生这些超级马鞍面。  
这些超级马鞍面的产生要考虑激活函数吗，时候Relu比SIgmoid不容易产生这些马鞍面？

# 激活函数的作用

增加非线性，增强学习能力

# 为啥使用收敛慢的（随机）梯度下降法

因为只需要计算一阶导数，而不需要计算二阶，因为几十万以上的参数，计算二劫导数太费时间,内存也是个问题（除非用L-BFGS）

# SVD PCA

SVD 将矩阵分解成累加求和的形式，其中每一项的系数即是原矩阵的奇异值。这些奇异值，按之前的几何解释，实际上就是空间超椭球各短轴的长度。现在想象二维平面中一个非常扁的椭圆（离心率非常高），它的长轴远远长于短轴，以至于整个椭圆看起来和一条线段没有什么区别。这时候，如果将椭圆的短轴强行置为零，从直观上看，椭圆退化为线段的过程并不突兀。回到 SVD 分解当中，较大的奇异值反映了矩阵本身的主要特征和信息；较小的奇异值则如例中椭圆非常短的短轴，几乎没有体现矩阵的特征和携带的信息。因此，若我们将 SVD 分解中较小的奇异值强行置为零，则相当于丢弃了矩阵中不重要的一部分信息。

因此，SVD 分解至少有两方面作用：

* 分析了解原矩阵的主要特征和携带的信息（取若干最大的奇异值），这引出了主成分分析（PCA）；
* 丢弃忽略原矩阵的次要特征和携带的次要信息（丢弃若干较小的奇异值），这引出了信息有损压缩、矩阵低秩近似等话题。



