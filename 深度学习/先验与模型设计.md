#先验与算法设计
这一节是想努力的达成这样一个目的，也就是根据“没有免费午餐定理”我们知道，不存在一个普适的算法在所有场合下都表现很好。因此，我们到底该怎么设计我们的算法，也就是基于不同的问题背景，得出先验知识，然后把先验知识作为我们设计算法的基本条件。我们首先要分析的是一些比较成功的网络，比如CNN，RNN，可能还有一些机器学习模型；分析它怎么利用起了先验知识而获得了相比于其它模型更好的成果。因此，这节的核心是先验与算法设计。最终的目的是通过分析已有的成功经验，借鉴这些经验而把它用在未来的问题上面去，也就是《论语》中的"告诸往而知来"。  
### NO FREE LUNCH
没有免费的午餐定理：没有一个模型是对所有的样本是最优的。  
比如下面的例子：  
![](/assets/NO_FREE_LUNCH.png)  
黑点是训练样本，白点是测试样本点。根据训练样本，我们得到模型A，B。如果测试点不同，则A，B的相对表现不同。问题的关键在于，我们先验的假设了\(测试\)样本点在所有空间是均匀分布的，因此，在训练集上训练出的所有模型的效果是一样的，因为你总有样本集是落在你模型预测的曲线之上的。  
因此，如果样本点在整个数据空间是均匀分布的前提下，则不存在一个模型或算法在所有问题上比其它算法更优越。  
但是现实遇到的问题是，数据的分布是受限制的，数据存在一个先验分布，因此会存在一些模型比其它模型更有效。这里NFL不再有是因为问题的前提--数据是均匀分布的--不再成立。因此，脱离场景谈方法是无效的，我们只能针对特定的问题，寻找最佳的优化方法。

### 正则化

没有免费的午餐定理告诉我们没有一个模型是对所有的样本是最优的[^1]，因此，对于特定的样本，我们需要把先验加进模型中\(也就是损失函数中\)，这个先验就是我们对这个样本的知识，比如样本数据的分布，样本的均值或者方差等\(统计物理学中的系统的能量可以看成一种均值，统计物理学中有很多启发性的模型\)。加到模型中的就是我们的正则项，正则项就是一个先验，是我们需要模型满足的约束。  
一个问题是，正则化只能使我们的模型更稳定，不会更精确，那先验怎么作为正则化的形式加进模型，使得我们的模型更优越呢？  

###一些技术
1. 误差反向传播，梯度下降方法
2. 梯度消失：Relu, leak-Relu
3. 循环网络，矩阵相乘与梯度爆炸： 
4. Pooling：特性提取，几何变换不变性  
5. Dropout与Bagging：  
6. 正则化  
7. 卷积  
8. 批量归一化与泛化能力  

##如何搭建一个卷积神经网络
在本章第一节，我们介绍了如何实现一个误差反向的神经网络。  


