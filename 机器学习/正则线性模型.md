#正则化回归
在线性回归分析中，如果是欠定问题，则可以把正则化应用于回归分析中，主要涉及L1(Lasso Regression),L2(Ridge Regression),L1+L2(Elastic Net)正则化。此外，Early Stopping也是一种正则化方法。   
欠定问题在当前的一个核心，最典型的就是深度学习，一般模型的参数数目远大于数据的数目，因此需要一些正则化方法。比如L1(Lasso Regression),L2(Ridge Regression),L1+L2(Elastic Net)正则化，Early Stopping。 
##线性回归问题
对于线性系统，也就是预测函数：  
&emsp;&emsp;$$ f(\mathbf{x}) = \beta_0 + \displaystyle \sum_{j=1}^px_{j}\beta_j$$  
然后通过最小二乘求解符合数据的模型。通过加正则项来约束模型的复杂度或者参数范围。对于非线性模型，只是把模型$$f(\mathbf{x})$$换成你想要的非线性函数即可，比如N元M次多项式，选最简单的就是N元二次方程，这是最简单的非线性模型。  
##非线性回归问题
###光学建模
物理学中有一些很有意思的非线性回归模型，比如光的衍射成像，光通过特定结构的衍射，反射成像，求解这些模型，第一步是建model,也就是通过求解Maxwell方程，得到非线性方程组，第二步就是通过实验数据来求解模型中的参数，这些参数有两类，一类书几何参数，也就是涉及到要处理问题的几何结构；第二类是物理参数，主要是描述系统材料属性的参数，比如折射率，反射率，光的波长等。  
###神经网络
一个典型的非线性回归例子就是深度学习，神经网络就是一个高度非线性的函数，非线性是由激活函数决定的，也就是激活函数是非线性函数。 通过这个非线性函数，可以把输入的数据映射到输出的数据空间；高度的非线性是神经网络具有强大的非线性拟合能力的根本。神经网络的表达能力由下面的定理给出。
####万能近似定理
万能近似定理（universal approximation theorem）(Hornik et al., 1989;Cybenko, 1989) 表明，一个前馈神经网络如果具有线性输出层和至少一层具有任何一种‘‘挤压’’ 性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的Borel 可测函数。
