#RNN
循环神经网络主要用于序列模型，为了捕获CNN捕获不了的序列的长程关联。LSTM的提出是为了弥补传统RNN梯度消失的问题，GRU是为了弥补LSTM的 ..  
##RNN
### 循环神经网络

循环神经网络中一些重要的设计模式包含如下几种：  
\(1\)每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络，如下图所示。  
RNN的输入到隐藏的连接由权值矩阵U参数化，隐藏到隐藏的循环由权重矩阵W参数化，隐藏到输出层的连接由权重矩阵V参数化。$$L^{(t)}$$是损失函数。  
![](/assets/RNN_General.png)  
前向传播公式：假设激活函数是tanh，我们有如下的更新方程：  
$$\kern{8 em} \mathbf{a}^{(t)} = \mathbf{b} + \mathbf{W}\mathbf{h}^{(t-1)} + \mathbf{U}x^{(t)}$$    
$$\kern{8 em} \mathbf{h}^{(t)} = \tanh(\mathbf{a})$$    
$$\kern{8 em} \mathbf{o}^{(t)} = \mathbf{c} + \mathbf{V}\mathbf{h}^{(t)} $$    
$$\kern{8 em} \mathbf{\hat y}^{(t)} = softmax(\mathbf{o}^{(t)})$$    
其中$$\mathbf{b},\mathbf{c}$$是偏置向量,$$\mathbf{U},\mathbf{V},\mathbf{W}$$是权重矩阵。  
这一循环网络将一个输入序列映射到相同长度的输出序列。与$$\mathbf{x}$$序列配对的$$\mathbf{y}$$的总的损失就是所有时间步的损失之和。例如，$${L}^{(t)}$$为给定的$$x^{(1)},x^{(2)},...,x^{(t)}$$后$$y^{(t)}$$的负对数似然，则：  
$$\kern{8 em} L((x^{(1)},x^{(2)},...,x^{(\tau)}),(y^{(1)},y^{(2)},...,y^{(\tau)})) = \displaystyle \sum_{t} L^{(t)}$$  
$$\kern{8 em} = -\displaystyle \sum_{t} \log p_{model}(y^{(t)}|(x^{(1)},x^{(2)},...,x^{(t)})$$  
其中$$p_{model}(y^{(t)}|(x^{(1)},x^{(2)},...,x^{(t)})$$需要读取模型输出向量$$\mathbf{\hat y}^{(t)}$$对应于$$\mathbf{y}^{(t)}$$的项。
应用于展开图且代价为$$O(\tau)$$的反向传播算法是通过时间反向传播(back-propagation through time, BPTT)。  
\(2\)每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络。如下图所示：  
该图中，RNN被训练将特定输出值放入o中，并且o是被允许传播到未来的唯一信息。此处没有从h前向传播的直接连接。之前的h仅通过产生的预测间接地连接到当前。o通常缺乏过去的重要信息，除非它非常高维并且内容丰富。这使得该图中的RNN不那么强大，但是它更容易训练，因为每个时间步可以与其他时间步分离训练，允许训练期间更多的并行化。  

![](/assets/RNN_DesignPattern_2.png)  
\(3\)隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络，如下图所示：  
这样的网络可以用于概括序列并产生用于进一步处理的固定大小的表示。在结束处可能存在目标，或者通过更下游模块的反向传播来获得输出$$o^{(t)}$$上的梯度。
![](/assets/RNN_DesignPattern_3.png)

##LSTM
##GRU