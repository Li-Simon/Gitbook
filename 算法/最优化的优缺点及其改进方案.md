# 最优化的优缺点及其改进方案
## 常见的损失函数
### 分类问题损失函数

y是标签，f是预测结果。   
对于二分类问题$$Y = [-1,1]$$  
0-1损失函数：  
&emsp;&emsp;$$ L_{0-1}(f,y) =1_{fy \le 0}$$  
Hinge损失函数：  
&emsp;&emsp;$$L_{hinge}(f,y) = \max(0,1-fy)$$  
Logistic损失函数：  
&emsp;&emsp;$$ L_{logistic}(f,y) = \log_2(1+\exp(-fy))$$  
当预测值$$f \in [-1, 1]$$时，可以定义如下损失函数：  
交叉熵（Cross Entropy）:  
&emsp;&emsp;$$ L_{cross entropy} = -\log_2(\frac{1 + fy}{2})$$

### 回归问题损失函数

对于回归问题，Y=R,我们希望$$f(x_i, \theta) \approx y_i$$,最常用的损失函数是平方损失函数：  
&emsp;&emsp;$$ L_{square}(f,y) = (f-y)^2$$  
平方损失函数对异常点惩罚很大，因此对异常点比较敏感。为了解决这个问题，引入了绝对损失函数：  
&emsp;&emsp;$$L_{square}(f,y) = |f-y|$$  
但是绝对损失函数在0点不可以求导。  
一个方法是在0点进行函数光滑化也就是在0点附件用平方函数代替。$$\delta->0$$  
&emsp;&emsp;$$ L_{square}(f,y) = \sqrt{(f-y)^2+\delta^2}$$  
或者用Huber损失函数：  
Huber损失函数：  
&emsp;&emsp;$$L_{square}(f,y) = \frac{1}{2}(f-y)^2, |f-y| < \delta$$  
&emsp;&emsp;$$L_{square}(f,y) = \delta|f-y| - \frac{1}{2}\delta^2, |f-y|>\delta$$  
误差大的时候选择线性惩罚，误差小的时候选择平方惩罚。是一种Robust Regression损失函数。    
还得补充与思考文本处理，信号处理中的损失函数。

#### 机器学习中哪些是凸优化问题，哪些是非凸优化问题

凸函数条件：Hassian矩阵为半正定。  
逻辑回归，线性回归，SVM是凸优化问题。PCA，矩阵分解以及NN是非凸优化问题。

## 梯度下降

#### 问题1：怎样验证自己代码中梯度公式的正确性？

&emsp;&emsp;$$|\frac{L(\theta + he_i) - L(\theta + he_i)}{2h} - \frac{\partial L(\theta)}{\theta_i}| \approx Mh^2$$  
若$$M \le 10^7$$,则:  
&emsp;&emsp;$$ |\frac{L(\theta + he_i) - L(\theta + he_i)}{2h} - \frac{\partial L(\theta)}{\theta_i}|  \le h$$   
因此取$$h=10^{-7}$$看,看上式是否成立，不成立则$$M\ge 10^7$$,则取$$h=10^{-8}$$，看上式得值是否变为原来的$$10^{-2}$$。如果是则，求导正确，不是则求导错误。

#### 问题2：当数据量特别大的时候经典梯度下降法存在什么问题，需要怎么改进？

因为经典的梯度下降是对整个训练集进行的，如果训练集特别大，每次更新要针对整个数据集，因此需要很大的计算力，计算很费时间。  
一般采用随机梯度下降发，每次训练一个数据。但是该方法很难收敛，会在极值点振荡，因此一般采用mini-batch Gradient Descent方法。一般batch的size m取2的整数次幂。  
为了充分利用数据以及避免数据的特定顺序给算法收敛带来的印象，一般每次遍历数据前，对数据先随机排序，然后每次取m个数据，直到遍历所有数据。有必要再按新规则排序，重复以上步骤。  
如何选择学习率$$\alpha$$，为了加快学习速度与提高精度。一开始取大的学习速率，误差曲线进入平台期后，再减小学习速率做更精细的调整。

#### 问题3：深度学习中常用的优化方法是随机梯度下降法，但是SGD偶尔也会失效，无法给出满意的训练结果，这是为什么？

当SGD遇到山谷的时候，由于沿山谷方向的梯度比较小，因此结果会在山谷方向来回移动，而不能像整体梯度下降一样能沿着山谷有一个确定方向的移动。  其次是遇到平坦区域，有些方向的梯度接近于0 ，因此需要很长世间才能走出这片区域。

#### 问题4：为了改进SGD，研究者都做了哪些改动？提出了哪些方法？他们各有哪些特点？

##### 动量\(Momentum\)方法

为了解决SGD在山谷振荡，鞍点停滞的现象。我们引入了带动量的SGD。普通SGD是：  
&emsp;&emsp;$$\theta_{t+1}=\theta_t - \eta g_t$$  
带动量的SGD公式是：  
&emsp;&emsp;$$v_{t}=\gamma v_{t-1} + \eta g_t$$  
&emsp;&emsp;$$\theta_{t+1}=\theta_t - v_t$$  
通过$$0 < \gamma < 1$$，能实现梯度的累积，也就是速度的累积，因此能更好的冲出山谷与平坦（平原）区域。

##### AdaGrad方法

惯性的获得是基于历史的，除了从过去的步伐中获得一股子冲劲，我们还想知道不同参数更新的步伐，这样，对于更新频率低的参数能加快更新步伐，而更新频率高的参数减小步伐。因此就有了AdaGrad方法，他采用"历史梯度平均和"来衡量不同参数的梯度的稀疏性。，取值越小表明越稀疏。更新公式如下：  
&emsp;&emsp;$$\theta_{t+1,i}=\theta_{t,i} - \frac{\eta}{(\sum_{k=0}^tg_{k,i}^2+\epsilon)^{1/2}}g_{t,i}$$  
$$\theta_{t+1,i}$$表示t+1时刻，参数向量的第i个参数，$$g_{k,i}$$表示k时刻，参数向量的第i个方向。此外，分母的求和新式实现了退火过程，意味着随着时间推移，学习速率越来越小，保证算法最终收敛。

##### Adam方法

Adam方法将惯性保持与环境感知这两个有点集合在一起。   
通过记录梯度的一阶矩来保持惯性，通过记录二阶矩来实现环境感知能力。通过指数衰减平均技术来实现时间久远的梯度对当前平均值的贡献呈指数衰减。计算公式如下：  
&emsp;&emsp;$$ m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t$$  
&emsp;&emsp;$$v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2$$  
Adam更新公式：  
&emsp;&emsp;$$\theta_{t+1} = \theta_{t} - \frac{\eta \hat m_t}{(\hat v_t + \epsilon)^{1/2}}$$  
其中:$$\hat m_t = \frac{m_t}{1-\beta_1^t}$$,$$\hat v_t = \frac{v_t}{1-\beta_2^t}$$

#### 问题5：L1正则化使得模型参数具有稀疏性的原理是什么？
#####角度1：通过做等高线图
![](/assets/L1_L2_regulation.png)
由图可知，L1更容易在轴上与登高线相交，而L2更容易在两轴之间相交。其实加L1,L2正则就是加一个约束，也就是带约束的优化问题，通过这个约束，我们就知道了解空间的限定范围。  
#####角度2：贝叶斯先验  
L1正则化相当于对模型参数引入了拉普拉斯先验，L2正则化相当于对模型参数引入了高斯先验。因为拉普拉斯先验分布中，参数取值为0的概率更高，因此更容易产生稀疏性。而高斯先验分布只会让所有参数趋于0，并且在趋于0的不同位置是等概率的，因此不会让参数等于0。

