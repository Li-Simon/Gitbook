# 高斯混合模型

##EM算法的引入
概率模型有时既含有可观测变量，又含有隐变量(潜在变量Latent variable)。如果概率模型的变量都市可观测变量，那么给定数据，就可以直接按极大似然估计方法，或者贝叶斯估计方法估计模型参数。  
但是当模型含有隐变量时，就不能简单地使用这些估计方法，EM算法就是含有隐变量的概率模型参数的极大似然估计方法，或极大后验概率估计方法。  
#### 三硬币模型
为了直观的理解含有隐变量的模型，我们讨论抛硬币的例子。  
三枚硬币A，B, C正面出现的正面的概率是$$\pi,p,q$$.进行如下实验： 先掷硬币A，正面选硬币B，反面选硬币C；然后掷选出的硬币，掷硬币的结果出现正面记作1，反面记作0.独立地重复n次实验，得到一个观察序列如下：  
1,1,0,1,0,0,1,0,1,1  
三硬币模型可以写作：  
&emsp;&emsp;$$P(y|\theta) = \sum_zP(y,z|\theta) = \sum_zP(z|\theta)P(y|z,\theta)$$  
&emsp;&emsp;&emsp;&emsp;&emsp;$$= \pi p^y(1-p)^{1-y} + (1-\pi)q^y(1-q)^{1-y}$$  
$$\Theta = (\pi, p, q)$$是模型参数，随机变量y的数据可以观察，就是B,C抛出的是正面还是反面;随机变量z的数据不可以观察,就是B,C哪枚硬币。我们需要求得就是模型的参数$$\Theta = (\pi, p, q)$$。  

#### 最大似然函数

最大似然就是寻找一组模型参数，使得在该参数下，观测序列出现的概率极大。因此我们可以定义我们的目标函数就是在该模型参数之下，每个样本出现的概率的乘积。我们要最大化的就是这些概率的乘积。    
将可观察数据表示为$$Y=(Y_1,Y_2,...,Y_n)^T$$，不可观测数据表示为   
&emsp;&emsp;$$Z=(Z_1,Z_2,..,Z_n)^T$$.  
以最简单的二分类模型为例，则观察数据的似然函数是：   
&emsp;&emsp;$$P(Y|\theta) = \sum _ZP(Z|\theta)P(Y|Z,\theta)$$  
&emsp;&emsp;&emsp;&emsp;$$  = \displaystyle \prod _{j=1}^n[\pi p^y(1-p)^{1-y} + (1-\pi)q^y(1-q)^{1-y}]$$  
考虑求模型参数 $$\Theta = (\pi, p, q)$$的极大似然估计， 即：  
&emsp;&emsp;$$\hat \theta = \arg \max_\theta \log(P(Y|\theta))$$  
只能通过迭代来求解模型参数。EM算法就是求解该问题的一种方法。其实上面的处理的方法就是Logistic回归处理二分类的方法。   

## EM算法

E步： 计算模型参数$$\pi^{(i)},p^{(i)},q^{(i)}$$下观测数据$$y_j$$来自掷硬币B的概率是\(无论是正面还是方面，都可以用下面公式进行计算\)：  
&emsp;&emsp;$$u^{(i+1)} = \frac{\pi^{(i)}(p^{(i)})^{y_j}(1-p^{(i)})^{1-y_j})}{\pi^{(i)}(p^{(i)})^{y_j}(1-p^{(i)})^{1-y_j}) + (1-\pi^{(i)})(q^{(i)})^{y_j}(1-q^{(i)})^{1-y_j})}$$  
M步：计算模型参数的新估计值：  
之所以抛硬币B是因为抛硬币A出现了正面，因此$$\pi$$\(抛A出现正面的几率如下计算\)  
&emsp;&emsp;$$\pi^{(i+1)} = \frac{1}{n}\displaystyle \sum _{j=1}^nu^{(i+1)} $$  
在抛硬币B的前提下，根据抛B得到正面的频率得到P\(B正面出现的概率\)  
&emsp;&emsp;$$p^{(i+1)} = \frac{\displaystyle \sum _{j=1}^nu^{(i+1)}y_j}{\displaystyle \sum _{j=1}^nu^{(i+1)} }$$  
在抛硬币C的前提下\(1-来自抛B的概率\)，根据抛C得到正面的频率得到P\(C正面出现的概率\)  
&emsp;&emsp;$$q^{(i+1)} = \frac{\displaystyle \sum _{j=1}^n(1 - u^{(i+1)})y_j}{\displaystyle \sum _{j=1}^n(1 - u^{(i+1)})}$$

###EM算法流程

输入：观察变量数据Y，隐变量数据Z，联合分布$$P(Y,Z|\theta)$$,条件分布$$P(Z|Y,\theta)$$;  
输出： 模型参数$$\theta$$.  
\(1\)选择参数的初始值$$\theta^{(0)}$$,开始迭代；  
\(2\)E步：记$$\theta^{(i)}$$为第i次迭代参数$$\theta$$的估计值，在第i+1次迭代的E步，计算$$\log P(Y,Z|\theta)$$在$$Y,\theta^{(i)}$$之下的对数期望：  
&emsp;&emsp;$$Q(\theta, \theta^{(i)}) = E_z[\log P(Y,Z|\theta)|Y,\theta^{(i)}] = \displaystyle \sum_z P(Z|Y,\theta^{(i)})\log P(Y,Z|\theta)$$  
这里，$$P(Y,Z|\theta^{(i)})$$是在给定观测数据Y和当前的参数估计$$\theta^{(i)}$$下隐变量数据Z的条件概率分布：  
\(3\)M步（求导令其等于0得极大）：求使$$Q(\theta, \theta^{(i)})$$极大化的$$\theta$$,确定第i+1次迭代的参数的估计值$$\theta^{(i+1)}$$:  
&emsp;&emsp;$$\theta^{(i+1)} = \arg \max_\theta Q(\theta, \theta^{(i)})$$  
\(4\)重复第\(2\)\(3\)步，直到收敛。  
简言之：  
E步: 就是求在观测数据Y以及当前估计参数$$\theta^{(i)}$$下的$$P(Y,Z|\theta)$$的对数期望值$$Q(\theta, \theta^{(i)})$$。   
M步:就是求使得$$Q(\theta, \theta^{(i)})$$极大的$$\theta$$，再将求得的$$\theta$$作为下一步$$\theta^{(i+1)}$$。  

##### Q函数

完全数据的对数似然函数$$\log P(Y,Z|\theta)$$关于给定观测数据Y和当前参数$$\theta$$下对未观察数据Z的条件概率分布$$\log P(Z|Y,\theta^{(i)})$$的期望成为Q函数，即：  
&emsp;&emsp;$$Q(\theta, \theta^{(i)}) = E_z[\log P(Y,Z|\theta)|Y,\theta^{(i)}]$$

## 高斯混合模型

高斯混合模型是指的具有如下形式的概率分布模型：  
&emsp;&emsp;$$P(y|\theta) = \displaystyle \sum _{k=1}^K \alpha_k\phi(y|\theta_k)$$  
其中$$\alpha_k$$是系数$$\alpha_k \ge 0, \displaystyle \sum _{k=1}^K \alpha_k = 1$$;$$\phi(y|\theta_k)$$是高斯分布函数，$$\theta_k = (u_k,\sigma^2_k)$$,  
&emsp;&emsp;$$P(y|\theta) = \frac{1}{\sqrt{2\pi}\sigma_k}exp(-\frac{(y-u_k)^2}{2\sigma_k^2})$$  
称为第k个分模型。

### 高斯混合模型参数估计得EM算法

可以用EM算法来求解GMM的参数$$\alpha_k,u_k,\sigma_k$$.  
$$\gamma_{jk} = 1$$表示第j个观测来自第k个分模型，$$\gamma_{jk} = 0$$表示第j个观测不是来自第k个分模型。  
有了观察数据$$y_j$$以及未观测数据$$\gamma_{jk}$$,     
$$\gamma_{jk} = 1$$，如果第j个观测来自第k个分模型。  
$$\gamma_{jk} = 0$$，其它情况。  
则完全数据是：  
&emsp;&emsp;$$ (y_j,\gamma_{j1},\gamma_{j2},...,\gamma_{jK}),j=1,2,...,K$$  
似然函数为：  
&emsp;&emsp;$$P(y,\gamma|\theta) = \displaystyle \prod _{j=1}^NP(y_j,\gamma_{j1},\gamma_{j2},...,\gamma_{jK}|\theta)$$  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$= \displaystyle \prod _{k=1}^K\displaystyle \prod _{j=1}^N[\alpha_k\phi(y_j|\theta_k)]^{\gamma_{jk}}$$  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$ = \displaystyle \prod _{k=1}^K\alpha_k^{n_k}\displaystyle \prod _{j=1}^N[\phi(y_j|\theta_k)]^{\gamma_{jk}}$$    
以上对任何分布都实用，接下来具体到GMM：  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$= \displaystyle \prod _{k=1}^K\alpha_k^{n_k}\displaystyle \prod _{j=1}^N[\frac{1}{\sqrt{2\pi}\sigma_k}\exp(-\frac{(y-u_k)^2}{2\sigma_k^2})]^{\gamma_{jk}}$$  
式中$$\displaystyle \sum _{j=1}^N\gamma_{jk} = n_k,\displaystyle \sum _{k=1}^Kn_k = N$$。  
转化为完全数据的对数似然函数。
##总结
###隐变量模型
对于隐变量模型，我们观测的现象可能由不同的因素导致，但是每次的结果只是由一个因素导致。每个因素起作用的概率不同，所有因素起作用的概率之和就是1。  
在这里，我们主要讨论的就是概率模型，我们需要求得是两组参数，一组参数是$$\theta_1,\theta_2,...,\theta_K$$，每一个$$\theta_k$$决定一个归一化的概率分布，比如高斯分布，K表示有K个不同的高斯分布，另外一组参数就是每个分布的权重$$\alpha_1,\alpha_2,...,\alpha_K$$。K还有另外一层含义，就是系统隐变量个数。 因此，总的概率分布就是：  
&emsp;&emsp;$$P(y|\Theta) = \displaystyle \sum _{k=1}^K \alpha_k \phi(y|\theta_k)$$  
满足$$\displaystyle \sum _{k=1}^K \alpha_k = 1$$。     
$$\phi(y|\theta_k)$$是归一化的概率分布。比如高斯均值，方差(多维是协方差矩阵)不同的分布。或者是任意分布，比如在人脸识别中应用比较多的t-分布。      
y是我们观测到的数据，比如硬币的正反面，或者一幅图像(一个矩阵)。  
###EM算法
极大化对数似然函数。  
完全数据的似然函数是：  
&emsp;&emsp;$$P(y,\gamma|\theta) = \displaystyle \prod _{j=1}^NP(y_j,\gamma_{j1},\gamma_{j2},...,\gamma_{jK}|\theta)$$  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$= \displaystyle \prod _{k=1}^K\displaystyle \prod _{j=1}^N[\alpha_k\phi(y_j|\theta_k)]^{\gamma_{jk}}$$  
&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$ = \displaystyle \prod _{k=1}^K\alpha_k^{n_k}\displaystyle \prod _{j=1}^N[\phi(y_j|\theta_k)]^{\gamma_{jk}}$$  
式中$$\displaystyle \sum _{j=1}^N\gamma_{jk} = n_k,\displaystyle \sum _{k=1}^Kn_k = N$$。   
   
完全数据的对数似然函数是：  
&emsp;&emsp;$$ \log P(y,\gamma|\theta) = \displaystyle \sum _{k=1}^K[n_k\log\alpha_k +\displaystyle \sum _{j=1}^N\gamma_{jk}\log[\phi(y_j|\theta_k)]]$$     


##极大似然与最大后验概率估计
最大似然估计是求参数$$\theta$$，使似然函数$$P(y|\theta)$$最大。最大后验概率估计则是求$$\theta$$使得$$P(y|\theta)P(\theta)$$最大。求得的$$\theta$$不单单让似然函数最大，要$$\theta$$出现的概率也很大。其实对数化之后，就是加上一个有关模型$$\theta$$的正则项。也就是结构风险最小化就等价于最大后验概率估计。     
因此，加正则约束的优化问题，实际上是最大化后验概率估计，而不是最大化似然函数。比如加L2正则化，就是认为，参数先验的服从高斯分布。  
MAP实际上在最大化  
&emsp;&emsp;$$P(\theta|y) = \frac{P(y|\theta)P(\theta)}{P(y)}$$    
因为$$P(y)$$是个固定值，也就是y在实验中出现的次数，比如硬币正面朝上的次数。所以可以去掉分母$$P(y)$$。因此最大化后验概率估计就是最大化$$P(\theta|y)$$。最大化$$P(\theta|y)$$的意义很明确，y出现了，要求$$\theta$$取什么值使得后验概率$$P(\theta|y)$$最大。   
那最大化$$P(\theta|y)$$与最大化$$P(y|\theta)$$的区别是什么？  
MAP最大化$$P(\theta|y)$$:也就是在已知的数据中，求结构风险最小的模型参数。  
MLP最大化$$P(y|\theta)$$:也就是求使得数据出现概率最大的模型参数,它相当于不对模型做惩罚，结果可能是训练集上效果好，测试集合上效果差。  



